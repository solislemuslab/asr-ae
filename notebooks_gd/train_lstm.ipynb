{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSfQlfz5Igv7","executionInfo":{"status":"ok","timestamp":1711053538089,"user_tz":300,"elapsed":52428,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"60a566f2-942c-4c9b-b760-4481afc42aaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/vae-asr\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/Shareddrives/vae-asr/"]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import sys\n","sys.path.append('/content/drive/Shareddrives/vae-asr/modules')\n","# from model import VAE\n","# help(VAE)\n","from data import MSA_Dataset\n","# help(MSA_Dataset)"],"metadata":{"id":"R-GOwm_6UzX5","executionInfo":{"status":"ok","timestamp":1711053546125,"user_tz":300,"elapsed":5681,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Run script to pre-process the VAE"],"metadata":{"id":"Vi44uGh0NNp6"}},{"cell_type":"markdown","source":["Query sequence for PF00565_full: SND1_HUMAN/552-660\n","\n","Query sequence for PF00041_full: TENA_HUMAN/804-884 (this is the same choice as original paper: https://github.com/BrooksResearchGroup-UM/PEVAE_Paper/blob/master/pfam_msa/script/proc_msa.py)\n","\n","Query sequence for PF00067_full: A0A8J5V3X2_ZIZPA/310-424 (chosen for its sparsity so that eliminating sequences that have too many gaps where the query sequence has a letter does not result in very few remaining sequences)\n","\n"],"metadata":{"id":"0PQ7HpcrYFeY"}},{"cell_type":"code","source":["MSA_id = \"PF00067_full\"\n","query_seq_id = \"A0A8J5V3X2_ZIZPA/310-424\""],"metadata":{"id":"EN-zbfv3ZxON","executionInfo":{"status":"ok","timestamp":1711053549205,"user_tz":300,"elapsed":517,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# !grep {query_seq_id} data/pevae_real/{MSA_id}.txt"],"metadata":{"id":"VxrahoStZ1G-","executionInfo":{"status":"ok","timestamp":1711053551559,"user_tz":300,"elapsed":4,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["The script writes the processed MSA objects as `.pkl` files in the `output/` directory."],"metadata":{"id":"EMYnCbwMQKJz"}},{"cell_type":"code","source":["! python scripts/proc_msa.py data/pevae_real/{MSA_id}.txt {query_seq_id}"],"metadata":{"id":"7BKMaf8SMwZz","executionInfo":{"status":"ok","timestamp":1710174829060,"user_tz":240,"elapsed":19766,"user":{"displayName":"Evan Gorstein","userId":"01543935332825698741"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf763204-e037-4213-84d7-1c52fb561108"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# of sequences in raw MSA: 17984\n"]}]},{"cell_type":"markdown","source":["Check resulting processed MSA"],"metadata":{"id":"qucqkBzF7l7b"}},{"cell_type":"code","source":["#Number of sequences\n","! wc -l ./output/{MSA_id}/seq_msa_char.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDNoCErd8fyY","executionInfo":{"status":"ok","timestamp":1711053559169,"user_tz":300,"elapsed":1773,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"040e4735-cade-4df0-a3d3-a9b1b97e520a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["9767 ./output/PF00067_full/seq_msa_char.txt\n"]}]},{"cell_type":"code","source":["#First few sequences\n","!head ./output/PF00565_full/seq_msa_char.txt | awk '{print $2}'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZmW-JGv6uGN","executionInfo":{"status":"ok","timestamp":1711053562662,"user_tz":300,"elapsed":1375,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"20bb9c56-a9ec-4c2a-dddf-d4a045205413"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["VMLSGVKCPSFKREADTPEPFAAEAKFFTESRLLQRDVQIIL.ESCPNQIILGTILHPNGNITELLLKEGFARCVDSMAYTQGAEKLRAAERSAKERKVRIWK\n","IRLHGIDTAETGQTCPGDWRCGAYATRRLTDLIDGRSVSCDALDRDAYNRVIARCYSDGEDLARALVAEGLAWAFTRYS.....DDYVSDEAHARADGRGVWR\n","IHLSSIRPPRLEGEGTYDIPYMFEAREFLRKKLIGKKVNVTVDYIAFSERTCATVAIGGINIAEALVSKGLATVIRYRQRSSHYDELLAAEARAIKNGKGLHS\n","VRLIGLDAPENLPR...PRCWARESTSALLRLAPRGSTVTLAFDRDDHRRQLRYVWTAGEFVNARQLAEGNAFALRVRPNVAHAEEFARLERRARAARKGLW.\n","IHLSSIRPPRLEGESAYDIPYMFEAREFLRKKLIGKKVNVTVDYISFSERTCATVSIGGINIAEALISKGLSTVIRYRQRSSHYDELLAAEARAIKNGKGLHS\n","VNLSSIRSPKMGNPRRKPAPYAREAKEFLRTRLIGQKVSVSMEYSDSKLMDFGSVFLVGPNVGELIVSRGFGTVIRHRDRSNHYDALLAAESRANSGKKGIH.\n","FVLGGIRAPKSARGPNKAEPFGQEAHDLATKRLTQRDVEVDVHSIDKVGGFIGELYINKESFAKILVEEGFATVHVYSAQAGNATELLGAEQRAKDARRGLWV\n","VTLAATRSPRAAAITNKSEELGDVARFFTESRLLHQDVTVSLLGLTSNTPFVATVTHAQGNIAAFLLQGGLARIVDHAGGPEEMGALRRAEADAKAAKKGIWH\n","VMLSGIKCPTFKREADVPEPFAAEAKFFTESRLLQRDVQIV.LESCHNQNILGTILHPNGNITELLLKEGFARCVDSIAYTRGADKLRAAERFAKERKLRIWR\n","VKVAGVITPQTAFNPRTADPLSEEAKNFVIRLVQQREVNVQVYTSDRGGNFISAVTLKGTNLSVALVEAGFATVGNADR.LPFCQQLADAEDEARSAGLNIWA\n"]}]},{"cell_type":"markdown","source":["## Load data and instantiate dataset"],"metadata":{"id":"fmIWSH0HVyn5"}},{"cell_type":"code","source":["## read multiple sequence alignment in binary representation\n","with open(f\"./output/{MSA_id}/seq_msa_binary.pkl\", 'rb') as file_handle:\n","    msa_binary = torch.tensor(pickle.load(file_handle))\n","# Number of sequences\n","n_seq = msa_binary.shape[0]\n","# Dimensions of one-hot encoding\n","nl = msa_binary.shape[1]\n","nc = msa_binary.shape[2]\n","# Print shape\n","msa_binary.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twjfbRCybyvp","executionInfo":{"status":"ok","timestamp":1711053566248,"user_tz":300,"elapsed":1549,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"4796a9db-9959-416b-baee-1d2f19caf7a4"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([9767, 107, 21])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["## each sequence has a label\n","with open(f\"./output/{MSA_id}/keys_list.pkl\", 'rb') as file_handle:\n","    msa_keys = pickle.load(file_handle)\n","msa_keys[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5is5eYwzcS2Z","executionInfo":{"status":"ok","timestamp":1711053568900,"user_tz":300,"elapsed":260,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"2549b20d-e2a0-41bf-e9e0-96671d4d3cb7"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A0A8C4CNX3_9TELE/41-498',\n"," 'A0A6D2KQW5_9BRAS/30-513',\n"," 'A0A8K0MXP5_COCNU/37-492',\n"," 'A0A6P5YD73_DURZI/55-522',\n"," 'A0A0D3FSJ4_9ORYZ/51-514',\n"," 'A0A2V1DIY2_9PLEO/30-468',\n"," 'A0A8J6AVG1_GALPY/899-1088',\n"," 'A0A168DGB0_CORFA/1-418',\n"," 'A0A8H6P498_9EURO/256-505',\n"," 'A0A7J6X3X1_THATH/28-493']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["## sequences in msa are weighted\n","with open(f\"./output/{MSA_id}/seq_weight.pkl\", 'rb') as file_handle:\n","    seq_weight = pickle.load(file_handle)\n","seq_weight = seq_weight.astype(np.float32)\n","seq_weight[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXklarQlbVpH","executionInfo":{"status":"ok","timestamp":1711053572481,"user_tz":300,"elapsed":1250,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"0148af7f-d223-4193-bceb-85602b7adfbc"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.3613485e-04, 2.0739369e-04, 8.0609527e-05, 5.5940869e-05,\n","       1.0388179e-04, 1.8525257e-04, 5.8904512e-05, 1.3547979e-04,\n","       7.5113967e-05, 6.1428691e-05], dtype=float32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["data = MSA_Dataset(msa_binary, seq_weight, msa_keys)\n","# Show a random sample\n","print(data[100][0].shape)\n","data[100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfNcfuWpf6p1","executionInfo":{"status":"ok","timestamp":1711053574957,"user_tz":300,"elapsed":6,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"ae184659-bf46-46f2-8436-9e012c612137"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([107, 21])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 1., 0.],\n","         ...,\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 0.]]),\n"," 0.00010963139,\n"," 'A0A0L1IWY7_ASPNO/723-1184')"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["We see that an item of the dataset contains the one-hot representation of the sequence, the weight assigned to the sequence (explained later), and the name of the sequence"],"metadata":{"id":"lRsWJdQnh0ia"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LSTM_VAE(nn.Module):\n","    def __init__(self, nl, nc=21, dim_latent_vars=10, num_hidden_units=[256, 256]):\n","        \"\"\"\n","        Our default is that the latent embeddings are dimension 10 and there are\n","        256 neurons in each of the hidden layers of the encoder and decoder\n","        \"\"\"\n","        super(LSTM_VAE, self).__init__()\n","\n","        ## num of amino acid types\n","        self.nc = nc\n","\n","        ## length of sequences in the MSA\n","        self.nl = nl\n","\n","        ## dimension of input\n","        self.dim_input = nc * nl\n","\n","        ## dimension of latent space\n","        self.dim_latent_vars = dim_latent_vars\n","\n","        ## num of hidden neurons in encoder and decoder networks\n","        self.num_hidden_units = num_hidden_units\n","\n","        ## encoder\n","        # self.encoder_linears = nn.ModuleList()\n","        # self.encoder_linears.append(nn.Linear(self.dim_input, num_hidden_units[0]))\n","        # for i in range(1, len(num_hidden_units)):\n","        #     self.encoder_linears.append(nn.Linear(num_hidden_units[i-1], num_hidden_units[i]))\n","\n","        # self.encoder_lstm = nn.LSTM(self.nc, num_hidden_units[0], 1, batch_first=True)\n","        # self.encoder_mu = nn.Linear(num_hidden_units[-1], dim_latent_vars)\n","        # self.encoder_logsigma = nn.Linear(num_hidden_units[-1], dim_latent_vars)\n","        self.encoder_lstm_mu = nn.LSTM(self.nc, dim_latent_vars, 1, batch_first=True)\n","        self.encoder_lstm_sigma = nn.LSTM(self.nc, dim_latent_vars, 1, batch_first=True)\n","\n","        ## decoder\n","        self.decoder_linears = nn.ModuleList()\n","        self.decoder_linears.append(nn.Linear(dim_latent_vars, num_hidden_units[0]))\n","        for i in range(1, len(num_hidden_units)):\n","            self.decoder_linears.append(nn.Linear(num_hidden_units[i-1], num_hidden_units[i]))\n","        self.decoder_linears.append(nn.Linear(num_hidden_units[-1], self.dim_input))\n","\n","    def encoder(self, x):\n","        '''\n","        encoder transforms x into latent space z\n","        '''\n","        # convert from matrix to vector by concatenating rows (which are one-hot vectors)\n","        # h = torch.flatten(x, start_dim=-2) # start_dim=-2 to maintain batch dimension\n","        # for T in self.encoder_linears:\n","        #     h = T(h)\n","        #     h = F.relu(h)\n","\n","        # h, _ = self.encoder_lstm(x)\n","        # h = h[:, -1, :]\n","        # mu = self.encoder_mu(h)\n","        # sigma = torch.exp(self.encoder_logsigma(h))\n","\n","        mu, _ = self.encoder_lstm_mu(x)\n","        mu = mu[:, -1, :]\n","        sigma, _ = self.encoder_lstm_sigma(x)\n","        sigma = torch.exp(sigma[:, -1, :])\n","        return mu, sigma\n","\n","    def decoder(self, z):\n","        '''\n","        decoder transforms latent space z into p, which is the probability  of x being 1.\n","        '''\n","        h = z\n","        for i in range(len(self.decoder_linears)-1):\n","            h = self.decoder_linears[i](h)\n","            h = F.relu(h)\n","        h = self.decoder_linears[-1](h) #Should now have dimension nc*nl\n","\n","        fixed_shape = tuple(h.shape[0:-1])\n","        h = torch.unsqueeze(h, -1)\n","        h = torch.reshape(h, fixed_shape + (-1, self.nc))\n","        log_p = F.log_softmax(h, dim = -1)\n","        #log_p = torch.reshape(log_p, fixed_shape + (-1,))\n","\n","        return log_p\n","\n","    def compute_weighted_elbo(self, x, weight):\n","        ## sample z from q(z|x)\n","        mu, sigma = self.encoder(x)\n","        eps = torch.randn_like(sigma)\n","        z = mu + sigma*eps\n","\n","        ## compute log p(x|z)\n","        log_p = self.decoder(z)\n","        log_PxGz = torch.sum(x*log_p, [-1,-2]) # sum over both position and character dimension\n","\n","        ## compute elbo\n","        elbo = log_PxGz - torch.sum(0.5*(sigma**2 + mu**2 - 2*torch.log(sigma) - 1), -1)\n","        weight = weight / torch.sum(weight)\n","        elbo = torch.sum(elbo*weight)\n","\n","        return elbo\n","\n","    def compute_elbo_with_multiple_samples(self, x, num_samples):\n","        '''\n","        Evidence lower bound is an lower bound of log P(x). Although it is a lower\n","        bound, we can use elbo to approximate log P(x).\n","        Using multiple samples to calculate the elbo makes it be a better approximation\n","        of log P(x).\n","        '''\n","\n","        with torch.no_grad():\n","            # x = x.expand(num_samples, x.shape[0], x.shape[1], x.shape[2])\n","            mu, sigma = self.encoder(x)\n","            eps = torch.randn_like(mu)\n","            z = mu + sigma * eps\n","            log_Pz = torch.sum(-0.5*z**2 - 0.5*torch.log(2*z.new_tensor(np.pi)), -1)\n","            log_p = self.decoder(z)\n","            log_PxGz = torch.sum(x*log_p, [-1,-2]) # sum over both position and character dimension\n","            log_Pxz = log_Pz + log_PxGz\n","\n","            log_QzGx = torch.sum(-0.5*(eps)**2 -\n","                                 0.5*torch.log(2*z.new_tensor(np.pi))\n","                                 - torch.log(sigma), -1)\n","            log_weight = (log_Pxz - log_QzGx).detach().data\n","            log_weight = log_weight.double()\n","            log_weight_max = torch.max(log_weight, 0)[0]\n","            log_weight = log_weight - log_weight_max\n","            weight = torch.exp(log_weight)\n","            elbo = torch.log(torch.mean(weight, 0)) + log_weight_max\n","            return elbo\n"],"metadata":{"id":"lLoSmXnFlNGn","executionInfo":{"status":"ok","timestamp":1711053578006,"user_tz":300,"elapsed":694,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Instantiate model"],"metadata":{"id":"mL486WPSWwry"}},{"cell_type":"code","source":["# For architecture hyper-parameters, we rely on the defaults in the class definition\n","model = LSTM_VAE(nl = nl, nc = nc)"],"metadata":{"id":"f6wWCLXIWIOq","executionInfo":{"status":"ok","timestamp":1711053584995,"user_tz":300,"elapsed":273,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Let's check that our model processes data the way we want it to"],"metadata":{"id":"IayZ9owwiMzN"}},{"cell_type":"code","source":["#Encoding\n","one_hot_ary = data[100][0]\n","batch_one_hot_ary = torch.unsqueeze(one_hot_ary, 0)\n","latent_parameters = model.encoder(batch_one_hot_ary)\n","print(f\"Mean and variance of latent vector:\")\n","print(latent_parameters)\n","#Decoding\n","mn_z = latent_parameters[0]\n","recon_log_probs = model.decoder(mn_z)\n","print(f\"Decoded output has shape {recon_log_probs.shape} and is given by:\")\n","print(recon_log_probs)\n","probs = torch.exp(recon_log_probs.squeeze())\n","print(\"The probability for each amino acid in each position is:\")\n","print(probs)\n","print(\"Rows should sum to 1: \")\n","print(torch.sum(probs, dim = 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3qcR_u-a5aE","executionInfo":{"status":"ok","timestamp":1711053587048,"user_tz":300,"elapsed":3,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"007886f7-551f-4434-bd52-6db173ef1c1c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean and variance of latent vector:\n","(tensor([[-0.1525, -0.1203, -0.1802,  0.0737, -0.0857, -0.2614, -0.2022, -0.0153,\n","          0.0207, -0.1515]], grad_fn=<SliceBackward0>), tensor([[0.8769, 1.0348, 1.0499, 0.8576, 1.0131, 0.8935, 1.2319, 0.9701, 0.9333,\n","         0.9465]], grad_fn=<ExpBackward0>))\n","Decoded output has shape torch.Size([1, 107, 21]) and is given by:\n","tensor([[[-3.0028, -3.1271, -2.9971,  ..., -3.0775, -3.0450, -3.0710],\n","         [-3.0170, -3.0657, -3.1120,  ..., -3.1630, -3.0219, -3.0229],\n","         [-3.0571, -3.0267, -3.0418,  ..., -3.0884, -3.0777, -3.1239],\n","         ...,\n","         [-3.0499, -3.0121, -3.0252,  ..., -3.0183, -3.0814, -3.0802],\n","         [-3.0117, -3.0074, -3.0252,  ..., -3.0825, -3.0829, -3.0389],\n","         [-3.0051, -3.1326, -3.0181,  ..., -2.9992, -3.0349, -2.9896]]],\n","       grad_fn=<LogSoftmaxBackward0>)\n","The probability for each amino acid in each position is:\n","tensor([[0.0496, 0.0438, 0.0499,  ..., 0.0461, 0.0476, 0.0464],\n","        [0.0489, 0.0466, 0.0445,  ..., 0.0423, 0.0487, 0.0487],\n","        [0.0470, 0.0485, 0.0477,  ..., 0.0456, 0.0461, 0.0440],\n","        ...,\n","        [0.0474, 0.0492, 0.0485,  ..., 0.0489, 0.0459, 0.0460],\n","        [0.0492, 0.0494, 0.0485,  ..., 0.0458, 0.0458, 0.0479],\n","        [0.0495, 0.0436, 0.0489,  ..., 0.0498, 0.0481, 0.0503]],\n","       grad_fn=<ExpBackward0>)\n","Rows should sum to 1: \n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n","       grad_fn=<SumBackward1>)\n"]}]},{"cell_type":"markdown","source":["## Train model"],"metadata":{"id":"G_WRwRp3iYcG"}},{"cell_type":"markdown","source":["We'll use K-fold cross validation to evaluate the training of the VAE. What this means in practice is that we partition the data into K equal-sized folds. Then for each fold, we train the VAE (for 30 epochs) using all data outside that fold and evaluate the trained model with the data from the fold."],"metadata":{"id":"Zb9-5QC1kon9"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","K= 5\n","# Initialize the k-fold cross validation\n","kf = KFold(n_splits=K, shuffle=True)"],"metadata":{"id":"YKFPSpNokn5-","executionInfo":{"status":"ok","timestamp":1711053591218,"user_tz":300,"elapsed":1050,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["for fold, (train_idx, valid_idx) in enumerate(kf.split(data)):\n","    print(f\"Fold {fold + 1}\")\n","    print(\"Indices of first 10 validation examples:\")\n","    print(valid_idx[:10])\n","    print(\"-------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTBdf0A1zo3g","executionInfo":{"status":"ok","timestamp":1711053593639,"user_tz":300,"elapsed":10,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"ab7e5626-2b6c-4e9f-8c6f-788b43e7fad0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n","Indices of first 10 validation examples:\n","[ 2  3  5 18 19 25 28 30 31 38]\n","-------\n","Fold 2\n","Indices of first 10 validation examples:\n","[ 1  6 11 12 15 29 32 34 41 51]\n","-------\n","Fold 3\n","Indices of first 10 validation examples:\n","[ 4  9 13 17 21 22 26 27 35 48]\n","-------\n","Fold 4\n","Indices of first 10 validation examples:\n","[ 0  7  8 20 23 24 36 37 47 50]\n","-------\n","Fold 5\n","Indices of first 10 validation examples:\n","[10 14 16 33 42 44 53 55 66 71]\n","-------\n"]}]},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","if device == \"cuda\":\n","  model.cuda()\n","\n","# Training hyperparameters\n","num_epochs = 500\n","dim_lat = 64\n","weight_decay = 0.01\n","batch_size = 128\n","verbose = False\n","\n","# Define how to do an epoch of training\n","def train(model, device, train_loader, optimizer, epoch, verbose):\n","\n","  model.train()\n","  running_loss = []\n","\n","  for batch_idx, (msa, weight, _) in enumerate(train_loader):\n","    msa, weight = msa.to(device), weight.to(device)\n","    optimizer.zero_grad()\n","    loss = (-1)*model.compute_weighted_elbo(msa, weight)\n","    loss.backward()\n","    optimizer.step()\n","    loss_scalar = loss.data.item()\n","    if verbose:\n","      print(\"Epoch: {:>4}, Step: {:>4}, loss: {:>4.2f}\".format(epoch, batch_idx, loss_scalar), \\\n","            flush = True)\n","    running_loss.append(loss_scalar)\n","\n","  return running_loss\n","\n","# Define how to evaluate the model on the validation data\n","def eval(model, device, valid_loader):\n","\n","  model.eval()\n","  running_elbos = []\n","  with torch.no_grad():\n","    for (msa, _, _) in valid_loader:\n","      msa = msa.to(device)\n","      elbo = model.compute_elbo_with_multiple_samples(msa, 1000)\n","      elbo_scalar = torch.sum(elbo).data.item()\n","      running_elbos.append(elbo_scalar)\n","\n","  return running_elbos\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dM6BF_Zixnz3","executionInfo":{"status":"ok","timestamp":1711053596925,"user_tz":300,"elapsed":735,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"24d090b6-630a-4215-fcc4-4d14a55f8f61"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["train_loss = {}\n","valid_elbos = {}\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(data)):\n","\n","  print(f\"Fold {fold + 1}\")\n","  print(\"-------\")\n","\n","  # put the training and validation data into separate dataloaders\n","  train_loader = DataLoader(dataset = data,\n","                           batch_size = batch_size,\n","                           sampler = torch.utils.data.SubsetRandomSampler(train_idx))\n","\n","  valid_loader = DataLoader(dataset = data,\n","                           batch_size = batch_size,\n","                           sampler = torch.utils.data.SubsetRandomSampler(valid_idx))\n","\n","  # Instantiate a new model for this fold\n","  model = LSTM_VAE(nl = nl, nc = nc, dim_latent_vars=dim_lat).to(device)\n","  optimizer = optim.Adam(model.parameters(), weight_decay = weight_decay)\n","\n","  # Define a key in the dictionaries for storing the losses and elbos for this fold\n","  train_loss[fold+1] = []\n","  valid_elbos[fold+1] = []\n","\n","  for epoch in range(num_epochs):\n","\n","    batch_losses = train(model, device, train_loader, optimizer, epoch, verbose)\n","    epoch_ave_train_loss = np.mean(batch_losses)\n","    print(f\"Training loss for fold {fold + 1}, epoch {epoch}: {epoch_ave_train_loss}\")\n","    train_loss[fold+1].append(epoch_ave_train_loss)\n","\n","    batch_elbos = eval(model, device, valid_loader)\n","    epoch_ave_valid_elbo = np.mean(batch_elbos)\n","    print(f\"Validation elbo for fold {fold + 1}, epoch {epoch}: {epoch_ave_valid_elbo}\")\n","    valid_elbos[fold+1].append(epoch_ave_valid_elbo)\n","\n","  model.cpu()\n","  torch.save(model.state_dict(), \"./lstm_output/{}/vae_fold{}_dimlat{}_{}_{:.2f}.model\".format(MSA_id, fold+1, dim_lat, num_epochs, weight_decay))\n"],"metadata":{"id":"xXVreHkyyW3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711054988405,"user_tz":300,"elapsed":1386393,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"fdb5c2b7-328d-4830-b021-7bd1bbc9d97f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Training loss for fold 1, epoch 4: 226.2987031013735\n","Validation elbo for fold 1, epoch 4: -183.2792555283746\n","Training loss for fold 1, epoch 5: 226.39288797686177\n","Validation elbo for fold 1, epoch 5: -186.46706487656087\n","Training loss for fold 1, epoch 6: 226.18800944666708\n","Validation elbo for fold 1, epoch 6: -185.84425971143386\n","Training loss for fold 1, epoch 7: 225.89177826912172\n","Validation elbo for fold 1, epoch 7: -187.2896179093749\n","Training loss for fold 1, epoch 8: 225.90841256418537\n","Validation elbo for fold 1, epoch 8: -186.726657860641\n","Training loss for fold 1, epoch 9: 225.59675647366433\n","Validation elbo for fold 1, epoch 9: -185.7054751966098\n","Training loss for fold 1, epoch 10: 225.8106728830645\n","Validation elbo for fold 1, epoch 10: -187.90614382256763\n","Training loss for fold 1, epoch 11: 225.64088587607108\n","Validation elbo for fold 1, epoch 11: -183.105823435372\n","Training loss for fold 1, epoch 12: 225.69934032809348\n","Validation elbo for fold 1, epoch 12: -185.72118464357288\n","Training loss for fold 1, epoch 13: 225.4728784868794\n","Validation elbo for fold 1, epoch 13: -185.52140940916098\n","Training loss for fold 1, epoch 14: 225.9131560787078\n","Validation elbo for fold 1, epoch 14: -185.22142126493128\n","Training loss for fold 1, epoch 15: 225.9215580109627\n","Validation elbo for fold 1, epoch 15: -187.65825962409627\n","Training loss for fold 1, epoch 16: 225.735477816674\n","Validation elbo for fold 1, epoch 16: -184.25924673701655\n","Training loss for fold 1, epoch 17: 225.97875730452998\n","Validation elbo for fold 1, epoch 17: -189.6206797372668\n","Training loss for fold 1, epoch 18: 226.62928156698905\n","Validation elbo for fold 1, epoch 18: -186.08410326163437\n","Training loss for fold 1, epoch 19: 225.9748328424269\n","Validation elbo for fold 1, epoch 19: -186.98185223462247\n","Training loss for fold 1, epoch 20: 225.64919625559162\n","Validation elbo for fold 1, epoch 20: -186.30275693337393\n","Training loss for fold 1, epoch 21: 225.2609134797127\n","Validation elbo for fold 1, epoch 21: -183.14199121318777\n","Training loss for fold 1, epoch 22: 225.72777409707345\n","Validation elbo for fold 1, epoch 22: -185.91392543575532\n","Training loss for fold 1, epoch 23: 225.41621866533833\n","Validation elbo for fold 1, epoch 23: -185.79158223398116\n","Training loss for fold 1, epoch 24: 225.39408899122668\n","Validation elbo for fold 1, epoch 24: -184.60183940824749\n","Training loss for fold 1, epoch 25: 225.32344448950982\n","Validation elbo for fold 1, epoch 25: -185.54185831069825\n","Training loss for fold 1, epoch 26: 225.46219536565965\n","Validation elbo for fold 1, epoch 26: -185.9149159247541\n","Training loss for fold 1, epoch 27: 225.37162214709866\n","Validation elbo for fold 1, epoch 27: -183.33890904320236\n","Training loss for fold 1, epoch 28: 225.35555882607736\n","Validation elbo for fold 1, epoch 28: -188.24464136085658\n","Training loss for fold 1, epoch 29: 225.16310365738408\n","Validation elbo for fold 1, epoch 29: -185.4932254462424\n","Training loss for fold 1, epoch 30: 225.1417743313697\n","Validation elbo for fold 1, epoch 30: -186.2152080082479\n","Training loss for fold 1, epoch 31: 225.16621718868132\n","Validation elbo for fold 1, epoch 31: -182.88797788100948\n","Training loss for fold 1, epoch 32: 225.2997283935547\n","Validation elbo for fold 1, epoch 32: -185.6716655308179\n","Training loss for fold 1, epoch 33: 225.42291112099923\n","Validation elbo for fold 1, epoch 33: -184.8311666030222\n","Training loss for fold 1, epoch 34: 225.01958785518522\n","Validation elbo for fold 1, epoch 34: -185.29120286126778\n","Training loss for fold 1, epoch 35: 225.2306380733367\n","Validation elbo for fold 1, epoch 35: -185.42168120299124\n","Training loss for fold 1, epoch 36: 225.28766090639175\n","Validation elbo for fold 1, epoch 36: -186.7464077799471\n","Training loss for fold 1, epoch 37: 225.31646334740424\n","Validation elbo for fold 1, epoch 37: -188.3953519178428\n","Training loss for fold 1, epoch 38: 225.06905069658833\n","Validation elbo for fold 1, epoch 38: -185.7266778067289\n","Training loss for fold 1, epoch 39: 225.4772710492534\n","Validation elbo for fold 1, epoch 39: -186.52338129730146\n","Training loss for fold 1, epoch 40: 225.39845595821257\n","Validation elbo for fold 1, epoch 40: -185.5392484731937\n","Training loss for fold 1, epoch 41: 225.21984371062248\n","Validation elbo for fold 1, epoch 41: -185.7287111908496\n","Training loss for fold 1, epoch 42: 225.05033210016066\n","Validation elbo for fold 1, epoch 42: -187.47069790291704\n","Training loss for fold 1, epoch 43: 225.16272341820502\n","Validation elbo for fold 1, epoch 43: -183.56392237552768\n","Training loss for fold 1, epoch 44: 225.01656390774636\n","Validation elbo for fold 1, epoch 44: -186.53029608322447\n","Training loss for fold 1, epoch 45: 224.89132419709236\n","Validation elbo for fold 1, epoch 45: -186.92634596311981\n","Training loss for fold 1, epoch 46: 224.98963879000755\n","Validation elbo for fold 1, epoch 46: -184.7751594013699\n","Training loss for fold 1, epoch 47: 225.09196373724168\n","Validation elbo for fold 1, epoch 47: -183.3853708112146\n","Training loss for fold 1, epoch 48: 225.07350773965157\n","Validation elbo for fold 1, epoch 48: -186.5545157691675\n","Training loss for fold 1, epoch 49: 225.02464885096396\n","Validation elbo for fold 1, epoch 49: -186.03429208119363\n","Training loss for fold 1, epoch 50: 225.09493304837136\n","Validation elbo for fold 1, epoch 50: -185.956071606784\n","Training loss for fold 1, epoch 51: 225.19662278698337\n","Validation elbo for fold 1, epoch 51: -184.7017059932607\n","Training loss for fold 1, epoch 52: 225.0467300415039\n","Validation elbo for fold 1, epoch 52: -185.20714184799462\n","Training loss for fold 1, epoch 53: 224.8382051529423\n","Validation elbo for fold 1, epoch 53: -185.9703493385025\n","Training loss for fold 1, epoch 54: 224.82444172520792\n","Validation elbo for fold 1, epoch 54: -183.77418074665098\n","Training loss for fold 1, epoch 55: 225.09671906502015\n","Validation elbo for fold 1, epoch 55: -183.83579825031543\n","Training loss for fold 1, epoch 56: 224.74785392515122\n","Validation elbo for fold 1, epoch 56: -184.5513027323069\n","Training loss for fold 1, epoch 57: 224.74457008607925\n","Validation elbo for fold 1, epoch 57: -184.43420656116774\n","Training loss for fold 1, epoch 58: 225.07667369227255\n","Validation elbo for fold 1, epoch 58: -185.24771078045035\n","Training loss for fold 1, epoch 59: 225.14777497322328\n","Validation elbo for fold 1, epoch 59: -185.41186620197817\n","Training loss for fold 1, epoch 60: 224.9992208173198\n","Validation elbo for fold 1, epoch 60: -186.59784626083635\n","Training loss for fold 1, epoch 61: 225.19068022697203\n","Validation elbo for fold 1, epoch 61: -185.23683065365356\n","Training loss for fold 1, epoch 62: 225.11969117195375\n","Validation elbo for fold 1, epoch 62: -186.09782187812246\n","Training loss for fold 1, epoch 63: 224.84146487328314\n","Validation elbo for fold 1, epoch 63: -185.2300465424052\n","Training loss for fold 1, epoch 64: 225.04503754646547\n","Validation elbo for fold 1, epoch 64: -187.99192074486479\n","Training loss for fold 1, epoch 65: 224.8460971463111\n","Validation elbo for fold 1, epoch 65: -184.87514844574315\n","Training loss for fold 1, epoch 66: 224.9235606039724\n","Validation elbo for fold 1, epoch 66: -184.81714234517477\n","Training loss for fold 1, epoch 67: 224.92130304151965\n","Validation elbo for fold 1, epoch 67: -185.38084017243563\n","Training loss for fold 1, epoch 68: 225.23526050198464\n","Validation elbo for fold 1, epoch 68: -186.45519509916778\n","Training loss for fold 1, epoch 69: 224.9683301371913\n","Validation elbo for fold 1, epoch 69: -186.058069336134\n","Training loss for fold 1, epoch 70: 224.77293937436997\n","Validation elbo for fold 1, epoch 70: -185.4810097413988\n","Training loss for fold 1, epoch 71: 225.04428100585938\n","Validation elbo for fold 1, epoch 71: -185.9847548811859\n","Training loss for fold 1, epoch 72: 224.8053246774981\n","Validation elbo for fold 1, epoch 72: -184.11653862116668\n","Training loss for fold 1, epoch 73: 225.15254506757182\n","Validation elbo for fold 1, epoch 73: -185.1602956750817\n","Training loss for fold 1, epoch 74: 225.07853649508567\n","Validation elbo for fold 1, epoch 74: -186.6203919883432\n","Training loss for fold 1, epoch 75: 225.1596443422379\n","Validation elbo for fold 1, epoch 75: -187.12346612684223\n","Training loss for fold 1, epoch 76: 224.81201885592552\n","Validation elbo for fold 1, epoch 76: -186.18657846239532\n","Training loss for fold 1, epoch 77: 224.98885247015184\n","Validation elbo for fold 1, epoch 77: -185.6419798981641\n","Training loss for fold 1, epoch 78: 225.190309832173\n","Validation elbo for fold 1, epoch 78: -184.74037894037255\n","Training loss for fold 1, epoch 79: 225.11327435893398\n","Validation elbo for fold 1, epoch 79: -185.51606877512097\n","Training loss for fold 1, epoch 80: 224.8785855693202\n","Validation elbo for fold 1, epoch 80: -185.35857391431222\n","Training loss for fold 1, epoch 81: 225.0265389719317\n","Validation elbo for fold 1, epoch 81: -185.4835579724882\n","Training loss for fold 1, epoch 82: 224.88547811200542\n","Validation elbo for fold 1, epoch 82: -186.99159569916748\n","Training loss for fold 1, epoch 83: 224.91251004126764\n","Validation elbo for fold 1, epoch 83: -185.70960686282183\n","Training loss for fold 1, epoch 84: 224.8676228677073\n","Validation elbo for fold 1, epoch 84: -185.14210377544254\n","Training loss for fold 1, epoch 85: 225.14605712890625\n","Validation elbo for fold 1, epoch 85: -186.3312236392519\n","Training loss for fold 1, epoch 86: 224.85411883938698\n","Validation elbo for fold 1, epoch 86: -184.77811185918597\n","Training loss for fold 1, epoch 87: 224.8239728866085\n","Validation elbo for fold 1, epoch 87: -185.01893264594298\n","Training loss for fold 1, epoch 88: 224.75197059877456\n","Validation elbo for fold 1, epoch 88: -187.11611442095\n","Training loss for fold 1, epoch 89: 224.71153529997795\n","Validation elbo for fold 1, epoch 89: -186.79232193087574\n","Training loss for fold 1, epoch 90: 224.80296891735446\n","Validation elbo for fold 1, epoch 90: -185.75381798771014\n","Training loss for fold 1, epoch 91: 225.1219721148091\n","Validation elbo for fold 1, epoch 91: -186.0740250444685\n","Training loss for fold 1, epoch 92: 225.1260769751764\n","Validation elbo for fold 1, epoch 92: -186.00450941642504\n","Training loss for fold 1, epoch 93: 224.84341627551663\n","Validation elbo for fold 1, epoch 93: -185.94754046458937\n","Training loss for fold 1, epoch 94: 225.16436127693422\n","Validation elbo for fold 1, epoch 94: -185.4283863115445\n","Training loss for fold 1, epoch 95: 224.9528353291173\n","Validation elbo for fold 1, epoch 95: -185.62164309980426\n","Training loss for fold 1, epoch 96: 224.93614885883946\n","Validation elbo for fold 1, epoch 96: -185.49453140819668\n","Training loss for fold 1, epoch 97: 224.7563498712355\n","Validation elbo for fold 1, epoch 97: -183.45880858967297\n","Training loss for fold 1, epoch 98: 224.72205328172254\n","Validation elbo for fold 1, epoch 98: -184.75596718230133\n","Training loss for fold 1, epoch 99: 224.81228736139113\n","Validation elbo for fold 1, epoch 99: -186.8491168430063\n","Training loss for fold 1, epoch 100: 224.73013084165513\n","Validation elbo for fold 1, epoch 100: -184.00101666538444\n","Training loss for fold 1, epoch 101: 224.7845690327306\n","Validation elbo for fold 1, epoch 101: -184.80466827396384\n","Training loss for fold 1, epoch 102: 224.64780893633443\n","Validation elbo for fold 1, epoch 102: -185.23441248395807\n","Training loss for fold 1, epoch 103: 224.80785148374497\n","Validation elbo for fold 1, epoch 103: -184.94245979576243\n","Training loss for fold 1, epoch 104: 224.97338621077998\n","Validation elbo for fold 1, epoch 104: -187.2288449756118\n","Training loss for fold 1, epoch 105: 225.00208725467806\n","Validation elbo for fold 1, epoch 105: -186.1130129114822\n","Training loss for fold 1, epoch 106: 224.86624637726814\n","Validation elbo for fold 1, epoch 106: -185.8497652569904\n","Training loss for fold 1, epoch 107: 224.66220314271987\n","Validation elbo for fold 1, epoch 107: -185.47574646290678\n","Training loss for fold 1, epoch 108: 224.9023395661385\n","Validation elbo for fold 1, epoch 108: -187.99223319497025\n","Training loss for fold 1, epoch 109: 224.76202466410976\n","Validation elbo for fold 1, epoch 109: -185.75769121756687\n","Training loss for fold 1, epoch 110: 224.85174831267327\n","Validation elbo for fold 1, epoch 110: -185.1592687731532\n","Training loss for fold 1, epoch 111: 224.90288543701172\n","Validation elbo for fold 1, epoch 111: -187.60712835795113\n","Training loss for fold 1, epoch 112: 224.87273825368572\n","Validation elbo for fold 1, epoch 112: -187.4095596626775\n","Training loss for fold 1, epoch 113: 224.87016222553868\n","Validation elbo for fold 1, epoch 113: -184.79368077626395\n","Training loss for fold 1, epoch 114: 224.66314746487527\n","Validation elbo for fold 1, epoch 114: -186.05635321781085\n","Training loss for fold 1, epoch 115: 224.71776605421496\n","Validation elbo for fold 1, epoch 115: -187.27740559398234\n","Training loss for fold 1, epoch 116: 224.69544219970703\n","Validation elbo for fold 1, epoch 116: -185.18791098009132\n","Training loss for fold 1, epoch 117: 224.98694930538053\n","Validation elbo for fold 1, epoch 117: -185.13276684399733\n","Training loss for fold 1, epoch 118: 224.63186571674962\n","Validation elbo for fold 1, epoch 118: -185.99949884316027\n","Training loss for fold 1, epoch 119: 225.26037253102947\n","Validation elbo for fold 1, epoch 119: -183.80227032028714\n","Training loss for fold 1, epoch 120: 224.78639590355658\n","Validation elbo for fold 1, epoch 120: -185.30654831914322\n","Training loss for fold 1, epoch 121: 224.93545409171813\n","Validation elbo for fold 1, epoch 121: -185.96834805250606\n","Training loss for fold 1, epoch 122: 224.6689256237399\n","Validation elbo for fold 1, epoch 122: -185.2520038910548\n","Training loss for fold 1, epoch 123: 224.82128020255797\n","Validation elbo for fold 1, epoch 123: -184.94928860482872\n","Training loss for fold 1, epoch 124: 224.71344560192478\n","Validation elbo for fold 1, epoch 124: -185.8750386518412\n","Training loss for fold 1, epoch 125: 224.9807077223255\n","Validation elbo for fold 1, epoch 125: -185.54351376922153\n","Training loss for fold 1, epoch 126: 224.92857139341294\n","Validation elbo for fold 1, epoch 126: -186.16920851759187\n","Training loss for fold 1, epoch 127: 224.69852029123615\n","Validation elbo for fold 1, epoch 127: -185.11939961598222\n","Training loss for fold 1, epoch 128: 224.95131511072958\n","Validation elbo for fold 1, epoch 128: -186.73619287930433\n","Training loss for fold 1, epoch 129: 224.6871534778226\n","Validation elbo for fold 1, epoch 129: -184.93650306266983\n","Training loss for fold 1, epoch 130: 224.73709377165764\n","Validation elbo for fold 1, epoch 130: -185.82279739812253\n","Training loss for fold 1, epoch 131: 225.08362677789503\n","Validation elbo for fold 1, epoch 131: -185.8797213127231\n","Training loss for fold 1, epoch 132: 224.73148345947266\n","Validation elbo for fold 1, epoch 132: -186.11816938857038\n","Training loss for fold 1, epoch 133: 224.6229435090096\n","Validation elbo for fold 1, epoch 133: -183.5590599274175\n","Training loss for fold 1, epoch 134: 224.7712929018082\n","Validation elbo for fold 1, epoch 134: -184.6663534560669\n","Training loss for fold 1, epoch 135: 224.767457285235\n","Validation elbo for fold 1, epoch 135: -186.10250027367076\n","Training loss for fold 1, epoch 136: 225.04446386521863\n","Validation elbo for fold 1, epoch 136: -184.72669651978606\n","Training loss for fold 1, epoch 137: 224.76324610556327\n","Validation elbo for fold 1, epoch 137: -184.81847639041467\n","Training loss for fold 1, epoch 138: 224.9577880367156\n","Validation elbo for fold 1, epoch 138: -186.08754074011438\n","Training loss for fold 1, epoch 139: 224.91478926135647\n","Validation elbo for fold 1, epoch 139: -185.34901560296322\n","Training loss for fold 1, epoch 140: 224.69990908715033\n","Validation elbo for fold 1, epoch 140: -186.18426784648358\n","Training loss for fold 1, epoch 141: 224.96634969403667\n","Validation elbo for fold 1, epoch 141: -185.98958948120418\n","Training loss for fold 1, epoch 142: 224.59056042086692\n","Validation elbo for fold 1, epoch 142: -185.8755423732635\n","Training loss for fold 1, epoch 143: 224.9533684022965\n","Validation elbo for fold 1, epoch 143: -188.67209690216237\n","Training loss for fold 1, epoch 144: 224.50996275871032\n","Validation elbo for fold 1, epoch 144: -184.49018600177914\n","Training loss for fold 1, epoch 145: 224.72593738186745\n","Validation elbo for fold 1, epoch 145: -185.98169077269623\n","Training loss for fold 1, epoch 146: 224.68600980697138\n","Validation elbo for fold 1, epoch 146: -186.65065509780024\n","Training loss for fold 1, epoch 147: 224.57130161408455\n","Validation elbo for fold 1, epoch 147: -186.54347299241448\n","Training loss for fold 1, epoch 148: 224.60931248818673\n","Validation elbo for fold 1, epoch 148: -184.75960342008068\n","Training loss for fold 1, epoch 149: 224.6810012325164\n","Validation elbo for fold 1, epoch 149: -185.80522122482955\n","Training loss for fold 1, epoch 150: 224.86536702802104\n","Validation elbo for fold 1, epoch 150: -185.90915373828648\n","Training loss for fold 1, epoch 151: 224.88076708393712\n","Validation elbo for fold 1, epoch 151: -184.95062537147723\n","Training loss for fold 1, epoch 152: 224.72825844057144\n","Validation elbo for fold 1, epoch 152: -184.13846532587502\n","Training loss for fold 1, epoch 153: 224.4310022169544\n","Validation elbo for fold 1, epoch 153: -185.44710926879972\n","Training loss for fold 1, epoch 154: 224.89581249606223\n","Validation elbo for fold 1, epoch 154: -186.1531281935075\n","Training loss for fold 1, epoch 155: 224.79266898862778\n","Validation elbo for fold 1, epoch 155: -186.7467368058346\n","Training loss for fold 1, epoch 156: 224.8581978582567\n","Validation elbo for fold 1, epoch 156: -185.71131996257992\n","Training loss for fold 1, epoch 157: 224.661136750252\n","Validation elbo for fold 1, epoch 157: -184.7954236637852\n","Training loss for fold 1, epoch 158: 224.84608361028856\n","Validation elbo for fold 1, epoch 158: -186.22188625266037\n","Training loss for fold 1, epoch 159: 224.72036669331212\n","Validation elbo for fold 1, epoch 159: -186.23552689746674\n","Training loss for fold 1, epoch 160: 224.59012209984564\n","Validation elbo for fold 1, epoch 160: -184.59204802588397\n","Training loss for fold 1, epoch 161: 224.95549921835624\n","Validation elbo for fold 1, epoch 161: -185.58752032213295\n","Training loss for fold 1, epoch 162: 224.79265742148124\n","Validation elbo for fold 1, epoch 162: -185.09850718913248\n","Training loss for fold 1, epoch 163: 224.81789324360508\n","Validation elbo for fold 1, epoch 163: -185.20500551881523\n","Training loss for fold 1, epoch 164: 224.52614445840157\n","Validation elbo for fold 1, epoch 164: -184.82381861166954\n","Training loss for fold 1, epoch 165: 225.53786936113912\n","Validation elbo for fold 1, epoch 165: -186.3395612886518\n","Training loss for fold 1, epoch 166: 225.0012465446226\n","Validation elbo for fold 1, epoch 166: -185.19178555962327\n","Training loss for fold 1, epoch 167: 224.80470595821257\n","Validation elbo for fold 1, epoch 167: -185.26198705844695\n","Training loss for fold 1, epoch 168: 225.00575871621407\n","Validation elbo for fold 1, epoch 168: -184.99047858635114\n","Training loss for fold 1, epoch 169: 224.62601003339213\n","Validation elbo for fold 1, epoch 169: -185.55369056562785\n","Training loss for fold 1, epoch 170: 224.43743994928175\n","Validation elbo for fold 1, epoch 170: -185.9963514488812\n","Training loss for fold 1, epoch 171: 224.64920142389113\n","Validation elbo for fold 1, epoch 171: -185.55685270640913\n","Training loss for fold 1, epoch 172: 224.5037342194588\n","Validation elbo for fold 1, epoch 172: -184.64270741656344\n","Training loss for fold 1, epoch 173: 224.54459454936367\n","Validation elbo for fold 1, epoch 173: -185.0145268456326\n","Training loss for fold 1, epoch 174: 224.81424171693862\n","Validation elbo for fold 1, epoch 174: -185.81375861383682\n","Training loss for fold 1, epoch 175: 224.5804960189327\n","Validation elbo for fold 1, epoch 175: -186.67760407952784\n","Training loss for fold 1, epoch 176: 224.95430780226184\n","Validation elbo for fold 1, epoch 176: -186.6261433775933\n","Training loss for fold 1, epoch 177: 224.52516494258757\n","Validation elbo for fold 1, epoch 177: -187.70616853992715\n","Training loss for fold 1, epoch 178: 224.6910587433846\n","Validation elbo for fold 1, epoch 178: -187.20034119679693\n","Training loss for fold 1, epoch 179: 224.70223137640184\n","Validation elbo for fold 1, epoch 179: -184.71048299191767\n","Training loss for fold 1, epoch 180: 224.73111355689264\n","Validation elbo for fold 1, epoch 180: -185.4487543792028\n","Training loss for fold 1, epoch 181: 224.77532639042025\n","Validation elbo for fold 1, epoch 181: -186.07764260372858\n","Training loss for fold 1, epoch 182: 224.56697747015184\n","Validation elbo for fold 1, epoch 182: -184.31129573606657\n","Training loss for fold 1, epoch 183: 224.6645510273595\n","Validation elbo for fold 1, epoch 183: -185.7041468626499\n","Training loss for fold 1, epoch 184: 224.68444159723097\n","Validation elbo for fold 1, epoch 184: -184.05974111851998\n","Training loss for fold 1, epoch 185: 224.549680894421\n","Validation elbo for fold 1, epoch 185: -185.53663871688343\n","Training loss for fold 1, epoch 186: 224.8060780186807\n","Validation elbo for fold 1, epoch 186: -186.64143176378306\n","Training loss for fold 1, epoch 187: 224.7307335638231\n","Validation elbo for fold 1, epoch 187: -185.57615630652245\n","Training loss for fold 1, epoch 188: 224.53941419047695\n","Validation elbo for fold 1, epoch 188: -184.10348124910374\n","Training loss for fold 1, epoch 189: 224.89024525303995\n","Validation elbo for fold 1, epoch 189: -185.81817281200898\n","Training loss for fold 1, epoch 190: 224.93870569044543\n","Validation elbo for fold 1, epoch 190: -185.71309277135504\n","Training loss for fold 1, epoch 191: 224.64144503685736\n","Validation elbo for fold 1, epoch 191: -185.62374164741726\n","Training loss for fold 1, epoch 192: 224.67769918134135\n","Validation elbo for fold 1, epoch 192: -185.39545423120813\n","Training loss for fold 1, epoch 193: 224.3560052687122\n","Validation elbo for fold 1, epoch 193: -185.0230589728415\n","Training loss for fold 1, epoch 194: 224.78817306026335\n","Validation elbo for fold 1, epoch 194: -187.36723160296935\n","Training loss for fold 1, epoch 195: 224.74237749653477\n","Validation elbo for fold 1, epoch 195: -185.12797031896685\n","Training loss for fold 1, epoch 196: 224.93329398862778\n","Validation elbo for fold 1, epoch 196: -186.57632787483027\n","Training loss for fold 1, epoch 197: 225.05380125968688\n","Validation elbo for fold 1, epoch 197: -185.90164409626158\n","Training loss for fold 1, epoch 198: 224.58114008749686\n","Validation elbo for fold 1, epoch 198: -185.06494477916902\n","Training loss for fold 1, epoch 199: 224.60251888152092\n","Validation elbo for fold 1, epoch 199: -184.86519390854713\n","Training loss for fold 1, epoch 200: 224.78264962473224\n","Validation elbo for fold 1, epoch 200: -185.71201619792328\n","Training loss for fold 1, epoch 201: 224.63180763490737\n","Validation elbo for fold 1, epoch 201: -185.6936478248673\n","Training loss for fold 1, epoch 202: 224.49727261450982\n","Validation elbo for fold 1, epoch 202: -185.10491688409238\n","Training loss for fold 1, epoch 203: 224.47350360501198\n","Validation elbo for fold 1, epoch 203: -186.343894130944\n","Training loss for fold 1, epoch 204: 224.88379619967552\n","Validation elbo for fold 1, epoch 204: -186.60388131333562\n","Training loss for fold 1, epoch 205: 224.73975052372103\n","Validation elbo for fold 1, epoch 205: -185.3500318036238\n","Training loss for fold 1, epoch 206: 224.76934051513672\n","Validation elbo for fold 1, epoch 206: -186.1394310528786\n","Training loss for fold 1, epoch 207: 224.41250659573464\n","Validation elbo for fold 1, epoch 207: -185.55481368047484\n","Training loss for fold 1, epoch 208: 224.55413572249873\n","Validation elbo for fold 1, epoch 208: -185.36091258786496\n","Training loss for fold 1, epoch 209: 225.0464841781124\n","Validation elbo for fold 1, epoch 209: -185.7517092285015\n","Training loss for fold 1, epoch 210: 224.83696008497668\n","Validation elbo for fold 1, epoch 210: -187.06206680687671\n","Training loss for fold 1, epoch 211: 224.75013019192605\n","Validation elbo for fold 1, epoch 211: -185.63241990529565\n","Training loss for fold 1, epoch 212: 224.6616228165165\n","Validation elbo for fold 1, epoch 212: -185.73006036275882\n","Training loss for fold 1, epoch 213: 224.6235856086977\n","Validation elbo for fold 1, epoch 213: -184.42487845615085\n","Training loss for fold 1, epoch 214: 224.84217588363154\n","Validation elbo for fold 1, epoch 214: -185.84280307344176\n","Training loss for fold 1, epoch 215: 224.55860703991306\n","Validation elbo for fold 1, epoch 215: -185.17604128935335\n","Training loss for fold 1, epoch 216: 224.49149322509766\n","Validation elbo for fold 1, epoch 216: -185.1900040681604\n","Training loss for fold 1, epoch 217: 224.57835265128844\n","Validation elbo for fold 1, epoch 217: -184.9444997587035\n","Training loss for fold 1, epoch 218: 224.63916064846902\n","Validation elbo for fold 1, epoch 218: -184.59225495760307\n","Training loss for fold 1, epoch 219: 224.75527043496407\n","Validation elbo for fold 1, epoch 219: -185.27352994778403\n","Training loss for fold 1, epoch 220: 224.71991188295425\n","Validation elbo for fold 1, epoch 220: -185.89794971872442\n","Training loss for fold 1, epoch 221: 224.81399240801412\n","Validation elbo for fold 1, epoch 221: -187.1592766077178\n","Training loss for fold 1, epoch 222: 224.58214987477947\n","Validation elbo for fold 1, epoch 222: -185.99590255616778\n","Training loss for fold 1, epoch 223: 224.5733886226531\n","Validation elbo for fold 1, epoch 223: -186.34011371448423\n","Training loss for fold 1, epoch 224: 224.81787552372103\n","Validation elbo for fold 1, epoch 224: -187.1272107296673\n","Training loss for fold 1, epoch 225: 224.67417390884893\n","Validation elbo for fold 1, epoch 225: -185.694415973672\n","Training loss for fold 1, epoch 226: 224.574952156313\n","Validation elbo for fold 1, epoch 226: -186.816035015463\n","Training loss for fold 1, epoch 227: 224.8028330649099\n","Validation elbo for fold 1, epoch 227: -186.13630676341657\n","Training loss for fold 1, epoch 228: 224.66360375189012\n","Validation elbo for fold 1, epoch 228: -185.2118589937333\n","Training loss for fold 1, epoch 229: 224.8032413605721\n","Validation elbo for fold 1, epoch 229: -187.01031974304829\n","Training loss for fold 1, epoch 230: 224.6250484835717\n","Validation elbo for fold 1, epoch 230: -185.44145411342015\n","Training loss for fold 1, epoch 231: 224.80781752063382\n","Validation elbo for fold 1, epoch 231: -186.97656019024777\n","Training loss for fold 1, epoch 232: 224.5834928943265\n","Validation elbo for fold 1, epoch 232: -185.3492956869054\n","Training loss for fold 1, epoch 233: 224.3804395121913\n","Validation elbo for fold 1, epoch 233: -185.87311155093653\n","Training loss for fold 1, epoch 234: 224.5732857488817\n","Validation elbo for fold 1, epoch 234: -186.66019219392481\n","Training loss for fold 1, epoch 235: 224.4978499873992\n","Validation elbo for fold 1, epoch 235: -186.5791096528541\n","Training loss for fold 1, epoch 236: 224.54463564965033\n","Validation elbo for fold 1, epoch 236: -185.80628032695364\n","Training loss for fold 1, epoch 237: 224.58145436932963\n","Validation elbo for fold 1, epoch 237: -185.9861607972714\n","Training loss for fold 1, epoch 238: 224.63950913952243\n","Validation elbo for fold 1, epoch 238: -186.48878850260206\n","Training loss for fold 1, epoch 239: 224.4974601499496\n","Validation elbo for fold 1, epoch 239: -186.1123140784898\n","Training loss for fold 1, epoch 240: 224.8522486532888\n","Validation elbo for fold 1, epoch 240: -186.2372938871078\n","Training loss for fold 1, epoch 241: 224.60923619424142\n","Validation elbo for fold 1, epoch 241: -186.31220607126335\n","Training loss for fold 1, epoch 242: 224.68229626071067\n","Validation elbo for fold 1, epoch 242: -185.66867990299536\n","Training loss for fold 1, epoch 243: 224.58737404115737\n","Validation elbo for fold 1, epoch 243: -186.40849604644134\n","Training loss for fold 1, epoch 244: 224.753056926112\n","Validation elbo for fold 1, epoch 244: -186.3250033175294\n","Training loss for fold 1, epoch 245: 224.725216527139\n","Validation elbo for fold 1, epoch 245: -185.36504002789815\n","Training loss for fold 1, epoch 246: 224.80821055750692\n","Validation elbo for fold 1, epoch 246: -185.97723265676197\n","Training loss for fold 1, epoch 247: 224.65486341907132\n","Validation elbo for fold 1, epoch 247: -185.84390999658797\n","Training loss for fold 1, epoch 248: 224.8635509860131\n","Validation elbo for fold 1, epoch 248: -185.3250508654973\n","Training loss for fold 1, epoch 249: 224.46151167346585\n","Validation elbo for fold 1, epoch 249: -185.49160478556985\n","Training loss for fold 1, epoch 250: 224.51966267247354\n","Validation elbo for fold 1, epoch 250: -185.55973577932264\n","Training loss for fold 1, epoch 251: 224.63853036203693\n","Validation elbo for fold 1, epoch 251: -187.07613734796575\n","Training loss for fold 1, epoch 252: 224.5896264353106\n","Validation elbo for fold 1, epoch 252: -185.40182294878548\n","Training loss for fold 1, epoch 253: 224.79721487722088\n","Validation elbo for fold 1, epoch 253: -186.73893288099202\n","Training loss for fold 1, epoch 254: 224.72502997613722\n","Validation elbo for fold 1, epoch 254: -185.08318636996609\n","Training loss for fold 1, epoch 255: 224.799688769925\n","Validation elbo for fold 1, epoch 255: -186.11118961997587\n","Training loss for fold 1, epoch 256: 224.5835891231414\n","Validation elbo for fold 1, epoch 256: -186.85246090123007\n","Training loss for fold 1, epoch 257: 224.81205257292717\n","Validation elbo for fold 1, epoch 257: -185.22895357984652\n","Training loss for fold 1, epoch 258: 224.6112318961851\n","Validation elbo for fold 1, epoch 258: -187.04795500657457\n","Training loss for fold 1, epoch 259: 224.4600352625693\n","Validation elbo for fold 1, epoch 259: -185.77425851587458\n","Training loss for fold 1, epoch 260: 224.41040285172002\n","Validation elbo for fold 1, epoch 260: -185.14360599289745\n","Training loss for fold 1, epoch 261: 224.55518045733052\n","Validation elbo for fold 1, epoch 261: -186.24194556298667\n","Training loss for fold 1, epoch 262: 224.58991044567478\n","Validation elbo for fold 1, epoch 262: -186.7675630218899\n","Training loss for fold 1, epoch 263: 224.65953014742942\n","Validation elbo for fold 1, epoch 263: -185.5332456099884\n","Training loss for fold 1, epoch 264: 224.5430435672883\n","Validation elbo for fold 1, epoch 264: -184.42427095684255\n","Training loss for fold 1, epoch 265: 224.93035027288622\n","Validation elbo for fold 1, epoch 265: -185.2506423950207\n","Training loss for fold 1, epoch 266: 224.69358874905495\n","Validation elbo for fold 1, epoch 266: -184.68548670359016\n","Training loss for fold 1, epoch 267: 224.74334889073526\n","Validation elbo for fold 1, epoch 267: -186.03072157063605\n","Training loss for fold 1, epoch 268: 224.86802993282194\n","Validation elbo for fold 1, epoch 268: -186.6619641085587\n","Training loss for fold 1, epoch 269: 224.70589742352885\n","Validation elbo for fold 1, epoch 269: -185.44342187218808\n","Training loss for fold 1, epoch 270: 224.858762925671\n","Validation elbo for fold 1, epoch 270: -185.57771589263137\n","Training loss for fold 1, epoch 271: 224.70928487470073\n","Validation elbo for fold 1, epoch 271: -185.49500181615437\n","Training loss for fold 1, epoch 272: 224.813592480075\n","Validation elbo for fold 1, epoch 272: -185.37087685363633\n","Training loss for fold 1, epoch 273: 224.6494428573116\n","Validation elbo for fold 1, epoch 273: -185.2764339243606\n","Training loss for fold 1, epoch 274: 224.7712136545489\n","Validation elbo for fold 1, epoch 274: -185.81348668617682\n","Training loss for fold 1, epoch 275: 225.05144894507623\n","Validation elbo for fold 1, epoch 275: -186.87566989693676\n","Training loss for fold 1, epoch 276: 224.7335899106918\n","Validation elbo for fold 1, epoch 276: -185.4630799357986\n","Training loss for fold 1, epoch 277: 224.56606957220262\n","Validation elbo for fold 1, epoch 277: -185.125082361736\n","Training loss for fold 1, epoch 278: 224.53722283147997\n","Validation elbo for fold 1, epoch 278: -186.35965790009243\n","Training loss for fold 1, epoch 279: 224.65043467860067\n","Validation elbo for fold 1, epoch 279: -186.0240990674269\n","Training loss for fold 1, epoch 280: 224.71866484611266\n","Validation elbo for fold 1, epoch 280: -186.47037873667637\n","Training loss for fold 1, epoch 281: 224.55410643546813\n","Validation elbo for fold 1, epoch 281: -184.9397993368306\n","Training loss for fold 1, epoch 282: 224.70339965820312\n","Validation elbo for fold 1, epoch 282: -184.38221880383475\n","Training loss for fold 1, epoch 283: 224.92030531360257\n","Validation elbo for fold 1, epoch 283: -186.41364345236548\n","Training loss for fold 1, epoch 284: 224.7039088587607\n","Validation elbo for fold 1, epoch 284: -185.70950767448267\n","Training loss for fold 1, epoch 285: 224.5309034778226\n","Validation elbo for fold 1, epoch 285: -184.6095998445416\n","Training loss for fold 1, epoch 286: 224.7523700344947\n","Validation elbo for fold 1, epoch 286: -186.13226823010766\n","Training loss for fold 1, epoch 287: 224.55806806010585\n","Validation elbo for fold 1, epoch 287: -185.6588309482842\n","Training loss for fold 1, epoch 288: 224.72771995298325\n","Validation elbo for fold 1, epoch 288: -186.34440375214137\n","Training loss for fold 1, epoch 289: 224.58189195202243\n","Validation elbo for fold 1, epoch 289: -184.50150270483516\n","Training loss for fold 1, epoch 290: 224.69969792519845\n","Validation elbo for fold 1, epoch 290: -185.1937564710949\n","Training loss for fold 1, epoch 291: 224.57096124464465\n","Validation elbo for fold 1, epoch 291: -185.30305704805352\n","Training loss for fold 1, epoch 292: 224.52338163314326\n","Validation elbo for fold 1, epoch 292: -185.64378329642693\n","Training loss for fold 1, epoch 293: 224.58944209929436\n","Validation elbo for fold 1, epoch 293: -185.40785378922897\n","Training loss for fold 1, epoch 294: 224.4377443867345\n","Validation elbo for fold 1, epoch 294: -184.7895925608358\n","Training loss for fold 1, epoch 295: 224.59154830440397\n","Validation elbo for fold 1, epoch 295: -186.0359933880363\n","Training loss for fold 1, epoch 296: 224.65856121432395\n","Validation elbo for fold 1, epoch 296: -185.84191086825777\n","Training loss for fold 1, epoch 297: 224.525512449203\n","Validation elbo for fold 1, epoch 297: -185.62639836468261\n","Training loss for fold 1, epoch 298: 224.65652342765563\n","Validation elbo for fold 1, epoch 298: -185.14707906712596\n","Training loss for fold 1, epoch 299: 224.6312465052451\n","Validation elbo for fold 1, epoch 299: -185.2059107259194\n","Training loss for fold 1, epoch 300: 224.30577579621345\n","Validation elbo for fold 1, epoch 300: -184.81174242556494\n","Training loss for fold 1, epoch 301: 224.6799567437941\n","Validation elbo for fold 1, epoch 301: -184.96615394015754\n","Training loss for fold 1, epoch 302: 224.38152953117125\n","Validation elbo for fold 1, epoch 302: -185.29305037721096\n","Training loss for fold 1, epoch 303: 224.76814909904235\n","Validation elbo for fold 1, epoch 303: -185.80808483893128\n","Training loss for fold 1, epoch 304: 224.7237775248866\n","Validation elbo for fold 1, epoch 304: -185.96874205997793\n","Training loss for fold 1, epoch 305: 224.56493303852696\n","Validation elbo for fold 1, epoch 305: -185.2058584211061\n","Training loss for fold 1, epoch 306: 224.80712152296496\n","Validation elbo for fold 1, epoch 306: -185.9421000892531\n","Training loss for fold 1, epoch 307: 224.7628902312248\n","Validation elbo for fold 1, epoch 307: -186.52813472650482\n","Training loss for fold 1, epoch 308: 224.67784586260396\n","Validation elbo for fold 1, epoch 308: -184.71670016253432\n","Training loss for fold 1, epoch 309: 224.68180428781818\n","Validation elbo for fold 1, epoch 309: -185.1584314729734\n","Training loss for fold 1, epoch 310: 224.63692449754285\n","Validation elbo for fold 1, epoch 310: -185.08184722290252\n","Training loss for fold 1, epoch 311: 224.77449823194934\n","Validation elbo for fold 1, epoch 311: -185.3076929175102\n","Training loss for fold 1, epoch 312: 224.61826915125692\n","Validation elbo for fold 1, epoch 312: -185.6726056121271\n","Training loss for fold 1, epoch 313: 224.62891658659905\n","Validation elbo for fold 1, epoch 313: -187.14643229250476\n","Training loss for fold 1, epoch 314: 224.62291865195\n","Validation elbo for fold 1, epoch 314: -185.4438971999648\n","Training loss for fold 1, epoch 315: 224.85352940713204\n","Validation elbo for fold 1, epoch 315: -186.49755079221868\n","Training loss for fold 1, epoch 316: 224.6805875224452\n","Validation elbo for fold 1, epoch 316: -186.73942660501146\n","Training loss for fold 1, epoch 317: 224.72240251110446\n","Validation elbo for fold 1, epoch 317: -185.37577182598977\n","Training loss for fold 1, epoch 318: 224.66141386955016\n","Validation elbo for fold 1, epoch 318: -186.29540620699487\n","Training loss for fold 1, epoch 319: 224.69026774744833\n","Validation elbo for fold 1, epoch 319: -185.55539851093096\n","Training loss for fold 1, epoch 320: 224.45217797064012\n","Validation elbo for fold 1, epoch 320: -186.28392330926278\n","Training loss for fold 1, epoch 321: 224.68071426883822\n","Validation elbo for fold 1, epoch 321: -185.33116612475084\n","Training loss for fold 1, epoch 322: 224.47560636458857\n","Validation elbo for fold 1, epoch 322: -186.06482573924453\n","Training loss for fold 1, epoch 323: 224.2906014227098\n","Validation elbo for fold 1, epoch 323: -185.05577998493277\n","Training loss for fold 1, epoch 324: 224.60528810562627\n","Validation elbo for fold 1, epoch 324: -184.56078648861987\n","Training loss for fold 1, epoch 325: 224.47092733075542\n","Validation elbo for fold 1, epoch 325: -184.00302454083936\n","Training loss for fold 1, epoch 326: 224.44817942957724\n","Validation elbo for fold 1, epoch 326: -185.6601243271741\n","Training loss for fold 1, epoch 327: 224.89551937964654\n","Validation elbo for fold 1, epoch 327: -186.29474777362572\n","Training loss for fold 1, epoch 328: 224.44202693816155\n","Validation elbo for fold 1, epoch 328: -185.42915938080057\n","Training loss for fold 1, epoch 329: 224.45550044890373\n","Validation elbo for fold 1, epoch 329: -187.06799909972142\n","Training loss for fold 1, epoch 330: 224.57085098758822\n","Validation elbo for fold 1, epoch 330: -185.91298544433536\n","Training loss for fold 1, epoch 331: 224.96939505300213\n","Validation elbo for fold 1, epoch 331: -185.64432519149443\n","Training loss for fold 1, epoch 332: 224.55525748960434\n","Validation elbo for fold 1, epoch 332: -185.92399056820568\n","Training loss for fold 1, epoch 333: 224.73900210472846\n","Validation elbo for fold 1, epoch 333: -185.2791157050676\n","Training loss for fold 1, epoch 334: 224.68842217230028\n","Validation elbo for fold 1, epoch 334: -185.5653456372491\n","Training loss for fold 1, epoch 335: 224.61946672008884\n","Validation elbo for fold 1, epoch 335: -184.86284471073876\n","Training loss for fold 1, epoch 336: 224.69439697265625\n","Validation elbo for fold 1, epoch 336: -185.11550936611587\n","Training loss for fold 1, epoch 337: 224.600220464891\n","Validation elbo for fold 1, epoch 337: -186.3162815326701\n","Training loss for fold 1, epoch 338: 224.72188395838583\n","Validation elbo for fold 1, epoch 338: -185.90627266681634\n","Training loss for fold 1, epoch 339: 224.6598188338741\n","Validation elbo for fold 1, epoch 339: -185.82400861908917\n","Training loss for fold 1, epoch 340: 224.47202891688192\n","Validation elbo for fold 1, epoch 340: -184.75583462476658\n","Training loss for fold 1, epoch 341: 224.89244128811745\n","Validation elbo for fold 1, epoch 341: -186.3295935158052\n","Training loss for fold 1, epoch 342: 224.36429448281564\n","Validation elbo for fold 1, epoch 342: -185.9825503683183\n","Training loss for fold 1, epoch 343: 224.71690590150894\n","Validation elbo for fold 1, epoch 343: -185.4454733532228\n","Training loss for fold 1, epoch 344: 224.58882461055632\n","Validation elbo for fold 1, epoch 344: -184.71834463540625\n","Training loss for fold 1, epoch 345: 224.56284602995842\n","Validation elbo for fold 1, epoch 345: -185.36422593794808\n","Training loss for fold 1, epoch 346: 225.0337152788716\n","Validation elbo for fold 1, epoch 346: -186.36150408170005\n","Training loss for fold 1, epoch 347: 224.4974087130639\n","Validation elbo for fold 1, epoch 347: -186.4971809224936\n","Training loss for fold 1, epoch 348: 224.52975094702936\n","Validation elbo for fold 1, epoch 348: -185.14774839261293\n","Training loss for fold 1, epoch 349: 224.81408543740548\n","Validation elbo for fold 1, epoch 349: -186.80067768009866\n","Training loss for fold 1, epoch 350: 224.75778124409337\n","Validation elbo for fold 1, epoch 350: -186.01798401634346\n","Training loss for fold 1, epoch 351: 224.55926833614225\n","Validation elbo for fold 1, epoch 351: -186.84997189476178\n","Training loss for fold 1, epoch 352: 224.53485451975178\n","Validation elbo for fold 1, epoch 352: -184.47394716824107\n","Training loss for fold 1, epoch 353: 224.91617805727066\n","Validation elbo for fold 1, epoch 353: -185.58493724261444\n","Training loss for fold 1, epoch 354: 224.50435835315335\n","Validation elbo for fold 1, epoch 354: -185.94628696622686\n","Training loss for fold 1, epoch 355: 224.66538804577243\n","Validation elbo for fold 1, epoch 355: -185.3580300658967\n","Training loss for fold 1, epoch 356: 224.96905492967176\n","Validation elbo for fold 1, epoch 356: -186.17227721820404\n","Training loss for fold 1, epoch 357: 224.6249018023091\n","Validation elbo for fold 1, epoch 357: -184.90800390196125\n","Training loss for fold 1, epoch 358: 224.42598970474737\n","Validation elbo for fold 1, epoch 358: -186.6063656998199\n","Training loss for fold 1, epoch 359: 224.43617322368007\n","Validation elbo for fold 1, epoch 359: -185.91768705816907\n","Training loss for fold 1, epoch 360: 224.71593647618448\n","Validation elbo for fold 1, epoch 360: -186.23020499653208\n","Training loss for fold 1, epoch 361: 224.76774351058467\n","Validation elbo for fold 1, epoch 361: -186.54471754273717\n","Training loss for fold 1, epoch 362: 224.6948973132718\n","Validation elbo for fold 1, epoch 362: -185.66206981630307\n","Training loss for fold 1, epoch 363: 224.63980471703314\n","Validation elbo for fold 1, epoch 363: -184.65931788571245\n","Training loss for fold 1, epoch 364: 224.77259137553554\n","Validation elbo for fold 1, epoch 364: -185.19331626733597\n","Training loss for fold 1, epoch 365: 224.52335480720765\n","Validation elbo for fold 1, epoch 365: -185.36998657287722\n","Training loss for fold 1, epoch 366: 224.53170702534337\n","Validation elbo for fold 1, epoch 366: -186.46311862743238\n","Training loss for fold 1, epoch 367: 224.86306910361014\n","Validation elbo for fold 1, epoch 367: -185.8510945673367\n","Training loss for fold 1, epoch 368: 224.75009967434792\n","Validation elbo for fold 1, epoch 368: -185.87687718316536\n","Training loss for fold 1, epoch 369: 225.0738281742219\n","Validation elbo for fold 1, epoch 369: -185.07860239862214\n","Training loss for fold 1, epoch 370: 224.79545002598917\n","Validation elbo for fold 1, epoch 370: -186.372377853047\n","Training loss for fold 1, epoch 371: 224.51196042952998\n","Validation elbo for fold 1, epoch 371: -184.7779272585376\n","Training loss for fold 1, epoch 372: 224.8930169382403\n","Validation elbo for fold 1, epoch 372: -186.39498306969716\n","Training loss for fold 1, epoch 373: 224.41649824573147\n","Validation elbo for fold 1, epoch 373: -184.771099783625\n","Training loss for fold 1, epoch 374: 224.7553477133474\n","Validation elbo for fold 1, epoch 374: -185.6460815649035\n","Training loss for fold 1, epoch 375: 224.5713139195596\n","Validation elbo for fold 1, epoch 375: -185.6857403899361\n","Training loss for fold 1, epoch 376: 224.78565831338204\n","Validation elbo for fold 1, epoch 376: -185.50444948930448\n","Training loss for fold 1, epoch 377: 225.1084461827432\n","Validation elbo for fold 1, epoch 377: -186.94560867178996\n","Training loss for fold 1, epoch 378: 224.7360901371125\n","Validation elbo for fold 1, epoch 378: -185.03267481850685\n","Training loss for fold 1, epoch 379: 224.643794890373\n","Validation elbo for fold 1, epoch 379: -185.1254104982148\n","Training loss for fold 1, epoch 380: 224.5558565201298\n","Validation elbo for fold 1, epoch 380: -185.97550255006814\n","Training loss for fold 1, epoch 381: 224.70386455905052\n","Validation elbo for fold 1, epoch 381: -186.14711514197748\n","Training loss for fold 1, epoch 382: 224.49367227861958\n","Validation elbo for fold 1, epoch 382: -184.9595193579668\n","Training loss for fold 1, epoch 383: 224.45804029895413\n","Validation elbo for fold 1, epoch 383: -185.66626201697878\n","Training loss for fold 1, epoch 384: 224.7937255367156\n","Validation elbo for fold 1, epoch 384: -185.1326750124382\n","Training loss for fold 1, epoch 385: 224.7348106138168\n","Validation elbo for fold 1, epoch 385: -185.64487556188283\n","Training loss for fold 1, epoch 386: 224.67389777398878\n","Validation elbo for fold 1, epoch 386: -185.9202888315589\n","Training loss for fold 1, epoch 387: 224.52451669016193\n","Validation elbo for fold 1, epoch 387: -185.71708848828533\n","Training loss for fold 1, epoch 388: 224.48154990903794\n","Validation elbo for fold 1, epoch 388: -186.42250648693678\n","Training loss for fold 1, epoch 389: 224.715698980516\n","Validation elbo for fold 1, epoch 389: -185.33160160215962\n","Training loss for fold 1, epoch 390: 225.0339604039346\n","Validation elbo for fold 1, epoch 390: -187.77555992534946\n","Training loss for fold 1, epoch 391: 224.68366241455078\n","Validation elbo for fold 1, epoch 391: -185.11272791821096\n","Training loss for fold 1, epoch 392: 224.54130135813068\n","Validation elbo for fold 1, epoch 392: -185.76517936205727\n","Training loss for fold 1, epoch 393: 224.56658172607422\n","Validation elbo for fold 1, epoch 393: -184.82112215813757\n","Training loss for fold 1, epoch 394: 224.66472847230972\n","Validation elbo for fold 1, epoch 394: -185.48564568721218\n","Training loss for fold 1, epoch 395: 224.32254200596964\n","Validation elbo for fold 1, epoch 395: -184.57120000687127\n","Training loss for fold 1, epoch 396: 224.63077225223665\n","Validation elbo for fold 1, epoch 396: -185.8343996827752\n","Training loss for fold 1, epoch 397: 224.76881359469505\n","Validation elbo for fold 1, epoch 397: -184.96798363591753\n","Training loss for fold 1, epoch 398: 224.7617871684413\n","Validation elbo for fold 1, epoch 398: -185.65035282971994\n","Training loss for fold 1, epoch 399: 224.3935042350523\n","Validation elbo for fold 1, epoch 399: -186.34097739571587\n","Training loss for fold 1, epoch 400: 224.4960647090789\n","Validation elbo for fold 1, epoch 400: -185.3138161663565\n","Training loss for fold 1, epoch 401: 224.55289483839465\n","Validation elbo for fold 1, epoch 401: -184.8661696740744\n","Training loss for fold 1, epoch 402: 225.02638293850808\n","Validation elbo for fold 1, epoch 402: -186.03809347164207\n","Training loss for fold 1, epoch 403: 225.27709296441847\n","Validation elbo for fold 1, epoch 403: -186.3628519433497\n","Training loss for fold 1, epoch 404: 225.02816058743386\n","Validation elbo for fold 1, epoch 404: -185.48599763924673\n","Training loss for fold 1, epoch 405: 224.90655886742377\n","Validation elbo for fold 1, epoch 405: -186.37106465988808\n","Training loss for fold 1, epoch 406: 224.57655408305507\n","Validation elbo for fold 1, epoch 406: -185.72384722054903\n","Training loss for fold 1, epoch 407: 224.4835906490203\n","Validation elbo for fold 1, epoch 407: -184.93622968873132\n","Training loss for fold 1, epoch 408: 224.65235088717552\n","Validation elbo for fold 1, epoch 408: -186.08860067692854\n","Training loss for fold 1, epoch 409: 224.72063544488722\n","Validation elbo for fold 1, epoch 409: -185.67205391967542\n","Training loss for fold 1, epoch 410: 224.3422383954448\n","Validation elbo for fold 1, epoch 410: -186.35155799237583\n","Training loss for fold 1, epoch 411: 224.460448234312\n","Validation elbo for fold 1, epoch 411: -185.9193268862873\n","Training loss for fold 1, epoch 412: 224.75661394673008\n","Validation elbo for fold 1, epoch 412: -184.87614515630204\n","Training loss for fold 1, epoch 413: 224.83740480484502\n","Validation elbo for fold 1, epoch 413: -184.94598428365742\n","Training loss for fold 1, epoch 414: 224.66890298166584\n","Validation elbo for fold 1, epoch 414: -184.58591029502392\n","Training loss for fold 1, epoch 415: 224.59877137214906\n","Validation elbo for fold 1, epoch 415: -185.44282430074873\n","Training loss for fold 1, epoch 416: 224.91436496857673\n","Validation elbo for fold 1, epoch 416: -185.58823168230782\n","Training loss for fold 1, epoch 417: 224.4073208224389\n","Validation elbo for fold 1, epoch 417: -184.46843978708966\n","Training loss for fold 1, epoch 418: 224.68360260994203\n","Validation elbo for fold 1, epoch 418: -186.08458808059754\n","Training loss for fold 1, epoch 419: 224.53763235768963\n","Validation elbo for fold 1, epoch 419: -185.62063695649454\n","Training loss for fold 1, epoch 420: 224.54461817587577\n","Validation elbo for fold 1, epoch 420: -186.90409472755022\n","Training loss for fold 1, epoch 421: 224.4426778977917\n","Validation elbo for fold 1, epoch 421: -185.11314387199312\n","Training loss for fold 1, epoch 422: 224.55159193469632\n","Validation elbo for fold 1, epoch 422: -185.7070698390766\n","Training loss for fold 1, epoch 423: 224.52921393609816\n","Validation elbo for fold 1, epoch 423: -185.75776831222032\n","Training loss for fold 1, epoch 424: 224.50596692485195\n","Validation elbo for fold 1, epoch 424: -185.36068568970768\n","Training loss for fold 1, epoch 425: 224.6967721754505\n","Validation elbo for fold 1, epoch 425: -186.7120239122458\n","Training loss for fold 1, epoch 426: 224.5586641373173\n","Validation elbo for fold 1, epoch 426: -185.61891388492583\n","Training loss for fold 1, epoch 427: 224.55433728618007\n","Validation elbo for fold 1, epoch 427: -185.86006606069387\n","Training loss for fold 1, epoch 428: 224.673335905998\n","Validation elbo for fold 1, epoch 428: -185.65591848241166\n","Training loss for fold 1, epoch 429: 224.5869418728736\n","Validation elbo for fold 1, epoch 429: -186.00053145990302\n","Training loss for fold 1, epoch 430: 224.95368317634828\n","Validation elbo for fold 1, epoch 430: -185.86482672538807\n","Training loss for fold 1, epoch 431: 224.80359378937752\n","Validation elbo for fold 1, epoch 431: -185.45736067998237\n","Training loss for fold 1, epoch 432: 224.75050821611958\n","Validation elbo for fold 1, epoch 432: -185.91975401093958\n","Training loss for fold 1, epoch 433: 224.68643385364163\n","Validation elbo for fold 1, epoch 433: -185.49449636941657\n","Training loss for fold 1, epoch 434: 224.64903456164944\n","Validation elbo for fold 1, epoch 434: -186.79500349273167\n","Training loss for fold 1, epoch 435: 224.71053068099482\n","Validation elbo for fold 1, epoch 435: -186.94889816831233\n","Training loss for fold 1, epoch 436: 224.38995484382875\n","Validation elbo for fold 1, epoch 436: -185.58690791414486\n","Training loss for fold 1, epoch 437: 224.67752985800468\n","Validation elbo for fold 1, epoch 437: -185.51942033815027\n","Training loss for fold 1, epoch 438: 224.8117673320155\n","Validation elbo for fold 1, epoch 438: -186.40758445026927\n","Training loss for fold 1, epoch 439: 224.92411336591167\n","Validation elbo for fold 1, epoch 439: -185.63979707364447\n","Training loss for fold 1, epoch 440: 224.41262817382812\n","Validation elbo for fold 1, epoch 440: -185.4531928084137\n","Training loss for fold 1, epoch 441: 224.78760971561556\n","Validation elbo for fold 1, epoch 441: -185.57638618074182\n","Training loss for fold 1, epoch 442: 224.4656701857044\n","Validation elbo for fold 1, epoch 442: -186.94393017137787\n","Training loss for fold 1, epoch 443: 224.44240200904107\n","Validation elbo for fold 1, epoch 443: -184.70404651924832\n","Training loss for fold 1, epoch 444: 224.66305000551284\n","Validation elbo for fold 1, epoch 444: -186.17084693839215\n","Training loss for fold 1, epoch 445: 224.8837907852665\n","Validation elbo for fold 1, epoch 445: -186.66363886105154\n","Training loss for fold 1, epoch 446: 224.62334762081022\n","Validation elbo for fold 1, epoch 446: -185.64039367697552\n","Training loss for fold 1, epoch 447: 224.376583714639\n","Validation elbo for fold 1, epoch 447: -185.93991132566194\n","Training loss for fold 1, epoch 448: 224.50172153595955\n","Validation elbo for fold 1, epoch 448: -185.36698017321953\n","Training loss for fold 1, epoch 449: 224.8633782171434\n","Validation elbo for fold 1, epoch 449: -185.09063069682867\n","Training loss for fold 1, epoch 450: 224.7031006351594\n","Validation elbo for fold 1, epoch 450: -185.63512972750007\n","Training loss for fold 1, epoch 451: 224.65248083299207\n","Validation elbo for fold 1, epoch 451: -185.6873610628358\n","Training loss for fold 1, epoch 452: 224.5777774933846\n","Validation elbo for fold 1, epoch 452: -185.50476793593327\n","Training loss for fold 1, epoch 453: 224.50057884954637\n","Validation elbo for fold 1, epoch 453: -185.5733636999919\n","Training loss for fold 1, epoch 454: 224.8798097179782\n","Validation elbo for fold 1, epoch 454: -186.45055836170434\n","Training loss for fold 1, epoch 455: 224.84661446848224\n","Validation elbo for fold 1, epoch 455: -184.91418649219943\n","Training loss for fold 1, epoch 456: 224.45128360871345\n","Validation elbo for fold 1, epoch 456: -185.5533814202155\n","Training loss for fold 1, epoch 457: 224.49656923355596\n","Validation elbo for fold 1, epoch 457: -185.1869139446502\n","Training loss for fold 1, epoch 458: 224.60306992069368\n","Validation elbo for fold 1, epoch 458: -185.23400231939826\n","Training loss for fold 1, epoch 459: 224.4326130036385\n","Validation elbo for fold 1, epoch 459: -185.74864620725353\n","Training loss for fold 1, epoch 460: 224.9642053419544\n","Validation elbo for fold 1, epoch 460: -185.14825646184076\n","Training loss for fold 1, epoch 461: 224.62454026745212\n","Validation elbo for fold 1, epoch 461: -186.08573323766328\n","Training loss for fold 1, epoch 462: 224.4977074899981\n","Validation elbo for fold 1, epoch 462: -184.92487394512642\n","Training loss for fold 1, epoch 463: 224.60278467978202\n","Validation elbo for fold 1, epoch 463: -185.82538329344925\n","Training loss for fold 1, epoch 464: 224.6720475227602\n","Validation elbo for fold 1, epoch 464: -185.3194027212499\n","Training loss for fold 1, epoch 465: 224.63128465221774\n","Validation elbo for fold 1, epoch 465: -185.28295361255783\n","Training loss for fold 1, epoch 466: 224.5292255032447\n","Validation elbo for fold 1, epoch 466: -186.0820221852844\n","Training loss for fold 1, epoch 467: 224.71332377772177\n","Validation elbo for fold 1, epoch 467: -186.90198322396057\n","Training loss for fold 1, epoch 468: 225.32310313563192\n","Validation elbo for fold 1, epoch 468: -186.66175273334366\n","Training loss for fold 1, epoch 469: 224.49744907502205\n","Validation elbo for fold 1, epoch 469: -185.4837469542929\n","Training loss for fold 1, epoch 470: 224.83644251669608\n","Validation elbo for fold 1, epoch 470: -186.71090325646963\n","Training loss for fold 1, epoch 471: 224.66313097553868\n","Validation elbo for fold 1, epoch 471: -185.64790435367894\n","Training loss for fold 1, epoch 472: 224.36543692311932\n","Validation elbo for fold 1, epoch 472: -185.7815106020224\n","Training loss for fold 1, epoch 473: 224.5965864119991\n","Validation elbo for fold 1, epoch 473: -185.2477245613547\n","Training loss for fold 1, epoch 474: 224.5090135143649\n","Validation elbo for fold 1, epoch 474: -185.2700020615499\n","Training loss for fold 1, epoch 475: 224.42713312948905\n","Validation elbo for fold 1, epoch 475: -185.84645443246933\n","Training loss for fold 1, epoch 476: 224.50869874031312\n","Validation elbo for fold 1, epoch 476: -186.48423578849236\n","Training loss for fold 1, epoch 477: 224.60139514553933\n","Validation elbo for fold 1, epoch 477: -185.7724246636165\n","Training loss for fold 1, epoch 478: 224.7375010828818\n","Validation elbo for fold 1, epoch 478: -186.25883027456894\n","Training loss for fold 1, epoch 479: 224.6088909026115\n","Validation elbo for fold 1, epoch 479: -185.9246286808805\n","Training loss for fold 1, epoch 480: 224.6275861186366\n","Validation elbo for fold 1, epoch 480: -185.04800064729332\n","Training loss for fold 1, epoch 481: 224.65308946178806\n","Validation elbo for fold 1, epoch 481: -185.38224947480757\n","Training loss for fold 1, epoch 482: 224.4470967938823\n","Validation elbo for fold 1, epoch 482: -185.55226542556545\n","Training loss for fold 1, epoch 483: 224.73206550844253\n","Validation elbo for fold 1, epoch 483: -187.17014350875465\n","Training loss for fold 1, epoch 484: 224.58840991604714\n","Validation elbo for fold 1, epoch 484: -185.36513242204447\n","Training loss for fold 1, epoch 485: 224.3486554545741\n","Validation elbo for fold 1, epoch 485: -184.80194352685595\n","Training loss for fold 1, epoch 486: 224.81195634411228\n","Validation elbo for fold 1, epoch 486: -185.7607248206644\n","Training loss for fold 1, epoch 487: 224.59970412715788\n","Validation elbo for fold 1, epoch 487: -185.44392758316246\n","Training loss for fold 1, epoch 488: 224.49478444745463\n","Validation elbo for fold 1, epoch 488: -185.4429445314206\n","Training loss for fold 1, epoch 489: 224.59565193422378\n","Validation elbo for fold 1, epoch 489: -185.7516039211353\n","Training loss for fold 1, epoch 490: 224.89528016121156\n","Validation elbo for fold 1, epoch 490: -186.45893784781856\n","Training loss for fold 1, epoch 491: 224.63531248031123\n","Validation elbo for fold 1, epoch 491: -185.71864452062837\n","Training loss for fold 1, epoch 492: 224.62184339954007\n","Validation elbo for fold 1, epoch 492: -185.2278907401128\n","Training loss for fold 1, epoch 493: 224.80220893121535\n","Validation elbo for fold 1, epoch 493: -185.91359092783097\n","Training loss for fold 1, epoch 494: 224.7448937200731\n","Validation elbo for fold 1, epoch 494: -186.51762953149915\n","Training loss for fold 1, epoch 495: 224.83861147972846\n","Validation elbo for fold 1, epoch 495: -185.49084921540498\n","Training loss for fold 1, epoch 496: 224.661988043016\n","Validation elbo for fold 1, epoch 496: -186.30561181065303\n","Training loss for fold 1, epoch 497: 224.3539047241211\n","Validation elbo for fold 1, epoch 497: -185.34882299085007\n","Training loss for fold 1, epoch 498: 225.16337117841167\n","Validation elbo for fold 1, epoch 498: -185.74006899643803\n","Training loss for fold 1, epoch 499: 224.87110753213204\n","Validation elbo for fold 1, epoch 499: -185.57878676496628\n","Fold 2\n","-------\n","Training loss for fold 2, epoch 0: 241.1590310373614\n","Validation elbo for fold 2, epoch 0: -185.21418089962486\n","Training loss for fold 2, epoch 1: 225.93929536880987\n","Validation elbo for fold 2, epoch 1: -182.67820330722344\n","Training loss for fold 2, epoch 2: 225.94456506544543\n","Validation elbo for fold 2, epoch 2: -185.41946851852677\n","Training loss for fold 2, epoch 3: 225.79945373535156\n","Validation elbo for fold 2, epoch 3: -183.43051574507655\n","Training loss for fold 2, epoch 4: 225.60059184412802\n","Validation elbo for fold 2, epoch 4: -183.77987901065057\n","Training loss for fold 2, epoch 5: 225.6599637923702\n","Validation elbo for fold 2, epoch 5: -183.16249446204392\n","Training loss for fold 2, epoch 6: 225.35567523587136\n","Validation elbo for fold 2, epoch 6: -186.07914297389974\n","Training loss for fold 2, epoch 7: 225.3597645913401\n","Validation elbo for fold 2, epoch 7: -185.36319723858014\n","Training loss for fold 2, epoch 8: 225.29404892459993\n","Validation elbo for fold 2, epoch 8: -185.95200159198612\n","Training loss for fold 2, epoch 9: 225.30865552348476\n","Validation elbo for fold 2, epoch 9: -183.4331779425203\n","Training loss for fold 2, epoch 10: 225.18551955684538\n","Validation elbo for fold 2, epoch 10: -182.82802341325407\n","Training loss for fold 2, epoch 11: 225.09696123676915\n","Validation elbo for fold 2, epoch 11: -184.16769767966525\n","Training loss for fold 2, epoch 12: 225.00936668149888\n","Validation elbo for fold 2, epoch 12: -183.20220927517624\n","Training loss for fold 2, epoch 13: 225.06189210953252\n","Validation elbo for fold 2, epoch 13: -185.96916967175784\n","Training loss for fold 2, epoch 14: 225.00512326148248\n","Validation elbo for fold 2, epoch 14: -184.35677334710118\n","Training loss for fold 2, epoch 15: 225.0893357799899\n","Validation elbo for fold 2, epoch 15: -183.8184508794336\n","Training loss for fold 2, epoch 16: 225.00395030360067\n","Validation elbo for fold 2, epoch 16: -183.26145221948093\n","Training loss for fold 2, epoch 17: 224.93130837717365\n","Validation elbo for fold 2, epoch 17: -184.10993265453203\n","Training loss for fold 2, epoch 18: 225.067623015373\n","Validation elbo for fold 2, epoch 18: -185.05362813202998\n","Training loss for fold 2, epoch 19: 225.15610873314643\n","Validation elbo for fold 2, epoch 19: -185.13868693836878\n","Training loss for fold 2, epoch 20: 225.24303067115045\n","Validation elbo for fold 2, epoch 20: -184.92888060531828\n","Training loss for fold 2, epoch 21: 224.90026067918348\n","Validation elbo for fold 2, epoch 21: -184.0655190471566\n","Training loss for fold 2, epoch 22: 225.03546585575228\n","Validation elbo for fold 2, epoch 22: -183.72028606441364\n","Training loss for fold 2, epoch 23: 224.98753553821194\n","Validation elbo for fold 2, epoch 23: -182.85827336467415\n","Training loss for fold 2, epoch 24: 224.96021098475302\n","Validation elbo for fold 2, epoch 24: -184.66966416044443\n","Training loss for fold 2, epoch 25: 224.66497433570123\n","Validation elbo for fold 2, epoch 25: -183.3210689003066\n","Training loss for fold 2, epoch 26: 224.81158274988974\n","Validation elbo for fold 2, epoch 26: -183.8461846021952\n","Training loss for fold 2, epoch 27: 224.55638860887098\n","Validation elbo for fold 2, epoch 27: -183.3022550023382\n","Training loss for fold 2, epoch 28: 224.97900956676853\n","Validation elbo for fold 2, epoch 28: -184.518672737084\n","Training loss for fold 2, epoch 29: 224.79844173308342\n","Validation elbo for fold 2, epoch 29: -184.73291242303608\n","Training loss for fold 2, epoch 30: 224.5732224987399\n","Validation elbo for fold 2, epoch 30: -185.84178213683882\n","Training loss for fold 2, epoch 31: 224.44378022224672\n","Validation elbo for fold 2, epoch 31: -184.70870122350144\n","Training loss for fold 2, epoch 32: 224.72931277367377\n","Validation elbo for fold 2, epoch 32: -182.87418535765744\n","Training loss for fold 2, epoch 33: 224.7670123192572\n","Validation elbo for fold 2, epoch 33: -183.92989004210574\n","Training loss for fold 2, epoch 34: 224.65545408187373\n","Validation elbo for fold 2, epoch 34: -185.7535379293883\n","Training loss for fold 2, epoch 35: 224.71988284203314\n","Validation elbo for fold 2, epoch 35: -184.34377332724455\n","Training loss for fold 2, epoch 36: 224.54496395972467\n","Validation elbo for fold 2, epoch 36: -184.8068044220285\n","Training loss for fold 2, epoch 37: 224.46695463119013\n","Validation elbo for fold 2, epoch 37: -184.59051128409675\n","Training loss for fold 2, epoch 38: 224.67811756749308\n","Validation elbo for fold 2, epoch 38: -185.2713337686152\n","Training loss for fold 2, epoch 39: 224.6539606894216\n","Validation elbo for fold 2, epoch 39: -183.8491355985089\n","Training loss for fold 2, epoch 40: 224.69985937303113\n","Validation elbo for fold 2, epoch 40: -184.62614722339012\n","Training loss for fold 2, epoch 41: 224.63646894885647\n","Validation elbo for fold 2, epoch 41: -186.02736041527896\n","Training loss for fold 2, epoch 42: 224.8489224833827\n","Validation elbo for fold 2, epoch 42: -185.12655264992276\n","Training loss for fold 2, epoch 43: 224.6951188118227\n","Validation elbo for fold 2, epoch 43: -185.65810438666261\n","Training loss for fold 2, epoch 44: 224.6608377272083\n","Validation elbo for fold 2, epoch 44: -185.5422636160153\n","Training loss for fold 2, epoch 45: 224.3318845687374\n","Validation elbo for fold 2, epoch 45: -184.38205012261005\n","Training loss for fold 2, epoch 46: 224.72794292819114\n","Validation elbo for fold 2, epoch 46: -185.60345104776854\n","Training loss for fold 2, epoch 47: 224.45240832913308\n","Validation elbo for fold 2, epoch 47: -183.5750571819041\n","Training loss for fold 2, epoch 48: 224.52793982721144\n","Validation elbo for fold 2, epoch 48: -183.746329476079\n","Training loss for fold 2, epoch 49: 224.64812912479525\n","Validation elbo for fold 2, epoch 49: -185.01470742544046\n","Training loss for fold 2, epoch 50: 224.94124701715285\n","Validation elbo for fold 2, epoch 50: -186.10687120388465\n","Training loss for fold 2, epoch 51: 224.33626507174583\n","Validation elbo for fold 2, epoch 51: -183.98263368823763\n","Training loss for fold 2, epoch 52: 224.74816230035597\n","Validation elbo for fold 2, epoch 52: -185.84367193431672\n","Training loss for fold 2, epoch 53: 224.6737306656376\n","Validation elbo for fold 2, epoch 53: -183.96779749378254\n","Training loss for fold 2, epoch 54: 224.2177776213615\n","Validation elbo for fold 2, epoch 54: -183.73931255555766\n","Training loss for fold 2, epoch 55: 224.40699251236455\n","Validation elbo for fold 2, epoch 55: -185.28331851354363\n","Training loss for fold 2, epoch 56: 224.62505783573275\n","Validation elbo for fold 2, epoch 56: -186.21563351754082\n","Training loss for fold 2, epoch 57: 224.26959154682774\n","Validation elbo for fold 2, epoch 57: -185.6853599785076\n","Training loss for fold 2, epoch 58: 224.17510223388672\n","Validation elbo for fold 2, epoch 58: -183.92058037413193\n","Training loss for fold 2, epoch 59: 224.15357823525704\n","Validation elbo for fold 2, epoch 59: -184.66693266991055\n","Training loss for fold 2, epoch 60: 224.25407015892768\n","Validation elbo for fold 2, epoch 60: -184.97749855243683\n","Training loss for fold 2, epoch 61: 224.54817371983683\n","Validation elbo for fold 2, epoch 61: -185.61608733538264\n","Training loss for fold 2, epoch 62: 224.85387839040447\n","Validation elbo for fold 2, epoch 62: -185.72992309998182\n","Training loss for fold 2, epoch 63: 224.6692101263231\n","Validation elbo for fold 2, epoch 63: -186.14110492225845\n","Training loss for fold 2, epoch 64: 224.25915379678048\n","Validation elbo for fold 2, epoch 64: -184.1715007886366\n","Training loss for fold 2, epoch 65: 224.3303473687941\n","Validation elbo for fold 2, epoch 65: -184.4566610559856\n","Training loss for fold 2, epoch 66: 224.51278981854838\n","Validation elbo for fold 2, epoch 66: -185.28573104713377\n","Training loss for fold 2, epoch 67: 224.07179383308656\n","Validation elbo for fold 2, epoch 67: -183.8034767434713\n","Training loss for fold 2, epoch 68: 224.30860950100808\n","Validation elbo for fold 2, epoch 68: -185.14319225832338\n","Training loss for fold 2, epoch 69: 224.17591168803554\n","Validation elbo for fold 2, epoch 69: -184.1361926155693\n","Training loss for fold 2, epoch 70: 224.41087218253844\n","Validation elbo for fold 2, epoch 70: -185.97345006879758\n","Training loss for fold 2, epoch 71: 224.1495093068769\n","Validation elbo for fold 2, epoch 71: -184.2932352648349\n","Training loss for fold 2, epoch 72: 224.43640948880105\n","Validation elbo for fold 2, epoch 72: -185.40839193322986\n","Training loss for fold 2, epoch 73: 224.17225277808404\n","Validation elbo for fold 2, epoch 73: -184.43659069651886\n","Training loss for fold 2, epoch 74: 224.1879365982548\n","Validation elbo for fold 2, epoch 74: -184.36147002950136\n","Training loss for fold 2, epoch 75: 224.47781372070312\n","Validation elbo for fold 2, epoch 75: -182.7156470071957\n","Training loss for fold 2, epoch 76: 224.43698415448588\n","Validation elbo for fold 2, epoch 76: -185.93957957952486\n","Training loss for fold 2, epoch 77: 224.31020084504158\n","Validation elbo for fold 2, epoch 77: -184.23557019551538\n","Training loss for fold 2, epoch 78: 224.26100109469505\n","Validation elbo for fold 2, epoch 78: -183.95674272978175\n","Training loss for fold 2, epoch 79: 224.2415008544922\n","Validation elbo for fold 2, epoch 79: -182.91379283138835\n","Training loss for fold 2, epoch 80: 224.24910736083984\n","Validation elbo for fold 2, epoch 80: -184.93514496600574\n","Training loss for fold 2, epoch 81: 224.10933266916584\n","Validation elbo for fold 2, epoch 81: -183.75315878812577\n","Training loss for fold 2, epoch 82: 224.04847988005608\n","Validation elbo for fold 2, epoch 82: -184.5492009251037\n","Training loss for fold 2, epoch 83: 224.34765182002897\n","Validation elbo for fold 2, epoch 83: -185.84210492132706\n","Training loss for fold 2, epoch 84: 224.24684512230658\n","Validation elbo for fold 2, epoch 84: -184.4813433593472\n","Training loss for fold 2, epoch 85: 224.27777419551725\n","Validation elbo for fold 2, epoch 85: -185.5343330086453\n","Training loss for fold 2, epoch 86: 224.19711008379537\n","Validation elbo for fold 2, epoch 86: -185.5152004110154\n","Training loss for fold 2, epoch 87: 224.29737263341104\n","Validation elbo for fold 2, epoch 87: -184.98423122697164\n","Training loss for fold 2, epoch 88: 224.20892653926725\n","Validation elbo for fold 2, epoch 88: -184.5980522660089\n","Training loss for fold 2, epoch 89: 224.07259122786982\n","Validation elbo for fold 2, epoch 89: -184.88450871188832\n","Training loss for fold 2, epoch 90: 224.19175892491495\n","Validation elbo for fold 2, epoch 90: -185.44607449663326\n","Training loss for fold 2, epoch 91: 224.13124552080708\n","Validation elbo for fold 2, epoch 91: -186.04901281111074\n","Training loss for fold 2, epoch 92: 223.95094422371156\n","Validation elbo for fold 2, epoch 92: -185.4490884560228\n","Training loss for fold 2, epoch 93: 223.9189192248929\n","Validation elbo for fold 2, epoch 93: -184.44567032917456\n","Training loss for fold 2, epoch 94: 224.15596869684035\n","Validation elbo for fold 2, epoch 94: -183.46886297522263\n","Training loss for fold 2, epoch 95: 224.41890445832283\n","Validation elbo for fold 2, epoch 95: -184.18218481522842\n","Training loss for fold 2, epoch 96: 224.40480090725808\n","Validation elbo for fold 2, epoch 96: -185.88164113002307\n","Training loss for fold 2, epoch 97: 224.32904175789125\n","Validation elbo for fold 2, epoch 97: -185.270233464977\n","Training loss for fold 2, epoch 98: 224.31997729885964\n","Validation elbo for fold 2, epoch 98: -185.6580414213081\n","Training loss for fold 2, epoch 99: 224.1215603736139\n","Validation elbo for fold 2, epoch 99: -185.6262614262102\n","Training loss for fold 2, epoch 100: 223.8514153265184\n","Validation elbo for fold 2, epoch 100: -183.24981445092308\n","Training loss for fold 2, epoch 101: 224.41061524421937\n","Validation elbo for fold 2, epoch 101: -185.78575163097815\n","Training loss for fold 2, epoch 102: 224.03330895208543\n","Validation elbo for fold 2, epoch 102: -184.38088481213617\n","Training loss for fold 2, epoch 103: 224.29942469443046\n","Validation elbo for fold 2, epoch 103: -183.8423110702612\n","Training loss for fold 2, epoch 104: 224.42996831094064\n","Validation elbo for fold 2, epoch 104: -183.71682317508566\n","Training loss for fold 2, epoch 105: 224.47401575888358\n","Validation elbo for fold 2, epoch 105: -183.62199457739578\n","Training loss for fold 2, epoch 106: 224.34481294693487\n","Validation elbo for fold 2, epoch 106: -185.9241786868077\n","Training loss for fold 2, epoch 107: 224.2582020913401\n","Validation elbo for fold 2, epoch 107: -185.35748041844886\n","Training loss for fold 2, epoch 108: 224.2692819410755\n","Validation elbo for fold 2, epoch 108: -185.61151126988426\n","Training loss for fold 2, epoch 109: 224.06674095892137\n","Validation elbo for fold 2, epoch 109: -185.80800342749092\n","Training loss for fold 2, epoch 110: 223.9805445517263\n","Validation elbo for fold 2, epoch 110: -182.71821015734793\n","Training loss for fold 2, epoch 111: 224.23697834630167\n","Validation elbo for fold 2, epoch 111: -184.92606467164387\n","Training loss for fold 2, epoch 112: 224.39820812594505\n","Validation elbo for fold 2, epoch 112: -184.400004631623\n","Training loss for fold 2, epoch 113: 224.23609481319303\n","Validation elbo for fold 2, epoch 113: -184.2698171504597\n","Training loss for fold 2, epoch 114: 224.27090331046813\n","Validation elbo for fold 2, epoch 114: -185.90490298579766\n","Training loss for fold 2, epoch 115: 224.18634451589276\n","Validation elbo for fold 2, epoch 115: -184.2911934919121\n","Training loss for fold 2, epoch 116: 223.88414321407194\n","Validation elbo for fold 2, epoch 116: -182.79267411033697\n","Training loss for fold 2, epoch 117: 224.18494636781753\n","Validation elbo for fold 2, epoch 117: -183.09464808312114\n","Training loss for fold 2, epoch 118: 224.0709986532888\n","Validation elbo for fold 2, epoch 118: -184.8848982281048\n","Training loss for fold 2, epoch 119: 224.16455964119203\n","Validation elbo for fold 2, epoch 119: -186.23952908925236\n","Training loss for fold 2, epoch 120: 224.5420638053648\n","Validation elbo for fold 2, epoch 120: -184.16316045312212\n","Training loss for fold 2, epoch 121: 224.1353302001953\n","Validation elbo for fold 2, epoch 121: -186.186064633788\n","Training loss for fold 2, epoch 122: 224.34695459181262\n","Validation elbo for fold 2, epoch 122: -185.3495281154901\n","Training loss for fold 2, epoch 123: 224.3381588843561\n","Validation elbo for fold 2, epoch 123: -185.23678555541017\n","Training loss for fold 2, epoch 124: 224.0904004496913\n","Validation elbo for fold 2, epoch 124: -184.0068287548428\n","Training loss for fold 2, epoch 125: 224.07761949108493\n","Validation elbo for fold 2, epoch 125: -185.18198027382795\n","Training loss for fold 2, epoch 126: 223.95153660928048\n","Validation elbo for fold 2, epoch 126: -185.52991811366175\n","Training loss for fold 2, epoch 127: 224.20752494565903\n","Validation elbo for fold 2, epoch 127: -185.74774067774922\n","Training loss for fold 2, epoch 128: 224.08760882962136\n","Validation elbo for fold 2, epoch 128: -185.08385824160575\n","Training loss for fold 2, epoch 129: 224.29209210795742\n","Validation elbo for fold 2, epoch 129: -186.03527140525074\n","Training loss for fold 2, epoch 130: 224.05745426301033\n","Validation elbo for fold 2, epoch 130: -183.22381768410082\n","Training loss for fold 2, epoch 131: 224.14408726846017\n","Validation elbo for fold 2, epoch 131: -184.11005900959591\n","Training loss for fold 2, epoch 132: 224.4474598053963\n","Validation elbo for fold 2, epoch 132: -185.01376712895734\n","Training loss for fold 2, epoch 133: 224.46607823525704\n","Validation elbo for fold 2, epoch 133: -184.99965805680756\n","Training loss for fold 2, epoch 134: 224.0177993774414\n","Validation elbo for fold 2, epoch 134: -184.24405296221144\n","Training loss for fold 2, epoch 135: 224.5302990328881\n","Validation elbo for fold 2, epoch 135: -184.03596940062806\n","Training loss for fold 2, epoch 136: 224.2464818646831\n","Validation elbo for fold 2, epoch 136: -185.53276950729744\n","Training loss for fold 2, epoch 137: 224.21826959425402\n","Validation elbo for fold 2, epoch 137: -184.40406617698062\n","Training loss for fold 2, epoch 138: 224.4094713272587\n","Validation elbo for fold 2, epoch 138: -183.88880229900602\n","Training loss for fold 2, epoch 139: 224.13072992140246\n","Validation elbo for fold 2, epoch 139: -183.48742853395007\n","Training loss for fold 2, epoch 140: 224.27434638238722\n","Validation elbo for fold 2, epoch 140: -184.42478619493397\n","Training loss for fold 2, epoch 141: 224.22802734375\n","Validation elbo for fold 2, epoch 141: -183.97199361152775\n","Training loss for fold 2, epoch 142: 224.07715089859502\n","Validation elbo for fold 2, epoch 142: -184.68804239465288\n","Training loss for fold 2, epoch 143: 224.60952660345262\n","Validation elbo for fold 2, epoch 143: -185.1922578566606\n","Training loss for fold 2, epoch 144: 224.07796256772934\n","Validation elbo for fold 2, epoch 144: -184.20377963110116\n","Training loss for fold 2, epoch 145: 224.0289594588741\n","Validation elbo for fold 2, epoch 145: -183.9935363161524\n","Training loss for fold 2, epoch 146: 224.05816650390625\n","Validation elbo for fold 2, epoch 146: -184.50959175050252\n","Training loss for fold 2, epoch 147: 224.46853883804815\n","Validation elbo for fold 2, epoch 147: -185.5272985010824\n","Training loss for fold 2, epoch 148: 224.16107030068673\n","Validation elbo for fold 2, epoch 148: -184.49591303706228\n","Training loss for fold 2, epoch 149: 224.24247077203566\n","Validation elbo for fold 2, epoch 149: -184.6650244504616\n","Training loss for fold 2, epoch 150: 224.28987269247733\n","Validation elbo for fold 2, epoch 150: -185.1083869396292\n","Training loss for fold 2, epoch 151: 224.01805262411796\n","Validation elbo for fold 2, epoch 151: -184.53591759515928\n","Training loss for fold 2, epoch 152: 224.2185794461158\n","Validation elbo for fold 2, epoch 152: -184.6247237377251\n","Training loss for fold 2, epoch 153: 223.88418948265814\n","Validation elbo for fold 2, epoch 153: -184.21239099804257\n","Training loss for fold 2, epoch 154: 224.1581297843687\n","Validation elbo for fold 2, epoch 154: -183.64418908349467\n","Training loss for fold 2, epoch 155: 224.48261064098728\n","Validation elbo for fold 2, epoch 155: -184.44122993585648\n","Training loss for fold 2, epoch 156: 224.1868382115518\n","Validation elbo for fold 2, epoch 156: -185.56964164613942\n","Training loss for fold 2, epoch 157: 224.1613523421749\n","Validation elbo for fold 2, epoch 157: -184.0879606589702\n","Training loss for fold 2, epoch 158: 224.01543303458922\n","Validation elbo for fold 2, epoch 158: -185.14039970044809\n","Training loss for fold 2, epoch 159: 224.04974291401524\n","Validation elbo for fold 2, epoch 159: -184.0895703727576\n","Training loss for fold 2, epoch 160: 223.92444832094253\n","Validation elbo for fold 2, epoch 160: -184.09778266913395\n","Training loss for fold 2, epoch 161: 224.15119269586378\n","Validation elbo for fold 2, epoch 161: -185.79477667039504\n","Training loss for fold 2, epoch 162: 224.34074328022618\n","Validation elbo for fold 2, epoch 162: -185.33600211663673\n","Training loss for fold 2, epoch 163: 224.15372762372417\n","Validation elbo for fold 2, epoch 163: -186.79224121303503\n","Training loss for fold 2, epoch 164: 224.06486437397618\n","Validation elbo for fold 2, epoch 164: -184.74388629032524\n","Training loss for fold 2, epoch 165: 224.04753924954323\n","Validation elbo for fold 2, epoch 165: -184.9585812666848\n","Training loss for fold 2, epoch 166: 223.93719334756173\n","Validation elbo for fold 2, epoch 166: -183.93135773643047\n","Training loss for fold 2, epoch 167: 223.94857566587388\n","Validation elbo for fold 2, epoch 167: -183.71788887676968\n","Training loss for fold 2, epoch 168: 224.10958197809035\n","Validation elbo for fold 2, epoch 168: -185.06710689394112\n","Training loss for fold 2, epoch 169: 223.98772676529424\n","Validation elbo for fold 2, epoch 169: -184.66093614165877\n","Training loss for fold 2, epoch 170: 223.94439426545173\n","Validation elbo for fold 2, epoch 170: -183.6278002942588\n","Training loss for fold 2, epoch 171: 223.9260783041677\n","Validation elbo for fold 2, epoch 171: -185.1084163500612\n","Training loss for fold 2, epoch 172: 224.19808910739036\n","Validation elbo for fold 2, epoch 172: -184.0653612094905\n","Training loss for fold 2, epoch 173: 223.97064971923828\n","Validation elbo for fold 2, epoch 173: -183.90788564597577\n","Training loss for fold 2, epoch 174: 224.5016597624748\n","Validation elbo for fold 2, epoch 174: -185.7949520520796\n","Training loss for fold 2, epoch 175: 223.93650227208292\n","Validation elbo for fold 2, epoch 175: -183.8313788919869\n","Training loss for fold 2, epoch 176: 224.15629183861518\n","Validation elbo for fold 2, epoch 176: -185.63075156598225\n","Training loss for fold 2, epoch 177: 223.92111009167087\n","Validation elbo for fold 2, epoch 177: -184.51175176750468\n","Training loss for fold 2, epoch 178: 224.06995342623802\n","Validation elbo for fold 2, epoch 178: -184.6897571673975\n","Training loss for fold 2, epoch 179: 223.90039800828504\n","Validation elbo for fold 2, epoch 179: -183.447729038923\n","Training loss for fold 2, epoch 180: 224.07504198628087\n","Validation elbo for fold 2, epoch 180: -183.62616755227708\n","Training loss for fold 2, epoch 181: 223.92806268507434\n","Validation elbo for fold 2, epoch 181: -185.72334695754128\n","Training loss for fold 2, epoch 182: 223.95943278651083\n","Validation elbo for fold 2, epoch 182: -184.1691358882639\n","Training loss for fold 2, epoch 183: 223.96930817634828\n","Validation elbo for fold 2, epoch 183: -186.42080692871752\n","Training loss for fold 2, epoch 184: 224.00257873535156\n","Validation elbo for fold 2, epoch 184: -183.96570541898728\n","Training loss for fold 2, epoch 185: 224.02465771090598\n","Validation elbo for fold 2, epoch 185: -184.69338191039503\n","Training loss for fold 2, epoch 186: 223.83529589253087\n","Validation elbo for fold 2, epoch 186: -184.2172236061673\n","Training loss for fold 2, epoch 187: 224.0787872806672\n","Validation elbo for fold 2, epoch 187: -183.75718989178154\n","Training loss for fold 2, epoch 188: 224.15303827101184\n","Validation elbo for fold 2, epoch 188: -183.10389829797697\n","Training loss for fold 2, epoch 189: 224.3539812641759\n","Validation elbo for fold 2, epoch 189: -184.76931858662857\n","Training loss for fold 2, epoch 190: 224.35171656454764\n","Validation elbo for fold 2, epoch 190: -186.20094491980973\n","Training loss for fold 2, epoch 191: 223.88180197438885\n","Validation elbo for fold 2, epoch 191: -185.35452117520674\n","Training loss for fold 2, epoch 192: 224.0803456460276\n","Validation elbo for fold 2, epoch 192: -185.64585569714296\n","Training loss for fold 2, epoch 193: 224.26809889270413\n","Validation elbo for fold 2, epoch 193: -185.42805408040374\n","Training loss for fold 2, epoch 194: 224.07148299678678\n","Validation elbo for fold 2, epoch 194: -184.70532135308372\n","Training loss for fold 2, epoch 195: 224.32334703014743\n","Validation elbo for fold 2, epoch 195: -185.46104279768142\n","Training loss for fold 2, epoch 196: 224.05066853184854\n","Validation elbo for fold 2, epoch 196: -185.65505909457607\n","Training loss for fold 2, epoch 197: 224.0908670733052\n","Validation elbo for fold 2, epoch 197: -183.45136342612687\n","Training loss for fold 2, epoch 198: 223.890258050734\n","Validation elbo for fold 2, epoch 198: -183.2795191777895\n","Training loss for fold 2, epoch 199: 224.0424524122669\n","Validation elbo for fold 2, epoch 199: -184.7845996997015\n","Training loss for fold 2, epoch 200: 224.25777706023186\n","Validation elbo for fold 2, epoch 200: -184.93451747580093\n","Training loss for fold 2, epoch 201: 224.30650600310295\n","Validation elbo for fold 2, epoch 201: -186.0117833014338\n","Training loss for fold 2, epoch 202: 224.56209884151335\n","Validation elbo for fold 2, epoch 202: -186.18508820899225\n","Training loss for fold 2, epoch 203: 224.57557309058404\n","Validation elbo for fold 2, epoch 203: -185.90600270272725\n","Training loss for fold 2, epoch 204: 224.05445221931703\n","Validation elbo for fold 2, epoch 204: -183.50938759990797\n","Training loss for fold 2, epoch 205: 223.9570546304026\n","Validation elbo for fold 2, epoch 205: -183.94602478900487\n","Training loss for fold 2, epoch 206: 223.87737692556072\n","Validation elbo for fold 2, epoch 206: -185.7089092349981\n","Training loss for fold 2, epoch 207: 223.80301690870715\n","Validation elbo for fold 2, epoch 207: -183.929067785189\n","Training loss for fold 2, epoch 208: 224.15198984453755\n","Validation elbo for fold 2, epoch 208: -186.05134104221216\n","Training loss for fold 2, epoch 209: 224.21175532187186\n","Validation elbo for fold 2, epoch 209: -184.9123140809766\n","Training loss for fold 2, epoch 210: 224.23808116297567\n","Validation elbo for fold 2, epoch 210: -184.85112782099878\n","Training loss for fold 2, epoch 211: 224.0411573840726\n","Validation elbo for fold 2, epoch 211: -184.166687904046\n","Training loss for fold 2, epoch 212: 224.20265173142957\n","Validation elbo for fold 2, epoch 212: -185.0317582405608\n","Training loss for fold 2, epoch 213: 224.27000870243197\n","Validation elbo for fold 2, epoch 213: -185.76141907159564\n","Training loss for fold 2, epoch 214: 223.8845704601657\n","Validation elbo for fold 2, epoch 214: -184.80413881307098\n","Training loss for fold 2, epoch 215: 224.20587748865927\n","Validation elbo for fold 2, epoch 215: -184.9571025707328\n","Training loss for fold 2, epoch 216: 223.9457520515688\n","Validation elbo for fold 2, epoch 216: -184.4257638184377\n","Training loss for fold 2, epoch 217: 224.04486945367628\n","Validation elbo for fold 2, epoch 217: -184.41997867999413\n","Training loss for fold 2, epoch 218: 224.04498094128024\n","Validation elbo for fold 2, epoch 218: -183.26010981733324\n","Training loss for fold 2, epoch 219: 224.15181043071132\n","Validation elbo for fold 2, epoch 219: -184.34794863601743\n","Training loss for fold 2, epoch 220: 224.25490200904107\n","Validation elbo for fold 2, epoch 220: -185.35791946029735\n","Training loss for fold 2, epoch 221: 223.97654453400642\n","Validation elbo for fold 2, epoch 221: -184.16131872614767\n","Training loss for fold 2, epoch 222: 223.9816623810799\n","Validation elbo for fold 2, epoch 222: -185.72224240710887\n","Training loss for fold 2, epoch 223: 224.1229720577117\n","Validation elbo for fold 2, epoch 223: -184.9734418375583\n","Training loss for fold 2, epoch 224: 224.05980682373047\n","Validation elbo for fold 2, epoch 224: -184.40428123689287\n","Training loss for fold 2, epoch 225: 223.9661884923135\n","Validation elbo for fold 2, epoch 225: -184.96231878377768\n","Training loss for fold 2, epoch 226: 224.0203338130828\n","Validation elbo for fold 2, epoch 226: -185.453904670325\n","Training loss for fold 2, epoch 227: 224.07458643759452\n","Validation elbo for fold 2, epoch 227: -185.86710641689166\n","Training loss for fold 2, epoch 228: 224.0042244695848\n","Validation elbo for fold 2, epoch 228: -184.71450114763354\n","Training loss for fold 2, epoch 229: 223.8757796748992\n","Validation elbo for fold 2, epoch 229: -183.9329669346996\n","Training loss for fold 2, epoch 230: 224.00824442217427\n","Validation elbo for fold 2, epoch 230: -184.128726974744\n","Training loss for fold 2, epoch 231: 223.8177696966356\n","Validation elbo for fold 2, epoch 231: -184.7330316526306\n","Training loss for fold 2, epoch 232: 223.93115578928303\n","Validation elbo for fold 2, epoch 232: -184.90342328561522\n","Training loss for fold 2, epoch 233: 224.1695295764554\n","Validation elbo for fold 2, epoch 233: -184.49767995091105\n","Training loss for fold 2, epoch 234: 223.9790723246913\n","Validation elbo for fold 2, epoch 234: -184.683018899802\n","Training loss for fold 2, epoch 235: 224.0535384147398\n","Validation elbo for fold 2, epoch 235: -185.57525238882883\n","Training loss for fold 2, epoch 236: 223.73736572265625\n","Validation elbo for fold 2, epoch 236: -184.58160336779093\n","Training loss for fold 2, epoch 237: 223.85285211378527\n","Validation elbo for fold 2, epoch 237: -184.6250936041919\n","Training loss for fold 2, epoch 238: 224.0671389179845\n","Validation elbo for fold 2, epoch 238: -185.35276700134006\n","Training loss for fold 2, epoch 239: 224.01236134190714\n","Validation elbo for fold 2, epoch 239: -185.33585449772062\n","Training loss for fold 2, epoch 240: 223.98625921433973\n","Validation elbo for fold 2, epoch 240: -185.72623467832844\n","Training loss for fold 2, epoch 241: 224.22752724924396\n","Validation elbo for fold 2, epoch 241: -184.50578711364886\n","Training loss for fold 2, epoch 242: 224.0791768720073\n","Validation elbo for fold 2, epoch 242: -183.19249104912342\n","Training loss for fold 2, epoch 243: 223.87613358036165\n","Validation elbo for fold 2, epoch 243: -185.065646203017\n","Training loss for fold 2, epoch 244: 224.09407831007434\n","Validation elbo for fold 2, epoch 244: -184.3905519480307\n","Training loss for fold 2, epoch 245: 223.92227049796813\n","Validation elbo for fold 2, epoch 245: -183.46921200994456\n","Training loss for fold 2, epoch 246: 223.95330539826423\n","Validation elbo for fold 2, epoch 246: -184.51238517645896\n","Training loss for fold 2, epoch 247: 223.98468386742377\n","Validation elbo for fold 2, epoch 247: -184.23756122905866\n","Training loss for fold 2, epoch 248: 224.13004401422316\n","Validation elbo for fold 2, epoch 248: -184.6976247713049\n","Training loss for fold 2, epoch 249: 223.91397168559413\n","Validation elbo for fold 2, epoch 249: -184.9489450230779\n","Training loss for fold 2, epoch 250: 224.04160874889743\n","Validation elbo for fold 2, epoch 250: -184.59826651836798\n","Training loss for fold 2, epoch 251: 223.83780916275518\n","Validation elbo for fold 2, epoch 251: -184.95748338610727\n","Training loss for fold 2, epoch 252: 223.99305823541457\n","Validation elbo for fold 2, epoch 252: -184.68383873088823\n","Training loss for fold 2, epoch 253: 224.04434130268712\n","Validation elbo for fold 2, epoch 253: -185.33392926750764\n","Training loss for fold 2, epoch 254: 223.88654647334928\n","Validation elbo for fold 2, epoch 254: -186.38989696850734\n","Training loss for fold 2, epoch 255: 224.08868703534526\n","Validation elbo for fold 2, epoch 255: -185.4872075770332\n","Training loss for fold 2, epoch 256: 223.9181890180034\n","Validation elbo for fold 2, epoch 256: -184.8439074675216\n","Training loss for fold 2, epoch 257: 223.85631881221647\n","Validation elbo for fold 2, epoch 257: -184.31873996615118\n","Training loss for fold 2, epoch 258: 223.9848128288023\n","Validation elbo for fold 2, epoch 258: -184.073246031597\n","Training loss for fold 2, epoch 259: 224.21221899217176\n","Validation elbo for fold 2, epoch 259: -183.79451372402852\n","Training loss for fold 2, epoch 260: 224.00016366281818\n","Validation elbo for fold 2, epoch 260: -184.44203495805993\n","Training loss for fold 2, epoch 261: 224.13755946005546\n","Validation elbo for fold 2, epoch 261: -184.99291896093152\n","Training loss for fold 2, epoch 262: 224.3484570903163\n","Validation elbo for fold 2, epoch 262: -185.42169303636229\n","Training loss for fold 2, epoch 263: 224.26357810728013\n","Validation elbo for fold 2, epoch 263: -183.8474044955504\n","Training loss for fold 2, epoch 264: 223.59515676190776\n","Validation elbo for fold 2, epoch 264: -184.94083003570285\n","Training loss for fold 2, epoch 265: 224.2357672414472\n","Validation elbo for fold 2, epoch 265: -185.56788537365648\n","Training loss for fold 2, epoch 266: 223.8844705397083\n","Validation elbo for fold 2, epoch 266: -184.44823264171987\n","Training loss for fold 2, epoch 267: 223.98162078857422\n","Validation elbo for fold 2, epoch 267: -184.58147754876163\n","Training loss for fold 2, epoch 268: 224.00238357051725\n","Validation elbo for fold 2, epoch 268: -185.32597954717136\n","Training loss for fold 2, epoch 269: 223.74348523539882\n","Validation elbo for fold 2, epoch 269: -183.8300126399593\n","Training loss for fold 2, epoch 270: 223.96305207283265\n","Validation elbo for fold 2, epoch 270: -185.29292611845284\n","Training loss for fold 2, epoch 271: 223.9902326522335\n","Validation elbo for fold 2, epoch 271: -185.13133990420647\n","Training loss for fold 2, epoch 272: 223.92205859768777\n","Validation elbo for fold 2, epoch 272: -184.79115886854424\n","Training loss for fold 2, epoch 273: 223.9710934546686\n","Validation elbo for fold 2, epoch 273: -183.87255308632876\n","Training loss for fold 2, epoch 274: 223.88172641877205\n","Validation elbo for fold 2, epoch 274: -182.79297696003343\n","Training loss for fold 2, epoch 275: 224.0775412282636\n","Validation elbo for fold 2, epoch 275: -184.55683097171323\n","Training loss for fold 2, epoch 276: 224.07149973223287\n","Validation elbo for fold 2, epoch 276: -185.41994196609193\n","Training loss for fold 2, epoch 277: 224.20542956936745\n","Validation elbo for fold 2, epoch 277: -184.83995464356371\n","Training loss for fold 2, epoch 278: 223.91731532927483\n","Validation elbo for fold 2, epoch 278: -185.11932455243874\n","Training loss for fold 2, epoch 279: 224.1536855389995\n","Validation elbo for fold 2, epoch 279: -185.0212691110318\n","Training loss for fold 2, epoch 280: 223.8820062452747\n","Validation elbo for fold 2, epoch 280: -184.56513698023872\n","Training loss for fold 2, epoch 281: 223.9347873810799\n","Validation elbo for fold 2, epoch 281: -183.94771516466483\n","Training loss for fold 2, epoch 282: 224.06232058617377\n","Validation elbo for fold 2, epoch 282: -185.05838237090296\n","Training loss for fold 2, epoch 283: 223.84954981650077\n","Validation elbo for fold 2, epoch 283: -185.58450647304406\n","Training loss for fold 2, epoch 284: 224.05587128669984\n","Validation elbo for fold 2, epoch 284: -184.8216822702482\n","Training loss for fold 2, epoch 285: 224.02933969805318\n","Validation elbo for fold 2, epoch 285: -185.8457889928376\n","Training loss for fold 2, epoch 286: 223.82190581290953\n","Validation elbo for fold 2, epoch 286: -183.62622027793114\n","Training loss for fold 2, epoch 287: 224.14806390577746\n","Validation elbo for fold 2, epoch 287: -186.03479381028936\n","Training loss for fold 2, epoch 288: 224.11154150193738\n","Validation elbo for fold 2, epoch 288: -184.34476490588963\n","Training loss for fold 2, epoch 289: 224.0623072962607\n","Validation elbo for fold 2, epoch 289: -185.7690368865246\n","Training loss for fold 2, epoch 290: 223.92056274414062\n","Validation elbo for fold 2, epoch 290: -183.9451678783215\n","Training loss for fold 2, epoch 291: 223.9621102117723\n","Validation elbo for fold 2, epoch 291: -184.28534618099204\n","Training loss for fold 2, epoch 292: 224.04715901036417\n","Validation elbo for fold 2, epoch 292: -184.61291202176534\n","Training loss for fold 2, epoch 293: 223.96814432451802\n","Validation elbo for fold 2, epoch 293: -184.2738464158353\n","Training loss for fold 2, epoch 294: 223.8169427687122\n","Validation elbo for fold 2, epoch 294: -184.74833479157587\n","Training loss for fold 2, epoch 295: 223.785766355453\n","Validation elbo for fold 2, epoch 295: -183.26474735160917\n","Training loss for fold 2, epoch 296: 224.73480815272177\n","Validation elbo for fold 2, epoch 296: -185.6100561415498\n","Training loss for fold 2, epoch 297: 224.03811817784464\n","Validation elbo for fold 2, epoch 297: -184.95210156848685\n","Training loss for fold 2, epoch 298: 223.84902732603013\n","Validation elbo for fold 2, epoch 298: -185.3755881566731\n","Training loss for fold 2, epoch 299: 224.11512731736707\n","Validation elbo for fold 2, epoch 299: -184.66287574801905\n","Training loss for fold 2, epoch 300: 223.7605206889491\n","Validation elbo for fold 2, epoch 300: -185.0034137353412\n","Training loss for fold 2, epoch 301: 224.0321544524162\n","Validation elbo for fold 2, epoch 301: -184.66138136392442\n","Training loss for fold 2, epoch 302: 223.94486433459866\n","Validation elbo for fold 2, epoch 302: -185.8564358349676\n","Training loss for fold 2, epoch 303: 223.99488978232108\n","Validation elbo for fold 2, epoch 303: -183.6249333076002\n","Training loss for fold 2, epoch 304: 224.0742903678648\n","Validation elbo for fold 2, epoch 304: -185.20901595142556\n","Training loss for fold 2, epoch 305: 223.82666261734502\n","Validation elbo for fold 2, epoch 305: -184.2891468887099\n","Training loss for fold 2, epoch 306: 223.92122994699787\n","Validation elbo for fold 2, epoch 306: -185.90722889807108\n","Training loss for fold 2, epoch 307: 224.0343226771201\n","Validation elbo for fold 2, epoch 307: -185.20217583658015\n","Training loss for fold 2, epoch 308: 223.9118206885553\n","Validation elbo for fold 2, epoch 308: -184.1654562444272\n","Training loss for fold 2, epoch 309: 223.92340407832975\n","Validation elbo for fold 2, epoch 309: -184.1985671238544\n","Training loss for fold 2, epoch 310: 224.16557016680318\n","Validation elbo for fold 2, epoch 310: -183.99939439721885\n","Training loss for fold 2, epoch 311: 223.7558121219758\n","Validation elbo for fold 2, epoch 311: -183.48539187064216\n","Training loss for fold 2, epoch 312: 224.40585351759387\n","Validation elbo for fold 2, epoch 312: -184.43804474104087\n","Training loss for fold 2, epoch 313: 224.1484837685862\n","Validation elbo for fold 2, epoch 313: -183.65621958158\n","Training loss for fold 2, epoch 314: 224.12313498220135\n","Validation elbo for fold 2, epoch 314: -184.8840179402455\n","Training loss for fold 2, epoch 315: 223.90933424426663\n","Validation elbo for fold 2, epoch 315: -184.38262475316782\n","Training loss for fold 2, epoch 316: 223.76666751984627\n","Validation elbo for fold 2, epoch 316: -184.42573347639495\n","Training loss for fold 2, epoch 317: 223.81892542685233\n","Validation elbo for fold 2, epoch 317: -184.26325022280344\n","Training loss for fold 2, epoch 318: 223.894896722609\n","Validation elbo for fold 2, epoch 318: -184.7032154791262\n","Training loss for fold 2, epoch 319: 224.1991673131143\n","Validation elbo for fold 2, epoch 319: -185.0893744579556\n","Training loss for fold 2, epoch 320: 223.9350150323683\n","Validation elbo for fold 2, epoch 320: -185.07502422886932\n","Training loss for fold 2, epoch 321: 224.2651593608241\n","Validation elbo for fold 2, epoch 321: -185.19033619345294\n","Training loss for fold 2, epoch 322: 224.19285263553743\n","Validation elbo for fold 2, epoch 322: -184.6382650068158\n","Training loss for fold 2, epoch 323: 224.2147425989951\n","Validation elbo for fold 2, epoch 323: -184.4114681847035\n","Training loss for fold 2, epoch 324: 224.36309248401272\n","Validation elbo for fold 2, epoch 324: -184.68441766070055\n","Training loss for fold 2, epoch 325: 223.93057595529865\n","Validation elbo for fold 2, epoch 325: -183.99062010056377\n","Training loss for fold 2, epoch 326: 224.0702635242093\n","Validation elbo for fold 2, epoch 326: -185.0113740168775\n","Training loss for fold 2, epoch 327: 223.76170915172946\n","Validation elbo for fold 2, epoch 327: -185.75628808754098\n","Training loss for fold 2, epoch 328: 223.88643523185485\n","Validation elbo for fold 2, epoch 328: -185.4631665750939\n","Training loss for fold 2, epoch 329: 223.982540745889\n","Validation elbo for fold 2, epoch 329: -184.42529357183335\n","Training loss for fold 2, epoch 330: 224.03175772390057\n","Validation elbo for fold 2, epoch 330: -184.84697099781158\n","Training loss for fold 2, epoch 331: 224.02391396799396\n","Validation elbo for fold 2, epoch 331: -184.67883851056192\n","Training loss for fold 2, epoch 332: 223.858277105516\n","Validation elbo for fold 2, epoch 332: -185.76377873950358\n","Training loss for fold 2, epoch 333: 224.09008641396798\n","Validation elbo for fold 2, epoch 333: -184.4357959617647\n","Training loss for fold 2, epoch 334: 223.98531415385585\n","Validation elbo for fold 2, epoch 334: -185.6498547310215\n","Training loss for fold 2, epoch 335: 224.03646161479335\n","Validation elbo for fold 2, epoch 335: -184.90337858385732\n","Training loss for fold 2, epoch 336: 224.01427312051095\n","Validation elbo for fold 2, epoch 336: -184.1995126834436\n","Training loss for fold 2, epoch 337: 224.0330579203944\n","Validation elbo for fold 2, epoch 337: -184.24602299790496\n","Training loss for fold 2, epoch 338: 223.722539347987\n","Validation elbo for fold 2, epoch 338: -183.98155491706217\n","Training loss for fold 2, epoch 339: 223.82723679081087\n","Validation elbo for fold 2, epoch 339: -183.84670301285706\n","Training loss for fold 2, epoch 340: 224.27798781856413\n","Validation elbo for fold 2, epoch 340: -185.43578608107373\n","Training loss for fold 2, epoch 341: 223.96921071698588\n","Validation elbo for fold 2, epoch 341: -184.2828848439843\n","Training loss for fold 2, epoch 342: 224.23420272334928\n","Validation elbo for fold 2, epoch 342: -184.58574854609736\n","Training loss for fold 2, epoch 343: 223.83258721136278\n","Validation elbo for fold 2, epoch 343: -183.4691640540006\n","Training loss for fold 2, epoch 344: 223.86283431514616\n","Validation elbo for fold 2, epoch 344: -185.31041364073485\n","Training loss for fold 2, epoch 345: 223.87090572234123\n","Validation elbo for fold 2, epoch 345: -184.28210720234875\n","Training loss for fold 2, epoch 346: 223.8617189468876\n","Validation elbo for fold 2, epoch 346: -183.849307347708\n","Training loss for fold 2, epoch 347: 223.87396338678175\n","Validation elbo for fold 2, epoch 347: -184.70455129115882\n","Training loss for fold 2, epoch 348: 223.89772279800908\n","Validation elbo for fold 2, epoch 348: -184.92671956057777\n","Training loss for fold 2, epoch 349: 224.13402778871597\n","Validation elbo for fold 2, epoch 349: -185.16690165919437\n","Training loss for fold 2, epoch 350: 223.90974475491433\n","Validation elbo for fold 2, epoch 350: -185.14448461532652\n","Training loss for fold 2, epoch 351: 223.89688725625314\n","Validation elbo for fold 2, epoch 351: -185.11729950671048\n","Training loss for fold 2, epoch 352: 224.02445540889616\n","Validation elbo for fold 2, epoch 352: -184.91862764631873\n","Training loss for fold 2, epoch 353: 224.26820422757058\n","Validation elbo for fold 2, epoch 353: -184.1428292013035\n","Training loss for fold 2, epoch 354: 224.08694802561115\n","Validation elbo for fold 2, epoch 354: -184.37736254167856\n","Training loss for fold 2, epoch 355: 223.8860109390751\n","Validation elbo for fold 2, epoch 355: -184.9609529324723\n","Training loss for fold 2, epoch 356: 224.01645020515687\n","Validation elbo for fold 2, epoch 356: -184.71728796413214\n","Training loss for fold 2, epoch 357: 224.17601185460245\n","Validation elbo for fold 2, epoch 357: -184.30344163777835\n","Training loss for fold 2, epoch 358: 223.73409320462136\n","Validation elbo for fold 2, epoch 358: -184.06805377550714\n","Training loss for fold 2, epoch 359: 224.44517024870842\n","Validation elbo for fold 2, epoch 359: -184.11556911123205\n","Training loss for fold 2, epoch 360: 224.14835210000314\n","Validation elbo for fold 2, epoch 360: -184.3129911616588\n","Training loss for fold 2, epoch 361: 223.94001327022428\n","Validation elbo for fold 2, epoch 361: -185.05668323194067\n","Training loss for fold 2, epoch 362: 224.00305618778353\n","Validation elbo for fold 2, epoch 362: -184.86809296341102\n","Training loss for fold 2, epoch 363: 223.91640915409212\n","Validation elbo for fold 2, epoch 363: -184.89583561658787\n","Training loss for fold 2, epoch 364: 224.20569216820502\n","Validation elbo for fold 2, epoch 364: -185.42795927304655\n","Training loss for fold 2, epoch 365: 223.87093673213835\n","Validation elbo for fold 2, epoch 365: -184.90928002664953\n","Training loss for fold 2, epoch 366: 223.9472624255765\n","Validation elbo for fold 2, epoch 366: -185.11576659787312\n","Training loss for fold 2, epoch 367: 223.9791727373677\n","Validation elbo for fold 2, epoch 367: -183.12654753863217\n","Training loss for fold 2, epoch 368: 223.93897985642957\n","Validation elbo for fold 2, epoch 368: -185.86444688388104\n","Training loss for fold 2, epoch 369: 224.0336862379505\n","Validation elbo for fold 2, epoch 369: -183.4777983495226\n","Training loss for fold 2, epoch 370: 223.94169419811618\n","Validation elbo for fold 2, epoch 370: -184.52293694762704\n","Training loss for fold 2, epoch 371: 224.2071060672883\n","Validation elbo for fold 2, epoch 371: -185.49320874183746\n","Training loss for fold 2, epoch 372: 224.29374325659967\n","Validation elbo for fold 2, epoch 372: -183.79300038582528\n","Training loss for fold 2, epoch 373: 223.96534778225808\n","Validation elbo for fold 2, epoch 373: -185.14574622885686\n","Training loss for fold 2, epoch 374: 223.79723308932395\n","Validation elbo for fold 2, epoch 374: -185.03970291724556\n","Training loss for fold 2, epoch 375: 224.26095433388986\n","Validation elbo for fold 2, epoch 375: -184.6102466778715\n","Training loss for fold 2, epoch 376: 224.11000208700858\n","Validation elbo for fold 2, epoch 376: -184.60706560100303\n","Training loss for fold 2, epoch 377: 224.20110690209174\n","Validation elbo for fold 2, epoch 377: -184.82720628734108\n","Training loss for fold 2, epoch 378: 223.8139175907258\n","Validation elbo for fold 2, epoch 378: -184.7309089230303\n","Training loss for fold 2, epoch 379: 223.95501044488722\n","Validation elbo for fold 2, epoch 379: -184.26746401402352\n","Training loss for fold 2, epoch 380: 223.90447431995022\n","Validation elbo for fold 2, epoch 380: -184.99848259608825\n","Training loss for fold 2, epoch 381: 223.935055640436\n","Validation elbo for fold 2, epoch 381: -185.14372986645992\n","Training loss for fold 2, epoch 382: 223.83623799970073\n","Validation elbo for fold 2, epoch 382: -183.21181968123608\n","Training loss for fold 2, epoch 383: 223.89993556853264\n","Validation elbo for fold 2, epoch 383: -183.8724284832311\n","Training loss for fold 2, epoch 384: 223.83283356697328\n","Validation elbo for fold 2, epoch 384: -184.12607555422824\n","Training loss for fold 2, epoch 385: 223.86734156454764\n","Validation elbo for fold 2, epoch 385: -185.4206665644363\n","Training loss for fold 2, epoch 386: 224.20370581842238\n","Validation elbo for fold 2, epoch 386: -185.46894259652632\n","Training loss for fold 2, epoch 387: 224.22052075785976\n","Validation elbo for fold 2, epoch 387: -185.00470153302507\n","Training loss for fold 2, epoch 388: 223.9787814232611\n","Validation elbo for fold 2, epoch 388: -184.86189089642352\n","Training loss for fold 2, epoch 389: 224.00312066847277\n","Validation elbo for fold 2, epoch 389: -184.7901024351894\n","Training loss for fold 2, epoch 390: 223.98690918953187\n","Validation elbo for fold 2, epoch 390: -183.6085480187598\n","Training loss for fold 2, epoch 391: 224.13458251953125\n","Validation elbo for fold 2, epoch 391: -185.22803252128682\n","Training loss for fold 2, epoch 392: 223.89247205180507\n","Validation elbo for fold 2, epoch 392: -184.8817451804823\n","Training loss for fold 2, epoch 393: 223.7620330318328\n","Validation elbo for fold 2, epoch 393: -185.01603470186294\n","Training loss for fold 2, epoch 394: 223.94942006757182\n","Validation elbo for fold 2, epoch 394: -185.16862746787442\n","Training loss for fold 2, epoch 395: 223.9672369187878\n","Validation elbo for fold 2, epoch 395: -185.55171810444844\n","Training loss for fold 2, epoch 396: 224.07061201526272\n","Validation elbo for fold 2, epoch 396: -184.6796163596195\n","Training loss for fold 2, epoch 397: 224.0228716942572\n","Validation elbo for fold 2, epoch 397: -184.84468353214936\n","Training loss for fold 2, epoch 398: 223.74886912684286\n","Validation elbo for fold 2, epoch 398: -184.03126397326045\n","Training loss for fold 2, epoch 399: 223.98255625078755\n","Validation elbo for fold 2, epoch 399: -185.38484650321976\n","Training loss for fold 2, epoch 400: 223.967769499748\n","Validation elbo for fold 2, epoch 400: -184.6408818707562\n","Training loss for fold 2, epoch 401: 223.9019044445407\n","Validation elbo for fold 2, epoch 401: -184.9676886309262\n","Training loss for fold 2, epoch 402: 223.8561772992534\n","Validation elbo for fold 2, epoch 402: -185.93445534222207\n","Training loss for fold 2, epoch 403: 223.91490173339844\n","Validation elbo for fold 2, epoch 403: -184.6244144265871\n","Training loss for fold 2, epoch 404: 223.94597527288622\n","Validation elbo for fold 2, epoch 404: -184.42607352771188\n","Training loss for fold 2, epoch 405: 223.99854057065903\n","Validation elbo for fold 2, epoch 405: -184.09566750193704\n","Training loss for fold 2, epoch 406: 224.00345168575163\n","Validation elbo for fold 2, epoch 406: -184.795619304857\n","Training loss for fold 2, epoch 407: 223.9740962366904\n","Validation elbo for fold 2, epoch 407: -184.58019520331544\n","Training loss for fold 2, epoch 408: 224.0728302001953\n","Validation elbo for fold 2, epoch 408: -185.18992620832074\n","Training loss for fold 2, epoch 409: 224.09415189681513\n","Validation elbo for fold 2, epoch 409: -185.41263311313418\n","Training loss for fold 2, epoch 410: 224.00124211465157\n","Validation elbo for fold 2, epoch 410: -183.69885673329964\n","Training loss for fold 2, epoch 411: 224.0305631083827\n","Validation elbo for fold 2, epoch 411: -184.8106736694013\n","Training loss for fold 2, epoch 412: 224.1609610280683\n","Validation elbo for fold 2, epoch 412: -185.23910741315592\n","Training loss for fold 2, epoch 413: 224.05118560791016\n","Validation elbo for fold 2, epoch 413: -183.85805785795816\n","Training loss for fold 2, epoch 414: 223.93008767404865\n","Validation elbo for fold 2, epoch 414: -184.24319919195662\n","Training loss for fold 2, epoch 415: 223.81443392845893\n","Validation elbo for fold 2, epoch 415: -183.7345468191357\n","Training loss for fold 2, epoch 416: 223.9224142259167\n","Validation elbo for fold 2, epoch 416: -185.64827674784058\n","Training loss for fold 2, epoch 417: 223.96729007844002\n","Validation elbo for fold 2, epoch 417: -185.06961306114476\n","Training loss for fold 2, epoch 418: 223.8979725991526\n","Validation elbo for fold 2, epoch 418: -184.16070662472828\n","Training loss for fold 2, epoch 419: 223.89886302332724\n","Validation elbo for fold 2, epoch 419: -184.8590549146141\n","Training loss for fold 2, epoch 420: 224.02196822627897\n","Validation elbo for fold 2, epoch 420: -184.17248541722665\n","Training loss for fold 2, epoch 421: 223.61928410683907\n","Validation elbo for fold 2, epoch 421: -183.12367801203834\n","Training loss for fold 2, epoch 422: 223.90468843521612\n","Validation elbo for fold 2, epoch 422: -185.3384581483275\n","Training loss for fold 2, epoch 423: 223.9786903627457\n","Validation elbo for fold 2, epoch 423: -184.14750162944307\n","Training loss for fold 2, epoch 424: 223.82824140979397\n","Validation elbo for fold 2, epoch 424: -184.5647607915471\n","Training loss for fold 2, epoch 425: 223.969972672001\n","Validation elbo for fold 2, epoch 425: -184.20483266493983\n","Training loss for fold 2, epoch 426: 224.01928858603202\n","Validation elbo for fold 2, epoch 426: -184.4989632632288\n","Training loss for fold 2, epoch 427: 224.04197668260144\n","Validation elbo for fold 2, epoch 427: -184.55489146863522\n","Training loss for fold 2, epoch 428: 223.71056981240548\n","Validation elbo for fold 2, epoch 428: -184.7770229508275\n","Training loss for fold 2, epoch 429: 224.04015128843247\n","Validation elbo for fold 2, epoch 429: -186.00679040047015\n","Training loss for fold 2, epoch 430: 224.21102486887287\n","Validation elbo for fold 2, epoch 430: -184.49801032259845\n","Training loss for fold 2, epoch 431: 223.92871807467552\n","Validation elbo for fold 2, epoch 431: -185.40032496444826\n","Training loss for fold 2, epoch 432: 223.90409481909967\n","Validation elbo for fold 2, epoch 432: -184.10809463897914\n","Training loss for fold 2, epoch 433: 224.01008482902282\n","Validation elbo for fold 2, epoch 433: -184.21773359998912\n","Training loss for fold 2, epoch 434: 224.0490230437248\n","Validation elbo for fold 2, epoch 434: -183.56930074285125\n","Training loss for fold 2, epoch 435: 223.6969203333701\n","Validation elbo for fold 2, epoch 435: -184.69733364101398\n","Training loss for fold 2, epoch 436: 223.9038098242975\n","Validation elbo for fold 2, epoch 436: -183.54166335448303\n","Training loss for fold 2, epoch 437: 223.8711722589308\n","Validation elbo for fold 2, epoch 437: -184.64853991737058\n","Training loss for fold 2, epoch 438: 224.08065722065587\n","Validation elbo for fold 2, epoch 438: -184.31649706911344\n","Training loss for fold 2, epoch 439: 223.86370234335624\n","Validation elbo for fold 2, epoch 439: -183.89043621942005\n","Training loss for fold 2, epoch 440: 224.12237598050027\n","Validation elbo for fold 2, epoch 440: -184.53483095980457\n","Training loss for fold 2, epoch 441: 223.94408121416646\n","Validation elbo for fold 2, epoch 441: -184.72305761386644\n","Training loss for fold 2, epoch 442: 224.16107079290575\n","Validation elbo for fold 2, epoch 442: -184.64035823147591\n","Training loss for fold 2, epoch 443: 223.79983422064012\n","Validation elbo for fold 2, epoch 443: -185.32900447789416\n","Training loss for fold 2, epoch 444: 224.10110695131362\n","Validation elbo for fold 2, epoch 444: -185.19035812929064\n","Training loss for fold 2, epoch 445: 223.66240913637222\n","Validation elbo for fold 2, epoch 445: -183.8701661334612\n","Training loss for fold 2, epoch 446: 223.74867863808907\n","Validation elbo for fold 2, epoch 446: -185.0573212067668\n","Training loss for fold 2, epoch 447: 224.00800323486328\n","Validation elbo for fold 2, epoch 447: -183.31455525333297\n","Training loss for fold 2, epoch 448: 224.05389921126826\n","Validation elbo for fold 2, epoch 448: -183.96036924690594\n","Training loss for fold 2, epoch 449: 223.87384230090726\n","Validation elbo for fold 2, epoch 449: -184.16327395253455\n","Training loss for fold 2, epoch 450: 223.93166006765057\n","Validation elbo for fold 2, epoch 450: -184.3141037974807\n","Training loss for fold 2, epoch 451: 224.0304937055034\n","Validation elbo for fold 2, epoch 451: -184.48178400429586\n","Training loss for fold 2, epoch 452: 223.80655645555066\n","Validation elbo for fold 2, epoch 452: -183.72693986187164\n","Training loss for fold 2, epoch 453: 224.20587355090726\n","Validation elbo for fold 2, epoch 453: -184.78511628080926\n","Training loss for fold 2, epoch 454: 223.9144776867282\n","Validation elbo for fold 2, epoch 454: -183.44658909374152\n","Training loss for fold 2, epoch 455: 223.91571463308026\n","Validation elbo for fold 2, epoch 455: -184.24248659111308\n","Training loss for fold 2, epoch 456: 223.9177497125441\n","Validation elbo for fold 2, epoch 456: -184.79561015947257\n","Training loss for fold 2, epoch 457: 224.13663679553616\n","Validation elbo for fold 2, epoch 457: -184.91846613444187\n","Training loss for fold 2, epoch 458: 223.78350731634325\n","Validation elbo for fold 2, epoch 458: -183.49487592037988\n","Training loss for fold 2, epoch 459: 224.46023633403163\n","Validation elbo for fold 2, epoch 459: -183.24276669419598\n","Training loss for fold 2, epoch 460: 224.3008816626764\n","Validation elbo for fold 2, epoch 460: -185.2257207290392\n","Training loss for fold 2, epoch 461: 223.97493891562186\n","Validation elbo for fold 2, epoch 461: -184.77285291779762\n","Training loss for fold 2, epoch 462: 223.88241675592238\n","Validation elbo for fold 2, epoch 462: -183.56739564682618\n","Training loss for fold 2, epoch 463: 224.27686235981602\n","Validation elbo for fold 2, epoch 463: -185.238660186191\n","Training loss for fold 2, epoch 464: 223.82755697927166\n","Validation elbo for fold 2, epoch 464: -183.10508940396832\n","Training loss for fold 2, epoch 465: 224.7408927179152\n","Validation elbo for fold 2, epoch 465: -184.71346657571036\n","Training loss for fold 2, epoch 466: 224.38856752457158\n","Validation elbo for fold 2, epoch 466: -184.46539504786796\n","Training loss for fold 2, epoch 467: 224.49952919252456\n","Validation elbo for fold 2, epoch 467: -185.99864957193194\n","Training loss for fold 2, epoch 468: 224.11243906328755\n","Validation elbo for fold 2, epoch 468: -185.72208787070886\n","Training loss for fold 2, epoch 469: 223.98673051403415\n","Validation elbo for fold 2, epoch 469: -184.8199913051568\n","Training loss for fold 2, epoch 470: 223.76461127496535\n","Validation elbo for fold 2, epoch 470: -183.32444823385885\n","Training loss for fold 2, epoch 471: 224.09095641105407\n","Validation elbo for fold 2, epoch 471: -185.40821019784318\n","Training loss for fold 2, epoch 472: 223.84761662637032\n","Validation elbo for fold 2, epoch 472: -184.07604714232133\n","Training loss for fold 2, epoch 473: 224.00912598640687\n","Validation elbo for fold 2, epoch 473: -184.47529090988564\n","Training loss for fold 2, epoch 474: 223.81438938263923\n","Validation elbo for fold 2, epoch 474: -184.4901761964927\n","Training loss for fold 2, epoch 475: 224.237915777391\n","Validation elbo for fold 2, epoch 475: -185.29699823813564\n","Training loss for fold 2, epoch 476: 223.89944777950163\n","Validation elbo for fold 2, epoch 476: -184.59763124738123\n","Training loss for fold 2, epoch 477: 224.05383793000252\n","Validation elbo for fold 2, epoch 477: -183.50413607089519\n","Training loss for fold 2, epoch 478: 224.43484275571763\n","Validation elbo for fold 2, epoch 478: -185.27081971296866\n","Training loss for fold 2, epoch 479: 223.91748735981602\n","Validation elbo for fold 2, epoch 479: -184.66206180471943\n","Training loss for fold 2, epoch 480: 224.05251779863912\n","Validation elbo for fold 2, epoch 480: -184.30909852886924\n","Training loss for fold 2, epoch 481: 224.04734999133694\n","Validation elbo for fold 2, epoch 481: -185.18759326710557\n","Training loss for fold 2, epoch 482: 223.84706829440208\n","Validation elbo for fold 2, epoch 482: -184.3101209963395\n","Training loss for fold 2, epoch 483: 223.99471824399888\n","Validation elbo for fold 2, epoch 483: -185.64181388413158\n","Training loss for fold 2, epoch 484: 223.66257772138042\n","Validation elbo for fold 2, epoch 484: -184.93356711765207\n","Training loss for fold 2, epoch 485: 223.80288671678113\n","Validation elbo for fold 2, epoch 485: -184.0960936416484\n","Training loss for fold 2, epoch 486: 223.8931387624433\n","Validation elbo for fold 2, epoch 486: -184.66948329579117\n","Training loss for fold 2, epoch 487: 224.11729554207093\n","Validation elbo for fold 2, epoch 487: -185.11400316141516\n","Training loss for fold 2, epoch 488: 223.94849986414755\n","Validation elbo for fold 2, epoch 488: -183.99677474013916\n","Training loss for fold 2, epoch 489: 223.95765562980407\n","Validation elbo for fold 2, epoch 489: -184.09887890784162\n","Training loss for fold 2, epoch 490: 224.15786718553113\n","Validation elbo for fold 2, epoch 490: -185.15554617067224\n","Training loss for fold 2, epoch 491: 223.95774841308594\n","Validation elbo for fold 2, epoch 491: -184.56784307983312\n","Training loss for fold 2, epoch 492: 224.2086678781817\n","Validation elbo for fold 2, epoch 492: -184.67353013506198\n","Training loss for fold 2, epoch 493: 224.02508175757623\n","Validation elbo for fold 2, epoch 493: -183.7332256652141\n","Training loss for fold 2, epoch 494: 223.86290544079196\n","Validation elbo for fold 2, epoch 494: -184.7974764983919\n","Training loss for fold 2, epoch 495: 224.1545348628875\n","Validation elbo for fold 2, epoch 495: -184.5151828271653\n","Training loss for fold 2, epoch 496: 223.85375139790196\n","Validation elbo for fold 2, epoch 496: -184.2199784379672\n","Training loss for fold 2, epoch 497: 224.18006946194558\n","Validation elbo for fold 2, epoch 497: -185.29813448058215\n","Training loss for fold 2, epoch 498: 224.0928708968624\n","Validation elbo for fold 2, epoch 498: -185.15192653321606\n","Training loss for fold 2, epoch 499: 223.8515189386183\n","Validation elbo for fold 2, epoch 499: -184.3979295818856\n","Fold 3\n","-------\n","Training loss for fold 3, epoch 0: 241.49525500882058\n","Validation elbo for fold 3, epoch 0: -185.22966065989084\n","Training loss for fold 3, epoch 1: 226.26651320918913\n","Validation elbo for fold 3, epoch 1: -186.68948873527114\n","Training loss for fold 3, epoch 2: 226.44452790291078\n","Validation elbo for fold 3, epoch 2: -182.25844677109956\n","Training loss for fold 3, epoch 3: 226.71788443288494\n","Validation elbo for fold 3, epoch 3: -187.5464795687251\n","Training loss for fold 3, epoch 4: 226.1838620093561\n","Validation elbo for fold 3, epoch 4: -184.16865943809935\n","Training loss for fold 3, epoch 5: 225.7420139928018\n","Validation elbo for fold 3, epoch 5: -186.13943361938172\n","Training loss for fold 3, epoch 6: 225.62887671685988\n","Validation elbo for fold 3, epoch 6: -182.87931955468147\n","Training loss for fold 3, epoch 7: 225.1095204507151\n","Validation elbo for fold 3, epoch 7: -182.84867328175778\n","Training loss for fold 3, epoch 8: 225.69613573628087\n","Validation elbo for fold 3, epoch 8: -184.25185929044545\n","Training loss for fold 3, epoch 9: 225.37548385127897\n","Validation elbo for fold 3, epoch 9: -181.48502659016947\n","Training loss for fold 3, epoch 10: 225.93757087953628\n","Validation elbo for fold 3, epoch 10: -186.3877605423654\n","Training loss for fold 3, epoch 11: 225.60872847034085\n","Validation elbo for fold 3, epoch 11: -184.4935027616457\n","Training loss for fold 3, epoch 12: 225.5011941232989\n","Validation elbo for fold 3, epoch 12: -186.72153505590705\n","Training loss for fold 3, epoch 13: 225.25297103389616\n","Validation elbo for fold 3, epoch 13: -186.5236866438836\n","Training loss for fold 3, epoch 14: 225.65102214197958\n","Validation elbo for fold 3, epoch 14: -184.94565597714953\n","Training loss for fold 3, epoch 15: 225.37952324651903\n","Validation elbo for fold 3, epoch 15: -184.92210896500586\n","Training loss for fold 3, epoch 16: 225.21187911495085\n","Validation elbo for fold 3, epoch 16: -183.07518110828806\n","Training loss for fold 3, epoch 17: 225.24451372700352\n","Validation elbo for fold 3, epoch 17: -181.5392375268181\n","Training loss for fold 3, epoch 18: 225.1021937708701\n","Validation elbo for fold 3, epoch 18: -184.23995176903182\n","Training loss for fold 3, epoch 19: 225.55518021122103\n","Validation elbo for fold 3, epoch 19: -183.17770045187171\n","Training loss for fold 3, epoch 20: 225.23007152926536\n","Validation elbo for fold 3, epoch 20: -186.01823536308672\n","Training loss for fold 3, epoch 21: 225.09012283817415\n","Validation elbo for fold 3, epoch 21: -182.87484163910716\n","Training loss for fold 3, epoch 22: 225.1119894212292\n","Validation elbo for fold 3, epoch 22: -181.68080148407205\n","Training loss for fold 3, epoch 23: 225.23496320170742\n","Validation elbo for fold 3, epoch 23: -186.47734673376618\n","Training loss for fold 3, epoch 24: 225.18699916716545\n","Validation elbo for fold 3, epoch 24: -183.37834146185594\n","Training loss for fold 3, epoch 25: 224.99994880922378\n","Validation elbo for fold 3, epoch 25: -182.50713647730862\n","Training loss for fold 3, epoch 26: 224.85191985099547\n","Validation elbo for fold 3, epoch 26: -183.30520207446267\n","Training loss for fold 3, epoch 27: 225.11682990289503\n","Validation elbo for fold 3, epoch 27: -185.45756955895266\n","Training loss for fold 3, epoch 28: 225.14957944808467\n","Validation elbo for fold 3, epoch 28: -184.4938532498631\n","Training loss for fold 3, epoch 29: 224.92795833464592\n","Validation elbo for fold 3, epoch 29: -184.25262058462278\n","Training loss for fold 3, epoch 30: 224.7362776725523\n","Validation elbo for fold 3, epoch 30: -183.25205723855169\n","Training loss for fold 3, epoch 31: 224.9072511734501\n","Validation elbo for fold 3, epoch 31: -183.0739267693276\n","Training loss for fold 3, epoch 32: 224.9533460063319\n","Validation elbo for fold 3, epoch 32: -183.1620810167496\n","Training loss for fold 3, epoch 33: 224.64681268507434\n","Validation elbo for fold 3, epoch 33: -187.7431679832505\n","Training loss for fold 3, epoch 34: 224.78707196635585\n","Validation elbo for fold 3, epoch 34: -183.1916830111126\n","Training loss for fold 3, epoch 35: 224.9885202223255\n","Validation elbo for fold 3, epoch 35: -185.55663457259502\n","Training loss for fold 3, epoch 36: 224.88796480240362\n","Validation elbo for fold 3, epoch 36: -183.94054643276874\n","Training loss for fold 3, epoch 37: 224.74136893979966\n","Validation elbo for fold 3, epoch 37: -184.46025945094766\n","Training loss for fold 3, epoch 38: 224.7556836528163\n","Validation elbo for fold 3, epoch 38: -185.12064640552916\n","Training loss for fold 3, epoch 39: 225.21575263238722\n","Validation elbo for fold 3, epoch 39: -185.27147287727956\n","Training loss for fold 3, epoch 40: 224.83773656045236\n","Validation elbo for fold 3, epoch 40: -183.58047990451166\n","Training loss for fold 3, epoch 41: 224.7900599818076\n","Validation elbo for fold 3, epoch 41: -184.85362890341622\n","Training loss for fold 3, epoch 42: 225.0329356039724\n","Validation elbo for fold 3, epoch 42: -182.7645086208434\n","Training loss for fold 3, epoch 43: 225.11090063279676\n","Validation elbo for fold 3, epoch 43: -185.07273354238885\n","Training loss for fold 3, epoch 44: 224.61932028493572\n","Validation elbo for fold 3, epoch 44: -183.3819867656835\n","Training loss for fold 3, epoch 45: 224.68261349585748\n","Validation elbo for fold 3, epoch 45: -182.4218935444843\n","Training loss for fold 3, epoch 46: 224.7367908108619\n","Validation elbo for fold 3, epoch 46: -186.2590893888646\n","Training loss for fold 3, epoch 47: 224.58907441169984\n","Validation elbo for fold 3, epoch 47: -184.95306090189405\n","Training loss for fold 3, epoch 48: 224.49075981878465\n","Validation elbo for fold 3, epoch 48: -183.71112330601542\n","Training loss for fold 3, epoch 49: 224.88711621684413\n","Validation elbo for fold 3, epoch 49: -182.34229663767275\n","Training loss for fold 3, epoch 50: 224.83133919008316\n","Validation elbo for fold 3, epoch 50: -185.785080532447\n","Training loss for fold 3, epoch 51: 224.8246553482548\n","Validation elbo for fold 3, epoch 51: -186.41709869572782\n","Training loss for fold 3, epoch 52: 224.97954534715223\n","Validation elbo for fold 3, epoch 52: -183.62457096202724\n","Training loss for fold 3, epoch 53: 224.70016381048387\n","Validation elbo for fold 3, epoch 53: -183.62461808621262\n","Training loss for fold 3, epoch 54: 224.71549323297316\n","Validation elbo for fold 3, epoch 54: -183.39948232777522\n","Training loss for fold 3, epoch 55: 224.99869045134514\n","Validation elbo for fold 3, epoch 55: -186.27211788332409\n","Training loss for fold 3, epoch 56: 224.52199185279107\n","Validation elbo for fold 3, epoch 56: -184.9285599073285\n","Training loss for fold 3, epoch 57: 224.5573947045111\n","Validation elbo for fold 3, epoch 57: -184.48881079652193\n","Training loss for fold 3, epoch 58: 224.67881725680442\n","Validation elbo for fold 3, epoch 58: -183.85196400750698\n","Training loss for fold 3, epoch 59: 224.65478466403098\n","Validation elbo for fold 3, epoch 59: -184.49675090854464\n","Training loss for fold 3, epoch 60: 224.70875844647807\n","Validation elbo for fold 3, epoch 60: -184.22955622777133\n","Training loss for fold 3, epoch 61: 224.54348656439012\n","Validation elbo for fold 3, epoch 61: -184.77938422312857\n","Training loss for fold 3, epoch 62: 224.82987705353767\n","Validation elbo for fold 3, epoch 62: -183.8208861171014\n","Training loss for fold 3, epoch 63: 225.15911569902974\n","Validation elbo for fold 3, epoch 63: -185.1147494480863\n","Training loss for fold 3, epoch 64: 224.73561071580457\n","Validation elbo for fold 3, epoch 64: -183.5484641322264\n","Training loss for fold 3, epoch 65: 224.63554406935168\n","Validation elbo for fold 3, epoch 65: -183.6854081976334\n","Training loss for fold 3, epoch 66: 225.0469493250693\n","Validation elbo for fold 3, epoch 66: -186.74576033581192\n","Training loss for fold 3, epoch 67: 224.71624509749873\n","Validation elbo for fold 3, epoch 67: -183.4281315114488\n","Training loss for fold 3, epoch 68: 224.6849114202684\n","Validation elbo for fold 3, epoch 68: -185.10606583850088\n","Training loss for fold 3, epoch 69: 224.71793143979966\n","Validation elbo for fold 3, epoch 69: -184.61647022698122\n","Training loss for fold 3, epoch 70: 224.4733948246125\n","Validation elbo for fold 3, epoch 70: -184.61926248569506\n","Training loss for fold 3, epoch 71: 224.4741417669481\n","Validation elbo for fold 3, epoch 71: -181.73623221651167\n","Training loss for fold 3, epoch 72: 224.6885033884356\n","Validation elbo for fold 3, epoch 72: -184.98265817832237\n","Training loss for fold 3, epoch 73: 224.66304114557082\n","Validation elbo for fold 3, epoch 73: -186.19508510155632\n","Training loss for fold 3, epoch 74: 224.68602703463645\n","Validation elbo for fold 3, epoch 74: -186.66097026802652\n","Training loss for fold 3, epoch 75: 224.72045529273248\n","Validation elbo for fold 3, epoch 75: -185.69513048097713\n","Training loss for fold 3, epoch 76: 224.43223522555442\n","Validation elbo for fold 3, epoch 76: -184.8064990610103\n","Training loss for fold 3, epoch 77: 224.4542681786322\n","Validation elbo for fold 3, epoch 77: -184.67224678353486\n","Training loss for fold 3, epoch 78: 224.79068805325417\n","Validation elbo for fold 3, epoch 78: -184.08608713876774\n","Training loss for fold 3, epoch 79: 224.54163926647556\n","Validation elbo for fold 3, epoch 79: -184.88065296415743\n","Training loss for fold 3, epoch 80: 224.58711981004285\n","Validation elbo for fold 3, epoch 80: -184.7529078917343\n","Training loss for fold 3, epoch 81: 224.77186805971206\n","Validation elbo for fold 3, epoch 81: -185.33934970066525\n","Training loss for fold 3, epoch 82: 224.55373727121662\n","Validation elbo for fold 3, epoch 82: -182.743859749208\n","Training loss for fold 3, epoch 83: 224.32044244581652\n","Validation elbo for fold 3, epoch 83: -184.45453383490442\n","Training loss for fold 3, epoch 84: 224.3710196710402\n","Validation elbo for fold 3, epoch 84: -184.11301794420632\n","Training loss for fold 3, epoch 85: 224.56346942532448\n","Validation elbo for fold 3, epoch 85: -184.25152424838706\n","Training loss for fold 3, epoch 86: 224.4579548989573\n","Validation elbo for fold 3, epoch 86: -185.02482601506046\n","Training loss for fold 3, epoch 87: 224.56648648169732\n","Validation elbo for fold 3, epoch 87: -184.78030797166406\n","Training loss for fold 3, epoch 88: 224.70765366092806\n","Validation elbo for fold 3, epoch 88: -183.3760851604502\n","Training loss for fold 3, epoch 89: 224.53169348932082\n","Validation elbo for fold 3, epoch 89: -185.54888372181586\n","Training loss for fold 3, epoch 90: 224.47954756213772\n","Validation elbo for fold 3, epoch 90: -186.10870851733534\n","Training loss for fold 3, epoch 91: 224.30290862052672\n","Validation elbo for fold 3, epoch 91: -182.27311027446476\n","Training loss for fold 3, epoch 92: 224.49823219545425\n","Validation elbo for fold 3, epoch 92: -183.71078087821365\n","Training loss for fold 3, epoch 93: 224.49154170866936\n","Validation elbo for fold 3, epoch 93: -183.38015776744555\n","Training loss for fold 3, epoch 94: 224.42566484020603\n","Validation elbo for fold 3, epoch 94: -184.20067681961552\n","Training loss for fold 3, epoch 95: 224.44984559089906\n","Validation elbo for fold 3, epoch 95: -185.33843082084638\n","Training loss for fold 3, epoch 96: 224.5430388912078\n","Validation elbo for fold 3, epoch 96: -183.64798695345485\n","Training loss for fold 3, epoch 97: 224.45233843403477\n","Validation elbo for fold 3, epoch 97: -183.57788057355253\n","Training loss for fold 3, epoch 98: 224.46201767459993\n","Validation elbo for fold 3, epoch 98: -185.54869428039024\n","Training loss for fold 3, epoch 99: 224.7060792984501\n","Validation elbo for fold 3, epoch 99: -187.0110580395376\n","Training loss for fold 3, epoch 100: 224.52912016837828\n","Validation elbo for fold 3, epoch 100: -183.87903867092552\n","Training loss for fold 3, epoch 101: 224.4430450931672\n","Validation elbo for fold 3, epoch 101: -183.9489123170614\n","Training loss for fold 3, epoch 102: 224.74933304325228\n","Validation elbo for fold 3, epoch 102: -184.78236820309013\n","Training loss for fold 3, epoch 103: 224.63797710787864\n","Validation elbo for fold 3, epoch 103: -185.64502947701587\n","Training loss for fold 3, epoch 104: 224.33407322052986\n","Validation elbo for fold 3, epoch 104: -183.652923249693\n","Training loss for fold 3, epoch 105: 224.41876663700228\n","Validation elbo for fold 3, epoch 105: -181.9371129039903\n","Training loss for fold 3, epoch 106: 224.55148315429688\n","Validation elbo for fold 3, epoch 106: -184.03579508361415\n","Training loss for fold 3, epoch 107: 224.62185742778163\n","Validation elbo for fold 3, epoch 107: -184.50416859871768\n","Training loss for fold 3, epoch 108: 224.51549037810295\n","Validation elbo for fold 3, epoch 108: -183.80031720959659\n","Training loss for fold 3, epoch 109: 224.50621303435295\n","Validation elbo for fold 3, epoch 109: -184.63350505061223\n","Training loss for fold 3, epoch 110: 224.51207388600994\n","Validation elbo for fold 3, epoch 110: -184.51131278818977\n","Training loss for fold 3, epoch 111: 224.68753027146863\n","Validation elbo for fold 3, epoch 111: -183.98101618182494\n","Training loss for fold 3, epoch 112: 224.39171723396547\n","Validation elbo for fold 3, epoch 112: -184.993040346189\n","Training loss for fold 3, epoch 113: 224.77407000141758\n","Validation elbo for fold 3, epoch 113: -185.11031419188345\n","Training loss for fold 3, epoch 114: 224.68959414574408\n","Validation elbo for fold 3, epoch 114: -183.15605709604523\n","Training loss for fold 3, epoch 115: 224.47612540952622\n","Validation elbo for fold 3, epoch 115: -183.76390733278544\n","Training loss for fold 3, epoch 116: 224.48541308987527\n","Validation elbo for fold 3, epoch 116: -184.7748619274065\n","Training loss for fold 3, epoch 117: 224.44447302049207\n","Validation elbo for fold 3, epoch 117: -185.17987167856478\n","Training loss for fold 3, epoch 118: 224.5093061385616\n","Validation elbo for fold 3, epoch 118: -182.45201896322385\n","Training loss for fold 3, epoch 119: 224.79014218238092\n","Validation elbo for fold 3, epoch 119: -182.24450864441002\n","Training loss for fold 3, epoch 120: 224.39022827148438\n","Validation elbo for fold 3, epoch 120: -185.23437192056568\n","Training loss for fold 3, epoch 121: 224.35722794071322\n","Validation elbo for fold 3, epoch 121: -184.14384468191093\n","Training loss for fold 3, epoch 122: 224.49364668323147\n","Validation elbo for fold 3, epoch 122: -183.26829802406795\n","Training loss for fold 3, epoch 123: 224.64047536542338\n","Validation elbo for fold 3, epoch 123: -185.31043685121176\n","Training loss for fold 3, epoch 124: 224.42765365108366\n","Validation elbo for fold 3, epoch 124: -182.45832396526134\n","Training loss for fold 3, epoch 125: 224.68582300986014\n","Validation elbo for fold 3, epoch 125: -183.57382142889537\n","Training loss for fold 3, epoch 126: 224.68445857878655\n","Validation elbo for fold 3, epoch 126: -184.32491518257302\n","Training loss for fold 3, epoch 127: 224.54124647571194\n","Validation elbo for fold 3, epoch 127: -185.277689331851\n","Training loss for fold 3, epoch 128: 224.63783436436808\n","Validation elbo for fold 3, epoch 128: -184.90267302557882\n","Training loss for fold 3, epoch 129: 224.50824663715977\n","Validation elbo for fold 3, epoch 129: -184.97141669239994\n","Training loss for fold 3, epoch 130: 224.62080949352634\n","Validation elbo for fold 3, epoch 130: -184.32186789155327\n","Training loss for fold 3, epoch 131: 224.43804464032573\n","Validation elbo for fold 3, epoch 131: -184.88999591284323\n","Training loss for fold 3, epoch 132: 224.45102863927042\n","Validation elbo for fold 3, epoch 132: -185.74681104173635\n","Training loss for fold 3, epoch 133: 224.5000514368857\n","Validation elbo for fold 3, epoch 133: -184.97466789824475\n","Training loss for fold 3, epoch 134: 224.21964017806513\n","Validation elbo for fold 3, epoch 134: -184.18742238626385\n","Training loss for fold 3, epoch 135: 224.57437896728516\n","Validation elbo for fold 3, epoch 135: -184.92689515320825\n","Training loss for fold 3, epoch 136: 224.2632776075794\n","Validation elbo for fold 3, epoch 136: -183.57030534082037\n","Training loss for fold 3, epoch 137: 224.2915474676317\n","Validation elbo for fold 3, epoch 137: -185.7563312023918\n","Training loss for fold 3, epoch 138: 224.44261563208795\n","Validation elbo for fold 3, epoch 138: -183.66313809953613\n","Training loss for fold 3, epoch 139: 224.4857148201235\n","Validation elbo for fold 3, epoch 139: -183.11018922667353\n","Training loss for fold 3, epoch 140: 224.47750140774636\n","Validation elbo for fold 3, epoch 140: -183.75561445840876\n","Training loss for fold 3, epoch 141: 224.3010709208827\n","Validation elbo for fold 3, epoch 141: -184.8006228986041\n","Training loss for fold 3, epoch 142: 224.71525401453817\n","Validation elbo for fold 3, epoch 142: -184.54339324731478\n","Training loss for fold 3, epoch 143: 224.41688119211506\n","Validation elbo for fold 3, epoch 143: -185.23859648591733\n","Training loss for fold 3, epoch 144: 224.34728560909147\n","Validation elbo for fold 3, epoch 144: -183.5662938010412\n","Training loss for fold 3, epoch 145: 224.26022363478137\n","Validation elbo for fold 3, epoch 145: -183.96594807666668\n","Training loss for fold 3, epoch 146: 224.35982218096333\n","Validation elbo for fold 3, epoch 146: -182.14528535920147\n","Training loss for fold 3, epoch 147: 224.36138202298073\n","Validation elbo for fold 3, epoch 147: -186.277798434577\n","Training loss for fold 3, epoch 148: 224.69916362147177\n","Validation elbo for fold 3, epoch 148: -183.96375681006867\n","Training loss for fold 3, epoch 149: 224.54984086559665\n","Validation elbo for fold 3, epoch 149: -183.94181763514055\n","Training loss for fold 3, epoch 150: 224.58280427994268\n","Validation elbo for fold 3, epoch 150: -183.7992181792236\n","Training loss for fold 3, epoch 151: 224.2993373255576\n","Validation elbo for fold 3, epoch 151: -183.6387178029617\n","Training loss for fold 3, epoch 152: 224.41329217726184\n","Validation elbo for fold 3, epoch 152: -183.76629305555906\n","Training loss for fold 3, epoch 153: 224.392942613171\n","Validation elbo for fold 3, epoch 153: -184.10950067235453\n","Training loss for fold 3, epoch 154: 224.9252681116904\n","Validation elbo for fold 3, epoch 154: -185.58649352229156\n","Training loss for fold 3, epoch 155: 224.66683073966735\n","Validation elbo for fold 3, epoch 155: -185.43134312316238\n","Training loss for fold 3, epoch 156: 224.4180671938004\n","Validation elbo for fold 3, epoch 156: -184.5074391716782\n","Training loss for fold 3, epoch 157: 224.65196498747795\n","Validation elbo for fold 3, epoch 157: -185.37659359949487\n","Training loss for fold 3, epoch 158: 224.37032834945185\n","Validation elbo for fold 3, epoch 158: -184.5351715028586\n","Training loss for fold 3, epoch 159: 224.5905038156817\n","Validation elbo for fold 3, epoch 159: -185.1437178023483\n","Training loss for fold 3, epoch 160: 224.66837384623867\n","Validation elbo for fold 3, epoch 160: -183.90690612402466\n","Training loss for fold 3, epoch 161: 224.59679388230848\n","Validation elbo for fold 3, epoch 161: -184.01922251739475\n","Training loss for fold 3, epoch 162: 224.28534107823526\n","Validation elbo for fold 3, epoch 162: -184.87472601544053\n","Training loss for fold 3, epoch 163: 224.5712883241715\n","Validation elbo for fold 3, epoch 163: -185.2490608026646\n","Training loss for fold 3, epoch 164: 224.37643358784337\n","Validation elbo for fold 3, epoch 164: -183.6366157083432\n","Training loss for fold 3, epoch 165: 224.64984106248426\n","Validation elbo for fold 3, epoch 165: -186.2310052979115\n","Training loss for fold 3, epoch 166: 224.21695807672316\n","Validation elbo for fold 3, epoch 166: -183.65704438550114\n","Training loss for fold 3, epoch 167: 224.4411653087985\n","Validation elbo for fold 3, epoch 167: -184.26101575601422\n","Training loss for fold 3, epoch 168: 224.61049061436808\n","Validation elbo for fold 3, epoch 168: -184.632854605586\n","Training loss for fold 3, epoch 169: 224.47603877898186\n","Validation elbo for fold 3, epoch 169: -184.43248416547323\n","Training loss for fold 3, epoch 170: 224.5375504032258\n","Validation elbo for fold 3, epoch 170: -183.93023964287676\n","Training loss for fold 3, epoch 171: 224.50940482847153\n","Validation elbo for fold 3, epoch 171: -184.32429091730313\n","Training loss for fold 3, epoch 172: 224.71141569076045\n","Validation elbo for fold 3, epoch 172: -184.54338253842306\n","Training loss for fold 3, epoch 173: 224.342404027139\n","Validation elbo for fold 3, epoch 173: -183.25315528857112\n","Training loss for fold 3, epoch 174: 224.42174480807395\n","Validation elbo for fold 3, epoch 174: -183.25040111125065\n","Training loss for fold 3, epoch 175: 224.5087115380072\n","Validation elbo for fold 3, epoch 175: -184.9059378507464\n","Training loss for fold 3, epoch 176: 224.31610132032824\n","Validation elbo for fold 3, epoch 176: -184.10586157358256\n","Training loss for fold 3, epoch 177: 224.28985497259325\n","Validation elbo for fold 3, epoch 177: -183.1470505539325\n","Training loss for fold 3, epoch 178: 224.29232517365486\n","Validation elbo for fold 3, epoch 178: -183.5216718082073\n","Training loss for fold 3, epoch 179: 224.44303844821067\n","Validation elbo for fold 3, epoch 179: -185.09137304141927\n","Training loss for fold 3, epoch 180: 225.024048097672\n","Validation elbo for fold 3, epoch 180: -182.50002266896684\n","Training loss for fold 3, epoch 181: 224.95394774406188\n","Validation elbo for fold 3, epoch 181: -184.7944224468958\n","Training loss for fold 3, epoch 182: 224.4917725593813\n","Validation elbo for fold 3, epoch 182: -183.42813143880437\n","Training loss for fold 3, epoch 183: 224.42689440327305\n","Validation elbo for fold 3, epoch 183: -185.0034577007719\n","Training loss for fold 3, epoch 184: 224.45289685649257\n","Validation elbo for fold 3, epoch 184: -185.15947307994145\n","Training loss for fold 3, epoch 185: 224.3574484548261\n","Validation elbo for fold 3, epoch 185: -185.04417496871466\n","Training loss for fold 3, epoch 186: 224.47457836520286\n","Validation elbo for fold 3, epoch 186: -184.58795643137842\n","Training loss for fold 3, epoch 187: 224.55637827227193\n","Validation elbo for fold 3, epoch 187: -184.61544734224728\n","Training loss for fold 3, epoch 188: 224.26893615722656\n","Validation elbo for fold 3, epoch 188: -184.06723833309616\n","Training loss for fold 3, epoch 189: 224.47683494321763\n","Validation elbo for fold 3, epoch 189: -183.39079573259147\n","Training loss for fold 3, epoch 190: 224.2844225975775\n","Validation elbo for fold 3, epoch 190: -185.0764740644491\n","Training loss for fold 3, epoch 191: 224.3973878429782\n","Validation elbo for fold 3, epoch 191: -185.05503758469132\n","Training loss for fold 3, epoch 192: 224.6009282758159\n","Validation elbo for fold 3, epoch 192: -184.9755638296429\n","Training loss for fold 3, epoch 193: 224.54723136655747\n","Validation elbo for fold 3, epoch 193: -184.14620418552164\n","Training loss for fold 3, epoch 194: 224.36968871085875\n","Validation elbo for fold 3, epoch 194: -184.52042632271298\n","Training loss for fold 3, epoch 195: 224.47916240076864\n","Validation elbo for fold 3, epoch 195: -185.3053351275904\n","Training loss for fold 3, epoch 196: 224.32542788597846\n","Validation elbo for fold 3, epoch 196: -184.63488744935214\n","Training loss for fold 3, epoch 197: 224.24341386364353\n","Validation elbo for fold 3, epoch 197: -185.12150785084566\n","Training loss for fold 3, epoch 198: 224.18596993723224\n","Validation elbo for fold 3, epoch 198: -183.574940703431\n","Training loss for fold 3, epoch 199: 224.52720888199346\n","Validation elbo for fold 3, epoch 199: -184.8226409221868\n","Training loss for fold 3, epoch 200: 224.29317425143333\n","Validation elbo for fold 3, epoch 200: -184.01491784105417\n","Training loss for fold 3, epoch 201: 224.3157698108304\n","Validation elbo for fold 3, epoch 201: -184.3170248727653\n","Training loss for fold 3, epoch 202: 224.5704109438004\n","Validation elbo for fold 3, epoch 202: -184.03328856946348\n","Training loss for fold 3, epoch 203: 224.4440201790102\n","Validation elbo for fold 3, epoch 203: -185.16244365293716\n","Training loss for fold 3, epoch 204: 224.43455013152092\n","Validation elbo for fold 3, epoch 204: -184.393081586392\n","Training loss for fold 3, epoch 205: 224.14610142861642\n","Validation elbo for fold 3, epoch 205: -184.01825661095506\n","Training loss for fold 3, epoch 206: 224.24027202975364\n","Validation elbo for fold 3, epoch 206: -183.18474584931144\n","Training loss for fold 3, epoch 207: 224.71004043086882\n","Validation elbo for fold 3, epoch 207: -185.00935814399293\n","Training loss for fold 3, epoch 208: 224.52576323478453\n","Validation elbo for fold 3, epoch 208: -185.20183985518278\n","Training loss for fold 3, epoch 209: 224.22375340615548\n","Validation elbo for fold 3, epoch 209: -185.15316218150335\n","Training loss for fold 3, epoch 210: 224.2704312724452\n","Validation elbo for fold 3, epoch 210: -184.0735185805603\n","Training loss for fold 3, epoch 211: 224.74439436389554\n","Validation elbo for fold 3, epoch 211: -184.03975454947758\n","Training loss for fold 3, epoch 212: 224.70641449959047\n","Validation elbo for fold 3, epoch 212: -184.10077087343487\n","Training loss for fold 3, epoch 213: 224.41343516688192\n","Validation elbo for fold 3, epoch 213: -184.61802167177765\n","Training loss for fold 3, epoch 214: 224.54224838749056\n","Validation elbo for fold 3, epoch 214: -185.07645259610433\n","Training loss for fold 3, epoch 215: 224.39605688279676\n","Validation elbo for fold 3, epoch 215: -184.18997995081472\n","Training loss for fold 3, epoch 216: 224.5508331791047\n","Validation elbo for fold 3, epoch 216: -183.66047271239404\n","Training loss for fold 3, epoch 217: 224.7647013510427\n","Validation elbo for fold 3, epoch 217: -184.20502367368735\n","Training loss for fold 3, epoch 218: 224.4551248858052\n","Validation elbo for fold 3, epoch 218: -185.8360402191371\n","Training loss for fold 3, epoch 219: 224.52835526004915\n","Validation elbo for fold 3, epoch 219: -185.91489921339348\n","Training loss for fold 3, epoch 220: 224.3781706287015\n","Validation elbo for fold 3, epoch 220: -183.96745501969613\n","Training loss for fold 3, epoch 221: 224.47944001228578\n","Validation elbo for fold 3, epoch 221: -184.44837201379272\n","Training loss for fold 3, epoch 222: 224.9613076486895\n","Validation elbo for fold 3, epoch 222: -185.35116447640445\n","Training loss for fold 3, epoch 223: 225.06549342986077\n","Validation elbo for fold 3, epoch 223: -183.95016529727013\n","Training loss for fold 3, epoch 224: 224.48538429506362\n","Validation elbo for fold 3, epoch 224: -185.80282384880965\n","Training loss for fold 3, epoch 225: 224.3216318930349\n","Validation elbo for fold 3, epoch 225: -184.15201746307702\n","Training loss for fold 3, epoch 226: 224.38879443753152\n","Validation elbo for fold 3, epoch 226: -184.17232291433766\n","Training loss for fold 3, epoch 227: 224.42065971128403\n","Validation elbo for fold 3, epoch 227: -184.10786006287339\n","Training loss for fold 3, epoch 228: 224.28978040141445\n","Validation elbo for fold 3, epoch 228: -183.5859765257254\n","Training loss for fold 3, epoch 229: 224.35166611209993\n","Validation elbo for fold 3, epoch 229: -184.4648715694889\n","Training loss for fold 3, epoch 230: 224.2098154867849\n","Validation elbo for fold 3, epoch 230: -184.60858844083805\n","Training loss for fold 3, epoch 231: 224.20121223695816\n","Validation elbo for fold 3, epoch 231: -184.75314503288072\n","Training loss for fold 3, epoch 232: 224.21066234957786\n","Validation elbo for fold 3, epoch 232: -184.3344695165112\n","Training loss for fold 3, epoch 233: 224.40994681081463\n","Validation elbo for fold 3, epoch 233: -185.5762722871176\n","Training loss for fold 3, epoch 234: 224.5087408250378\n","Validation elbo for fold 3, epoch 234: -185.04796738223837\n","Training loss for fold 3, epoch 235: 224.30766025666267\n","Validation elbo for fold 3, epoch 235: -184.23120217541245\n","Training loss for fold 3, epoch 236: 224.40611808530747\n","Validation elbo for fold 3, epoch 236: -183.8140502268452\n","Training loss for fold 3, epoch 237: 224.82833222419984\n","Validation elbo for fold 3, epoch 237: -183.91288947058848\n","Training loss for fold 3, epoch 238: 225.06547570997668\n","Validation elbo for fold 3, epoch 238: -183.88842438287244\n","Training loss for fold 3, epoch 239: 224.32161220427483\n","Validation elbo for fold 3, epoch 239: -184.24257485286\n","Training loss for fold 3, epoch 240: 224.11965966993762\n","Validation elbo for fold 3, epoch 240: -185.3631423719694\n","Training loss for fold 3, epoch 241: 224.2767816358997\n","Validation elbo for fold 3, epoch 241: -184.06589442102427\n","Training loss for fold 3, epoch 242: 224.24354110225553\n","Validation elbo for fold 3, epoch 242: -183.80232675448653\n","Training loss for fold 3, epoch 243: 224.15734371062248\n","Validation elbo for fold 3, epoch 243: -183.45305328987916\n","Training loss for fold 3, epoch 244: 224.4648420272335\n","Validation elbo for fold 3, epoch 244: -184.74049973946208\n","Training loss for fold 3, epoch 245: 224.3489731819399\n","Validation elbo for fold 3, epoch 245: -185.3348529655185\n","Training loss for fold 3, epoch 246: 224.62298657817226\n","Validation elbo for fold 3, epoch 246: -184.58768680667356\n","Training loss for fold 3, epoch 247: 224.5374280868038\n","Validation elbo for fold 3, epoch 247: -184.81668891135052\n","Training loss for fold 3, epoch 248: 224.56922174269152\n","Validation elbo for fold 3, epoch 248: -183.9424696799017\n","Training loss for fold 3, epoch 249: 224.18932465584047\n","Validation elbo for fold 3, epoch 249: -183.90310154712012\n","Training loss for fold 3, epoch 250: 224.59434213945943\n","Validation elbo for fold 3, epoch 250: -184.55486492156308\n","Training loss for fold 3, epoch 251: 224.5100803990518\n","Validation elbo for fold 3, epoch 251: -183.997747454417\n","Training loss for fold 3, epoch 252: 224.1033669748614\n","Validation elbo for fold 3, epoch 252: -184.35525495370007\n","Training loss for fold 3, epoch 253: 224.2865755634923\n","Validation elbo for fold 3, epoch 253: -184.35298986006066\n","Training loss for fold 3, epoch 254: 224.04322298111455\n","Validation elbo for fold 3, epoch 254: -184.18410401389792\n","Training loss for fold 3, epoch 255: 224.4503710346837\n","Validation elbo for fold 3, epoch 255: -184.4237779430538\n","Training loss for fold 3, epoch 256: 224.25406056065714\n","Validation elbo for fold 3, epoch 256: -183.69959273199368\n","Training loss for fold 3, epoch 257: 224.20678760159402\n","Validation elbo for fold 3, epoch 257: -184.2534774544757\n","Training loss for fold 3, epoch 258: 224.30494936050908\n","Validation elbo for fold 3, epoch 258: -184.3406500533943\n","Training loss for fold 3, epoch 259: 224.23070452290196\n","Validation elbo for fold 3, epoch 259: -183.7513701589982\n","Training loss for fold 3, epoch 260: 224.16866548599737\n","Validation elbo for fold 3, epoch 260: -184.75931174625026\n","Training loss for fold 3, epoch 261: 224.34283274988974\n","Validation elbo for fold 3, epoch 261: -184.93965829870967\n","Training loss for fold 3, epoch 262: 224.25871719852572\n","Validation elbo for fold 3, epoch 262: -184.3449716287691\n","Training loss for fold 3, epoch 263: 224.438234390751\n","Validation elbo for fold 3, epoch 263: -185.5741705554977\n","Training loss for fold 3, epoch 264: 224.3217566705519\n","Validation elbo for fold 3, epoch 264: -183.82018546684776\n","Training loss for fold 3, epoch 265: 224.1525152883222\n","Validation elbo for fold 3, epoch 265: -185.41241859749692\n","Training loss for fold 3, epoch 266: 224.33870893909085\n","Validation elbo for fold 3, epoch 266: -184.73060000532143\n","Training loss for fold 3, epoch 267: 224.35224471553678\n","Validation elbo for fold 3, epoch 267: -183.15530727774566\n","Training loss for fold 3, epoch 268: 224.63024262459047\n","Validation elbo for fold 3, epoch 268: -183.49589203762037\n","Training loss for fold 3, epoch 269: 224.7501023815524\n","Validation elbo for fold 3, epoch 269: -185.26805768119664\n","Training loss for fold 3, epoch 270: 224.4437691473192\n","Validation elbo for fold 3, epoch 270: -184.88417716056904\n","Training loss for fold 3, epoch 271: 224.21622294764364\n","Validation elbo for fold 3, epoch 271: -184.27561832886826\n","Training loss for fold 3, epoch 272: 224.55200638309603\n","Validation elbo for fold 3, epoch 272: -184.95916590154644\n","Training loss for fold 3, epoch 273: 224.4276364234186\n","Validation elbo for fold 3, epoch 273: -184.27681514078296\n","Training loss for fold 3, epoch 274: 224.3352538078062\n","Validation elbo for fold 3, epoch 274: -184.28342183978015\n","Training loss for fold 3, epoch 275: 224.55247965166646\n","Validation elbo for fold 3, epoch 275: -184.1913014147436\n","Training loss for fold 3, epoch 276: 224.42462084370274\n","Validation elbo for fold 3, epoch 276: -182.91594996907412\n","Training loss for fold 3, epoch 277: 224.6667473085465\n","Validation elbo for fold 3, epoch 277: -184.7083450134315\n","Training loss for fold 3, epoch 278: 224.46996799592048\n","Validation elbo for fold 3, epoch 278: -184.58944710309345\n","Training loss for fold 3, epoch 279: 224.26128264396422\n","Validation elbo for fold 3, epoch 279: -184.60140079276113\n","Training loss for fold 3, epoch 280: 224.54039493683845\n","Validation elbo for fold 3, epoch 280: -184.55629229982353\n","Training loss for fold 3, epoch 281: 224.28177692044167\n","Validation elbo for fold 3, epoch 281: -184.31320254320283\n","Training loss for fold 3, epoch 282: 224.19938167448967\n","Validation elbo for fold 3, epoch 282: -183.34719063124396\n","Training loss for fold 3, epoch 283: 224.36806045040007\n","Validation elbo for fold 3, epoch 283: -184.93942279964034\n","Training loss for fold 3, epoch 284: 224.35262864635837\n","Validation elbo for fold 3, epoch 284: -183.26346312714895\n","Training loss for fold 3, epoch 285: 224.20901489257812\n","Validation elbo for fold 3, epoch 285: -184.18853100403513\n","Training loss for fold 3, epoch 286: 224.49685841221964\n","Validation elbo for fold 3, epoch 286: -185.09912573201572\n","Training loss for fold 3, epoch 287: 224.2913582094254\n","Validation elbo for fold 3, epoch 287: -185.01421319532977\n","Training loss for fold 3, epoch 288: 224.06002463063885\n","Validation elbo for fold 3, epoch 288: -183.58378851251103\n","Training loss for fold 3, epoch 289: 224.27511227515436\n","Validation elbo for fold 3, epoch 289: -182.97164393962709\n","Training loss for fold 3, epoch 290: 224.15079965899068\n","Validation elbo for fold 3, epoch 290: -185.17240456759058\n","Training loss for fold 3, epoch 291: 224.57698772799583\n","Validation elbo for fold 3, epoch 291: -184.3737267249724\n","Training loss for fold 3, epoch 292: 224.28171711583292\n","Validation elbo for fold 3, epoch 292: -183.6224789341034\n","Training loss for fold 3, epoch 293: 224.07528711134387\n","Validation elbo for fold 3, epoch 293: -183.51606816143942\n","Training loss for fold 3, epoch 294: 224.51670443627143\n","Validation elbo for fold 3, epoch 294: -183.6510462376243\n","Training loss for fold 3, epoch 295: 224.35372038810485\n","Validation elbo for fold 3, epoch 295: -184.19817822300774\n","Training loss for fold 3, epoch 296: 224.27002519176853\n","Validation elbo for fold 3, epoch 296: -183.825013341549\n","Training loss for fold 3, epoch 297: 224.59093032344694\n","Validation elbo for fold 3, epoch 297: -185.33388133984866\n","Training loss for fold 3, epoch 298: 224.29303593789376\n","Validation elbo for fold 3, epoch 298: -185.39846540658013\n","Training loss for fold 3, epoch 299: 224.34738356067288\n","Validation elbo for fold 3, epoch 299: -181.9025637906626\n","Training loss for fold 3, epoch 300: 224.32703498102003\n","Validation elbo for fold 3, epoch 300: -183.4480426846377\n","Training loss for fold 3, epoch 301: 224.42791994156377\n","Validation elbo for fold 3, epoch 301: -184.41305558925683\n","Training loss for fold 3, epoch 302: 224.4546651532573\n","Validation elbo for fold 3, epoch 302: -185.17194627461416\n","Training loss for fold 3, epoch 303: 224.29552656604397\n","Validation elbo for fold 3, epoch 303: -184.33558303306546\n","Training loss for fold 3, epoch 304: 224.65807564027847\n","Validation elbo for fold 3, epoch 304: -183.42236472020227\n","Training loss for fold 3, epoch 305: 224.3894035585465\n","Validation elbo for fold 3, epoch 305: -183.78489285015405\n","Training loss for fold 3, epoch 306: 224.23846632434476\n","Validation elbo for fold 3, epoch 306: -185.39427519787688\n","Training loss for fold 3, epoch 307: 224.32843337520475\n","Validation elbo for fold 3, epoch 307: -182.71122600551303\n","Training loss for fold 3, epoch 308: 224.44215368455457\n","Validation elbo for fold 3, epoch 308: -184.0107811287118\n","Training loss for fold 3, epoch 309: 224.57125485327936\n","Validation elbo for fold 3, epoch 309: -183.52999703092283\n","Training loss for fold 3, epoch 310: 224.23424259308845\n","Validation elbo for fold 3, epoch 310: -183.7812968605461\n","Training loss for fold 3, epoch 311: 224.08942511773878\n","Validation elbo for fold 3, epoch 311: -184.53239720459402\n","Training loss for fold 3, epoch 312: 224.52842417070943\n","Validation elbo for fold 3, epoch 312: -184.1129895932617\n","Training loss for fold 3, epoch 313: 224.16603383710307\n","Validation elbo for fold 3, epoch 313: -183.25319705749888\n","Training loss for fold 3, epoch 314: 224.364862995763\n","Validation elbo for fold 3, epoch 314: -183.8780330592203\n","Training loss for fold 3, epoch 315: 224.14589518885458\n","Validation elbo for fold 3, epoch 315: -185.58499767029951\n","Training loss for fold 3, epoch 316: 224.37442238100112\n","Validation elbo for fold 3, epoch 316: -184.93539528631663\n","Training loss for fold 3, epoch 317: 224.41477867864793\n","Validation elbo for fold 3, epoch 317: -184.05999669922153\n","Training loss for fold 3, epoch 318: 224.75045973254788\n","Validation elbo for fold 3, epoch 318: -185.55123919962978\n","Training loss for fold 3, epoch 319: 224.31190712221206\n","Validation elbo for fold 3, epoch 319: -184.47067901815555\n","Training loss for fold 3, epoch 320: 224.1736056420111\n","Validation elbo for fold 3, epoch 320: -183.85966047784171\n","Training loss for fold 3, epoch 321: 224.48773980909777\n","Validation elbo for fold 3, epoch 321: -184.37322546442613\n","Training loss for fold 3, epoch 322: 224.2677954396894\n","Validation elbo for fold 3, epoch 322: -183.762199596541\n","Training loss for fold 3, epoch 323: 224.19735594718688\n","Validation elbo for fold 3, epoch 323: -185.34926746677215\n","Training loss for fold 3, epoch 324: 224.10235374204575\n","Validation elbo for fold 3, epoch 324: -184.35393429013476\n","Training loss for fold 3, epoch 325: 224.29594076833416\n","Validation elbo for fold 3, epoch 325: -182.76308380485025\n","Training loss for fold 3, epoch 326: 224.2766571044922\n","Validation elbo for fold 3, epoch 326: -184.91432573835027\n","Training loss for fold 3, epoch 327: 224.44222111855782\n","Validation elbo for fold 3, epoch 327: -183.62686696166648\n","Training loss for fold 3, epoch 328: 224.4934350290606\n","Validation elbo for fold 3, epoch 328: -184.63658796800092\n","Training loss for fold 3, epoch 329: 224.3666226786952\n","Validation elbo for fold 3, epoch 329: -184.95881596275416\n","Training loss for fold 3, epoch 330: 224.38824167559224\n","Validation elbo for fold 3, epoch 330: -185.16096733265718\n","Training loss for fold 3, epoch 331: 224.30104680215157\n","Validation elbo for fold 3, epoch 331: -184.34839834971146\n","Training loss for fold 3, epoch 332: 224.1280000748173\n","Validation elbo for fold 3, epoch 332: -182.3980680146157\n","Training loss for fold 3, epoch 333: 224.34698535550027\n","Validation elbo for fold 3, epoch 333: -184.47959897053732\n","Training loss for fold 3, epoch 334: 224.39033483689832\n","Validation elbo for fold 3, epoch 334: -184.4247504651733\n","Training loss for fold 3, epoch 335: 224.14762361588018\n","Validation elbo for fold 3, epoch 335: -184.1421851780447\n","Training loss for fold 3, epoch 336: 224.05123138427734\n","Validation elbo for fold 3, epoch 336: -183.52221874083847\n","Training loss for fold 3, epoch 337: 224.84342661211568\n","Validation elbo for fold 3, epoch 337: -183.5437742557226\n","Training loss for fold 3, epoch 338: 224.35663161739225\n","Validation elbo for fold 3, epoch 338: -184.38137904366533\n","Training loss for fold 3, epoch 339: 224.1452619491085\n","Validation elbo for fold 3, epoch 339: -183.9103088414579\n","Training loss for fold 3, epoch 340: 224.15232726066344\n","Validation elbo for fold 3, epoch 340: -184.89098053741327\n","Training loss for fold 3, epoch 341: 224.1977292952999\n","Validation elbo for fold 3, epoch 341: -184.36455316203575\n","Training loss for fold 3, epoch 342: 224.4103991600775\n","Validation elbo for fold 3, epoch 342: -184.84495880808805\n","Training loss for fold 3, epoch 343: 224.27619121920677\n","Validation elbo for fold 3, epoch 343: -184.7350560859714\n","Training loss for fold 3, epoch 344: 224.25038466915007\n","Validation elbo for fold 3, epoch 344: -184.6078152807901\n","Training loss for fold 3, epoch 345: 224.30377295709425\n","Validation elbo for fold 3, epoch 345: -185.4132739030897\n","Training loss for fold 3, epoch 346: 224.13785774477066\n","Validation elbo for fold 3, epoch 346: -183.72239945716484\n","Training loss for fold 3, epoch 347: 224.47271310129474\n","Validation elbo for fold 3, epoch 347: -185.5769637562857\n","Training loss for fold 3, epoch 348: 224.3126922115203\n","Validation elbo for fold 3, epoch 348: -184.4705491656205\n","Training loss for fold 3, epoch 349: 224.3286898213048\n","Validation elbo for fold 3, epoch 349: -184.27051680217804\n","Training loss for fold 3, epoch 350: 224.43841134348224\n","Validation elbo for fold 3, epoch 350: -184.2454227452914\n","Training loss for fold 3, epoch 351: 224.1557875602476\n","Validation elbo for fold 3, epoch 351: -184.91575119087784\n","Training loss for fold 3, epoch 352: 224.42196015388734\n","Validation elbo for fold 3, epoch 352: -183.9481912647458\n","Training loss for fold 3, epoch 353: 224.18797572966545\n","Validation elbo for fold 3, epoch 353: -184.63119409070026\n","Training loss for fold 3, epoch 354: 224.12076716269218\n","Validation elbo for fold 3, epoch 354: -184.81379079008775\n","Training loss for fold 3, epoch 355: 224.2678013463174\n","Validation elbo for fold 3, epoch 355: -184.90573087940754\n","Training loss for fold 3, epoch 356: 224.2870564614573\n","Validation elbo for fold 3, epoch 356: -184.51332171314692\n","Training loss for fold 3, epoch 357: 224.1967556861139\n","Validation elbo for fold 3, epoch 357: -185.69496067745865\n","Training loss for fold 3, epoch 358: 224.45005601452243\n","Validation elbo for fold 3, epoch 358: -184.36208650289834\n","Training loss for fold 3, epoch 359: 224.21863728184854\n","Validation elbo for fold 3, epoch 359: -183.59134564568322\n","Training loss for fold 3, epoch 360: 224.29334382087953\n","Validation elbo for fold 3, epoch 360: -183.24029454431093\n","Training loss for fold 3, epoch 361: 224.49329253165953\n","Validation elbo for fold 3, epoch 361: -183.69071737360537\n","Training loss for fold 3, epoch 362: 224.30626358524447\n","Validation elbo for fold 3, epoch 362: -183.4292316994077\n","Training loss for fold 3, epoch 363: 224.17526565059538\n","Validation elbo for fold 3, epoch 363: -185.9241839754298\n","Training loss for fold 3, epoch 364: 224.347295207362\n","Validation elbo for fold 3, epoch 364: -184.00514939491984\n","Training loss for fold 3, epoch 365: 224.2431421587544\n","Validation elbo for fold 3, epoch 365: -183.3055391191416\n","Training loss for fold 3, epoch 366: 224.075686547064\n","Validation elbo for fold 3, epoch 366: -184.0258227777867\n","Training loss for fold 3, epoch 367: 224.25689992597026\n","Validation elbo for fold 3, epoch 367: -184.80818007948346\n","Training loss for fold 3, epoch 368: 224.1725050403226\n","Validation elbo for fold 3, epoch 368: -184.60314911264084\n","Training loss for fold 3, epoch 369: 224.20774767475743\n","Validation elbo for fold 3, epoch 369: -185.2657833563459\n","Training loss for fold 3, epoch 370: 224.2165778375441\n","Validation elbo for fold 3, epoch 370: -185.06980913736982\n","Training loss for fold 3, epoch 371: 224.17154718214465\n","Validation elbo for fold 3, epoch 371: -185.18248232390724\n","Training loss for fold 3, epoch 372: 224.27135861304498\n","Validation elbo for fold 3, epoch 372: -184.1070345599624\n","Training loss for fold 3, epoch 373: 224.33398486721902\n","Validation elbo for fold 3, epoch 373: -184.40049650772667\n","Training loss for fold 3, epoch 374: 224.14441065634452\n","Validation elbo for fold 3, epoch 374: -185.35938848615413\n","Training loss for fold 3, epoch 375: 224.11835356681578\n","Validation elbo for fold 3, epoch 375: -183.86832216739208\n","Training loss for fold 3, epoch 376: 224.40456021216607\n","Validation elbo for fold 3, epoch 376: -184.74711588589676\n","Training loss for fold 3, epoch 377: 224.1399868380639\n","Validation elbo for fold 3, epoch 377: -184.36715389479625\n","Training loss for fold 3, epoch 378: 224.27696449525894\n","Validation elbo for fold 3, epoch 378: -184.99047212376183\n","Training loss for fold 3, epoch 379: 224.41700646185106\n","Validation elbo for fold 3, epoch 379: -183.79592369834546\n","Training loss for fold 3, epoch 380: 224.20857755599482\n","Validation elbo for fold 3, epoch 380: -185.38525790438626\n","Training loss for fold 3, epoch 381: 224.10204438240297\n","Validation elbo for fold 3, epoch 381: -183.75748576803136\n","Training loss for fold 3, epoch 382: 224.4040990029612\n","Validation elbo for fold 3, epoch 382: -185.31296456718235\n","Training loss for fold 3, epoch 383: 224.21981762301536\n","Validation elbo for fold 3, epoch 383: -184.17512852674625\n","Training loss for fold 3, epoch 384: 224.18109007804625\n","Validation elbo for fold 3, epoch 384: -184.52179311598746\n","Training loss for fold 3, epoch 385: 224.1808358469317\n","Validation elbo for fold 3, epoch 385: -184.516433584528\n","Training loss for fold 3, epoch 386: 224.17694067185926\n","Validation elbo for fold 3, epoch 386: -183.6116811995925\n","Training loss for fold 3, epoch 387: 224.12675426852317\n","Validation elbo for fold 3, epoch 387: -183.79014912701962\n","Training loss for fold 3, epoch 388: 224.1925267865581\n","Validation elbo for fold 3, epoch 388: -183.97974072276082\n","Training loss for fold 3, epoch 389: 224.15202971427672\n","Validation elbo for fold 3, epoch 389: -185.74556248103067\n","Training loss for fold 3, epoch 390: 224.52733193674396\n","Validation elbo for fold 3, epoch 390: -185.0023600457261\n","Training loss for fold 3, epoch 391: 224.36118833480342\n","Validation elbo for fold 3, epoch 391: -183.80119904692114\n","Training loss for fold 3, epoch 392: 224.2759780883789\n","Validation elbo for fold 3, epoch 392: -184.56998547126452\n","Training loss for fold 3, epoch 393: 224.40833110194052\n","Validation elbo for fold 3, epoch 393: -185.64811195937506\n","Training loss for fold 3, epoch 394: 224.29490612399192\n","Validation elbo for fold 3, epoch 394: -184.7825998528797\n","Training loss for fold 3, epoch 395: 224.28677712717365\n","Validation elbo for fold 3, epoch 395: -184.02537624286884\n","Training loss for fold 3, epoch 396: 224.4692653532951\n","Validation elbo for fold 3, epoch 396: -184.8460372401904\n","Training loss for fold 3, epoch 397: 224.25244706676853\n","Validation elbo for fold 3, epoch 397: -185.630040495295\n","Training loss for fold 3, epoch 398: 224.141599593624\n","Validation elbo for fold 3, epoch 398: -183.37065089768856\n","Training loss for fold 3, epoch 399: 224.48613935901272\n","Validation elbo for fold 3, epoch 399: -184.42290410851072\n","Training loss for fold 3, epoch 400: 224.33468160321635\n","Validation elbo for fold 3, epoch 400: -183.52545149582056\n","Training loss for fold 3, epoch 401: 224.21566969348538\n","Validation elbo for fold 3, epoch 401: -183.5201782921099\n","Training loss for fold 3, epoch 402: 224.27193303262032\n","Validation elbo for fold 3, epoch 402: -183.55804829823012\n","Training loss for fold 3, epoch 403: 224.28986186365927\n","Validation elbo for fold 3, epoch 403: -185.36703750784977\n","Training loss for fold 3, epoch 404: 224.30496141987462\n","Validation elbo for fold 3, epoch 404: -183.7145703210354\n","Training loss for fold 3, epoch 405: 224.0653376425466\n","Validation elbo for fold 3, epoch 405: -183.64706369216265\n","Training loss for fold 3, epoch 406: 224.23243171938003\n","Validation elbo for fold 3, epoch 406: -184.77016379220117\n","Training loss for fold 3, epoch 407: 224.50705989714592\n","Validation elbo for fold 3, epoch 407: -184.86169032296706\n","Training loss for fold 3, epoch 408: 224.09012185373615\n","Validation elbo for fold 3, epoch 408: -184.27034838545535\n","Training loss for fold 3, epoch 409: 224.183841828377\n","Validation elbo for fold 3, epoch 409: -184.36788435934793\n","Training loss for fold 3, epoch 410: 224.17715232603013\n","Validation elbo for fold 3, epoch 410: -183.9021017232145\n","Training loss for fold 3, epoch 411: 224.35472623763545\n","Validation elbo for fold 3, epoch 411: -183.15426205981584\n","Training loss for fold 3, epoch 412: 224.57008829424458\n","Validation elbo for fold 3, epoch 412: -185.40917112062806\n","Training loss for fold 3, epoch 413: 224.27256627236642\n","Validation elbo for fold 3, epoch 413: -183.93925286353107\n","Training loss for fold 3, epoch 414: 224.09551805065524\n","Validation elbo for fold 3, epoch 414: -183.52901797715685\n","Training loss for fold 3, epoch 415: 224.28753785164125\n","Validation elbo for fold 3, epoch 415: -184.62784930255214\n","Training loss for fold 3, epoch 416: 224.4255671347341\n","Validation elbo for fold 3, epoch 416: -183.9625306167609\n","Training loss for fold 3, epoch 417: 224.07996048465853\n","Validation elbo for fold 3, epoch 417: -184.6688667948923\n","Training loss for fold 3, epoch 418: 224.12965417677356\n","Validation elbo for fold 3, epoch 418: -183.624926225956\n","Training loss for fold 3, epoch 419: 224.11890928206904\n","Validation elbo for fold 3, epoch 419: -184.45240119298535\n","Training loss for fold 3, epoch 420: 224.01237290905368\n","Validation elbo for fold 3, epoch 420: -182.5533932518693\n","Training loss for fold 3, epoch 421: 224.3029270787393\n","Validation elbo for fold 3, epoch 421: -184.8490353888198\n","Training loss for fold 3, epoch 422: 224.3864241569273\n","Validation elbo for fold 3, epoch 422: -183.86170243268066\n","Training loss for fold 3, epoch 423: 224.13076585338962\n","Validation elbo for fold 3, epoch 423: -183.61104474633692\n","Training loss for fold 3, epoch 424: 224.25523499519593\n","Validation elbo for fold 3, epoch 424: -183.97502723349513\n","Training loss for fold 3, epoch 425: 224.33666278469948\n","Validation elbo for fold 3, epoch 425: -184.66390992278434\n","Training loss for fold 3, epoch 426: 224.30598917315083\n","Validation elbo for fold 3, epoch 426: -183.73066569593396\n","Training loss for fold 3, epoch 427: 224.5058406706779\n","Validation elbo for fold 3, epoch 427: -183.2293715375817\n","Training loss for fold 3, epoch 428: 224.30265734272618\n","Validation elbo for fold 3, epoch 428: -184.8115562287821\n","Training loss for fold 3, epoch 429: 224.24846895279424\n","Validation elbo for fold 3, epoch 429: -183.44685129078889\n","Training loss for fold 3, epoch 430: 224.68981662873298\n","Validation elbo for fold 3, epoch 430: -184.78871469811094\n","Training loss for fold 3, epoch 431: 224.22420624763734\n","Validation elbo for fold 3, epoch 431: -183.06613116352756\n","Training loss for fold 3, epoch 432: 224.1015140164283\n","Validation elbo for fold 3, epoch 432: -184.21580958167468\n","Training loss for fold 3, epoch 433: 224.28018729917466\n","Validation elbo for fold 3, epoch 433: -183.94502120750937\n","Training loss for fold 3, epoch 434: 224.3364006780809\n","Validation elbo for fold 3, epoch 434: -184.25027620665276\n","Training loss for fold 3, epoch 435: 224.1409951486895\n","Validation elbo for fold 3, epoch 435: -184.98657271748738\n","Training loss for fold 3, epoch 436: 224.0205818914598\n","Validation elbo for fold 3, epoch 436: -184.01634598342113\n","Training loss for fold 3, epoch 437: 224.0527570170741\n","Validation elbo for fold 3, epoch 437: -183.62872824473652\n","Training loss for fold 3, epoch 438: 224.68337003646357\n","Validation elbo for fold 3, epoch 438: -185.76327953756692\n","Training loss for fold 3, epoch 439: 224.32642069170552\n","Validation elbo for fold 3, epoch 439: -184.35570382390338\n","Training loss for fold 3, epoch 440: 224.39276319934476\n","Validation elbo for fold 3, epoch 440: -184.85724208289088\n","Training loss for fold 3, epoch 441: 224.38785602200417\n","Validation elbo for fold 3, epoch 441: -184.29584756323328\n","Training loss for fold 3, epoch 442: 224.37442508820564\n","Validation elbo for fold 3, epoch 442: -184.10807975375246\n","Training loss for fold 3, epoch 443: 224.41492683656753\n","Validation elbo for fold 3, epoch 443: -185.46005352373191\n","Training loss for fold 3, epoch 444: 224.1000235772902\n","Validation elbo for fold 3, epoch 444: -183.80507313397197\n","Training loss for fold 3, epoch 445: 224.3413127776115\n","Validation elbo for fold 3, epoch 445: -185.87731472746583\n","Training loss for fold 3, epoch 446: 224.22782528784967\n","Validation elbo for fold 3, epoch 446: -184.3293283175904\n","Training loss for fold 3, epoch 447: 224.3215861166677\n","Validation elbo for fold 3, epoch 447: -184.40862054431466\n","Training loss for fold 3, epoch 448: 224.0709974227413\n","Validation elbo for fold 3, epoch 448: -185.08200058221763\n","Training loss for fold 3, epoch 449: 224.24400034258443\n","Validation elbo for fold 3, epoch 449: -184.99469250730536\n","Training loss for fold 3, epoch 450: 224.32883920977193\n","Validation elbo for fold 3, epoch 450: -183.2132704396892\n","Training loss for fold 3, epoch 451: 224.20735316122733\n","Validation elbo for fold 3, epoch 451: -184.44571766003844\n","Training loss for fold 3, epoch 452: 224.10625433152723\n","Validation elbo for fold 3, epoch 452: -184.8733269073233\n","Training loss for fold 3, epoch 453: 224.28440561602193\n","Validation elbo for fold 3, epoch 453: -183.8432637767835\n","Training loss for fold 3, epoch 454: 224.3928729641822\n","Validation elbo for fold 3, epoch 454: -184.75035386415198\n","Training loss for fold 3, epoch 455: 224.40117694485573\n","Validation elbo for fold 3, epoch 455: -183.6286154477051\n","Training loss for fold 3, epoch 456: 224.40018832299018\n","Validation elbo for fold 3, epoch 456: -183.93637164685015\n","Training loss for fold 3, epoch 457: 224.1568903769216\n","Validation elbo for fold 3, epoch 457: -182.7120650487417\n","Training loss for fold 3, epoch 458: 224.54620951990927\n","Validation elbo for fold 3, epoch 458: -184.43285840882186\n","Training loss for fold 3, epoch 459: 224.38877868652344\n","Validation elbo for fold 3, epoch 459: -184.40174623952117\n","Training loss for fold 3, epoch 460: 224.25520201652282\n","Validation elbo for fold 3, epoch 460: -184.89486367343247\n","Training loss for fold 3, epoch 461: 224.25125491234564\n","Validation elbo for fold 3, epoch 461: -184.27405364707244\n","Training loss for fold 3, epoch 462: 224.11014950659967\n","Validation elbo for fold 3, epoch 462: -183.17312771443883\n","Training loss for fold 3, epoch 463: 224.32730028706212\n","Validation elbo for fold 3, epoch 463: -185.239945366267\n","Training loss for fold 3, epoch 464: 224.19790747857863\n","Validation elbo for fold 3, epoch 464: -184.15540190139166\n","Training loss for fold 3, epoch 465: 224.24127369542276\n","Validation elbo for fold 3, epoch 465: -184.25942880938794\n","Training loss for fold 3, epoch 466: 224.2248771421371\n","Validation elbo for fold 3, epoch 466: -184.3585098509052\n","Training loss for fold 3, epoch 467: 224.42590602751702\n","Validation elbo for fold 3, epoch 467: -185.13292106333984\n","Training loss for fold 3, epoch 468: 224.16399654265373\n","Validation elbo for fold 3, epoch 468: -185.63638390933525\n","Training loss for fold 3, epoch 469: 224.3136215209961\n","Validation elbo for fold 3, epoch 469: -184.45574407317105\n","Training loss for fold 3, epoch 470: 224.32568900815903\n","Validation elbo for fold 3, epoch 470: -184.6412884235442\n","Training loss for fold 3, epoch 471: 224.4311087823683\n","Validation elbo for fold 3, epoch 471: -184.85213487951725\n","Training loss for fold 3, epoch 472: 224.21475539668913\n","Validation elbo for fold 3, epoch 472: -184.34289920967976\n","Training loss for fold 3, epoch 473: 224.26171112060547\n","Validation elbo for fold 3, epoch 473: -183.98586976859022\n","Training loss for fold 3, epoch 474: 224.93899339245212\n","Validation elbo for fold 3, epoch 474: -183.60404574028573\n","Training loss for fold 3, epoch 475: 224.18995321950604\n","Validation elbo for fold 3, epoch 475: -185.39653114062043\n","Training loss for fold 3, epoch 476: 224.31441620857484\n","Validation elbo for fold 3, epoch 476: -184.00881594998282\n","Training loss for fold 3, epoch 477: 224.3107902772965\n","Validation elbo for fold 3, epoch 477: -184.31154118954035\n","Training loss for fold 3, epoch 478: 224.32909417921496\n","Validation elbo for fold 3, epoch 478: -184.53118675032044\n","Training loss for fold 3, epoch 479: 224.21742470033706\n","Validation elbo for fold 3, epoch 479: -184.16635707519225\n","Training loss for fold 3, epoch 480: 224.29197520594442\n","Validation elbo for fold 3, epoch 480: -184.0872490796307\n","Training loss for fold 3, epoch 481: 224.15940561602193\n","Validation elbo for fold 3, epoch 481: -184.51980261341006\n","Training loss for fold 3, epoch 482: 224.2222179289787\n","Validation elbo for fold 3, epoch 482: -184.47150223245134\n","Training loss for fold 3, epoch 483: 224.15213652580016\n","Validation elbo for fold 3, epoch 483: -184.4537548714828\n","Training loss for fold 3, epoch 484: 224.36358322635775\n","Validation elbo for fold 3, epoch 484: -184.6365884784969\n","Training loss for fold 3, epoch 485: 224.43836261380105\n","Validation elbo for fold 3, epoch 485: -184.48713153633614\n","Training loss for fold 3, epoch 486: 224.31747215024888\n","Validation elbo for fold 3, epoch 486: -184.51349451940473\n","Training loss for fold 3, epoch 487: 224.32052833803237\n","Validation elbo for fold 3, epoch 487: -184.5469723821552\n","Training loss for fold 3, epoch 488: 224.16951604043282\n","Validation elbo for fold 3, epoch 488: -184.81102065696444\n","Training loss for fold 3, epoch 489: 224.46959981610698\n","Validation elbo for fold 3, epoch 489: -185.53702642837655\n","Training loss for fold 3, epoch 490: 224.39395633820564\n","Validation elbo for fold 3, epoch 490: -183.88605319217825\n","Training loss for fold 3, epoch 491: 224.34536423221712\n","Validation elbo for fold 3, epoch 491: -184.03481917851195\n","Training loss for fold 3, epoch 492: 224.35292348554057\n","Validation elbo for fold 3, epoch 492: -185.4983133119341\n","Training loss for fold 3, epoch 493: 224.3641126078944\n","Validation elbo for fold 3, epoch 493: -183.7792650185857\n","Training loss for fold 3, epoch 494: 224.10662226523124\n","Validation elbo for fold 3, epoch 494: -184.43984460831268\n","Training loss for fold 3, epoch 495: 224.04711077290196\n","Validation elbo for fold 3, epoch 495: -184.22341591341944\n","Training loss for fold 3, epoch 496: 224.19620563137917\n","Validation elbo for fold 3, epoch 496: -184.17479899066598\n","Training loss for fold 3, epoch 497: 224.23335364557082\n","Validation elbo for fold 3, epoch 497: -183.51633836889323\n","Training loss for fold 3, epoch 498: 224.1090545654297\n","Validation elbo for fold 3, epoch 498: -184.35558602203895\n","Training loss for fold 3, epoch 499: 224.24707744967552\n","Validation elbo for fold 3, epoch 499: -183.89428682004578\n","Fold 4\n","-------\n","Training loss for fold 4, epoch 0: 241.71960203109248\n","Validation elbo for fold 4, epoch 0: -188.28935184943614\n","Training loss for fold 4, epoch 1: 226.05930820588142\n","Validation elbo for fold 4, epoch 1: -183.29457309359788\n","Training loss for fold 4, epoch 2: 226.019651105327\n","Validation elbo for fold 4, epoch 2: -184.64287142520553\n","Training loss for fold 4, epoch 3: 225.7715321202432\n","Validation elbo for fold 4, epoch 3: -185.84879120864014\n","Training loss for fold 4, epoch 4: 226.45294460173577\n","Validation elbo for fold 4, epoch 4: -188.17564803646826\n","Training loss for fold 4, epoch 5: 225.821414332236\n","Validation elbo for fold 4, epoch 5: -184.8228172384759\n","Training loss for fold 4, epoch 6: 225.59612815610825\n","Validation elbo for fold 4, epoch 6: -183.70263335333178\n","Training loss for fold 4, epoch 7: 225.54208078692037\n","Validation elbo for fold 4, epoch 7: -184.34257136041248\n","Training loss for fold 4, epoch 8: 225.37398233721333\n","Validation elbo for fold 4, epoch 8: -186.4845259007207\n","Training loss for fold 4, epoch 9: 225.60568163471837\n","Validation elbo for fold 4, epoch 9: -183.6822000401864\n","Training loss for fold 4, epoch 10: 225.24804441390498\n","Validation elbo for fold 4, epoch 10: -183.6490786369677\n","Training loss for fold 4, epoch 11: 225.26190530100178\n","Validation elbo for fold 4, epoch 11: -186.25736010180748\n","Training loss for fold 4, epoch 12: 225.23598947832662\n","Validation elbo for fold 4, epoch 12: -185.73439966856986\n","Training loss for fold 4, epoch 13: 225.35994868124686\n","Validation elbo for fold 4, epoch 13: -186.6860803225291\n","Training loss for fold 4, epoch 14: 225.1490190567509\n","Validation elbo for fold 4, epoch 14: -186.33048752982023\n","Training loss for fold 4, epoch 15: 225.3490755634923\n","Validation elbo for fold 4, epoch 15: -184.96347472006605\n","Training loss for fold 4, epoch 16: 225.092889355075\n","Validation elbo for fold 4, epoch 16: -187.08673251858733\n","Training loss for fold 4, epoch 17: 225.34945703321887\n","Validation elbo for fold 4, epoch 17: -186.33854480712756\n","Training loss for fold 4, epoch 18: 225.38994376890122\n","Validation elbo for fold 4, epoch 18: -185.8365328799472\n","Training loss for fold 4, epoch 19: 225.2120806786322\n","Validation elbo for fold 4, epoch 19: -184.8394682066146\n","Training loss for fold 4, epoch 20: 225.32859383859943\n","Validation elbo for fold 4, epoch 20: -187.64220830424748\n","Training loss for fold 4, epoch 21: 225.00528126378214\n","Validation elbo for fold 4, epoch 21: -184.76172665376262\n","Training loss for fold 4, epoch 22: 225.28508807766823\n","Validation elbo for fold 4, epoch 22: -184.57710485892375\n","Training loss for fold 4, epoch 23: 225.22312804191344\n","Validation elbo for fold 4, epoch 23: -184.9542805088173\n","Training loss for fold 4, epoch 24: 224.9960701234879\n","Validation elbo for fold 4, epoch 24: -186.12945857675317\n","Training loss for fold 4, epoch 25: 225.01434818390877\n","Validation elbo for fold 4, epoch 25: -185.75656284550604\n","Training loss for fold 4, epoch 26: 225.01699706046813\n","Validation elbo for fold 4, epoch 26: -184.1633686061871\n","Training loss for fold 4, epoch 27: 225.40492100869454\n","Validation elbo for fold 4, epoch 27: -186.36223394555913\n","Training loss for fold 4, epoch 28: 225.14783059397053\n","Validation elbo for fold 4, epoch 28: -185.67967866958804\n","Training loss for fold 4, epoch 29: 224.92936288156818\n","Validation elbo for fold 4, epoch 29: -185.50241755219233\n","Training loss for fold 4, epoch 30: 224.99279539046748\n","Validation elbo for fold 4, epoch 30: -186.50757676947842\n","Training loss for fold 4, epoch 31: 224.81838890814012\n","Validation elbo for fold 4, epoch 31: -186.67029804724902\n","Training loss for fold 4, epoch 32: 224.92115857524257\n","Validation elbo for fold 4, epoch 32: -186.50525508548418\n","Training loss for fold 4, epoch 33: 224.68214441114856\n","Validation elbo for fold 4, epoch 33: -185.5727280653428\n","Training loss for fold 4, epoch 34: 224.84026755056072\n","Validation elbo for fold 4, epoch 34: -185.53247052258607\n","Training loss for fold 4, epoch 35: 224.90299618628717\n","Validation elbo for fold 4, epoch 35: -187.25041518356124\n","Training loss for fold 4, epoch 36: 225.09965219805318\n","Validation elbo for fold 4, epoch 36: -186.54268866457716\n","Training loss for fold 4, epoch 37: 224.84728413243448\n","Validation elbo for fold 4, epoch 37: -185.77859472117456\n","Training loss for fold 4, epoch 38: 224.75953895814956\n","Validation elbo for fold 4, epoch 38: -187.68033572187318\n","Training loss for fold 4, epoch 39: 224.74525377827305\n","Validation elbo for fold 4, epoch 39: -185.0159073798111\n","Training loss for fold 4, epoch 40: 224.71488608083416\n","Validation elbo for fold 4, epoch 40: -186.11613428152322\n","Training loss for fold 4, epoch 41: 224.67891988446635\n","Validation elbo for fold 4, epoch 41: -187.96268874019458\n","Training loss for fold 4, epoch 42: 224.77972436720324\n","Validation elbo for fold 4, epoch 42: -184.8607398293074\n","Training loss for fold 4, epoch 43: 224.83801909415953\n","Validation elbo for fold 4, epoch 43: -186.99387828117733\n","Training loss for fold 4, epoch 44: 224.71834736485636\n","Validation elbo for fold 4, epoch 44: -185.78141606203954\n","Training loss for fold 4, epoch 45: 224.68773183514995\n","Validation elbo for fold 4, epoch 45: -185.74181269229376\n","Training loss for fold 4, epoch 46: 224.82860048355596\n","Validation elbo for fold 4, epoch 46: -187.61794519048703\n","Training loss for fold 4, epoch 47: 224.6752432546308\n","Validation elbo for fold 4, epoch 47: -185.10521277127077\n","Training loss for fold 4, epoch 48: 224.78897267003214\n","Validation elbo for fold 4, epoch 48: -186.65049295225555\n","Training loss for fold 4, epoch 49: 224.94431920205392\n","Validation elbo for fold 4, epoch 49: -185.61636190137688\n","Training loss for fold 4, epoch 50: 224.8491023894279\n","Validation elbo for fold 4, epoch 50: -186.72494653247335\n","Training loss for fold 4, epoch 51: 224.5351345923639\n","Validation elbo for fold 4, epoch 51: -185.02780601222554\n","Training loss for fold 4, epoch 52: 224.64844242219002\n","Validation elbo for fold 4, epoch 52: -185.927097979972\n","Training loss for fold 4, epoch 53: 224.50235502181513\n","Validation elbo for fold 4, epoch 53: -187.90507860058582\n","Training loss for fold 4, epoch 54: 225.1091057562059\n","Validation elbo for fold 4, epoch 54: -186.98808010693054\n","Training loss for fold 4, epoch 55: 224.73685726042717\n","Validation elbo for fold 4, epoch 55: -186.58458450031551\n","Training loss for fold 4, epoch 56: 224.9353738600208\n","Validation elbo for fold 4, epoch 56: -185.5425040215317\n","Training loss for fold 4, epoch 57: 224.6918251283707\n","Validation elbo for fold 4, epoch 57: -183.98976650050201\n","Training loss for fold 4, epoch 58: 225.1136974211662\n","Validation elbo for fold 4, epoch 58: -186.78884452851312\n","Training loss for fold 4, epoch 59: 224.63289765388734\n","Validation elbo for fold 4, epoch 59: -185.37058893261926\n","Training loss for fold 4, epoch 60: 224.81341995731478\n","Validation elbo for fold 4, epoch 60: -186.68818194379116\n","Training loss for fold 4, epoch 61: 224.89149425875755\n","Validation elbo for fold 4, epoch 61: -186.2851555180723\n","Training loss for fold 4, epoch 62: 224.5561287172379\n","Validation elbo for fold 4, epoch 62: -186.42759076285614\n","Training loss for fold 4, epoch 63: 224.60655286235195\n","Validation elbo for fold 4, epoch 63: -185.6863873728346\n","Training loss for fold 4, epoch 64: 224.48913426553048\n","Validation elbo for fold 4, epoch 64: -186.5906675646683\n","Training loss for fold 4, epoch 65: 224.68646067957724\n","Validation elbo for fold 4, epoch 65: -187.55294011274384\n","Training loss for fold 4, epoch 66: 224.34257531935168\n","Validation elbo for fold 4, epoch 66: -184.947243402485\n","Training loss for fold 4, epoch 67: 224.67165104035408\n","Validation elbo for fold 4, epoch 67: -184.29901773179074\n","Training loss for fold 4, epoch 68: 224.51658581149192\n","Validation elbo for fold 4, epoch 68: -187.10712312957952\n","Training loss for fold 4, epoch 69: 224.6521532612462\n","Validation elbo for fold 4, epoch 69: -186.57839630154737\n","Training loss for fold 4, epoch 70: 224.67116251299458\n","Validation elbo for fold 4, epoch 70: -185.68472316553277\n","Training loss for fold 4, epoch 71: 224.41241184357673\n","Validation elbo for fold 4, epoch 71: -184.62143224573708\n","Training loss for fold 4, epoch 72: 224.49332846364666\n","Validation elbo for fold 4, epoch 72: -185.71017201503616\n","Training loss for fold 4, epoch 73: 224.5489728373866\n","Validation elbo for fold 4, epoch 73: -186.6241579013897\n","Training loss for fold 4, epoch 74: 224.58599632017075\n","Validation elbo for fold 4, epoch 74: -184.88773789356873\n","Training loss for fold 4, epoch 75: 224.4662340225712\n","Validation elbo for fold 4, epoch 75: -186.91168395792408\n","Training loss for fold 4, epoch 76: 224.43501847790134\n","Validation elbo for fold 4, epoch 76: -184.21800126710468\n","Training loss for fold 4, epoch 77: 224.6300792078818\n","Validation elbo for fold 4, epoch 77: -187.19259897728688\n","Training loss for fold 4, epoch 78: 224.61594440091042\n","Validation elbo for fold 4, epoch 78: -186.1282624555756\n","Training loss for fold 4, epoch 79: 224.46299349877143\n","Validation elbo for fold 4, epoch 79: -185.95519784641243\n","Training loss for fold 4, epoch 80: 224.56976638301725\n","Validation elbo for fold 4, epoch 80: -186.4148864774472\n","Training loss for fold 4, epoch 81: 224.76633945588142\n","Validation elbo for fold 4, epoch 81: -184.4168019460828\n","Training loss for fold 4, epoch 82: 224.3459987025107\n","Validation elbo for fold 4, epoch 82: -184.1233874694609\n","Training loss for fold 4, epoch 83: 224.7603058353547\n","Validation elbo for fold 4, epoch 83: -186.14956789198834\n","Training loss for fold 4, epoch 84: 224.76558685302734\n","Validation elbo for fold 4, epoch 84: -187.55446411041595\n","Training loss for fold 4, epoch 85: 224.7721153997606\n","Validation elbo for fold 4, epoch 85: -188.37972551608016\n","Training loss for fold 4, epoch 86: 224.42096291818928\n","Validation elbo for fold 4, epoch 86: -186.14914517539648\n","Training loss for fold 4, epoch 87: 224.69847008489793\n","Validation elbo for fold 4, epoch 87: -186.44726387756936\n","Training loss for fold 4, epoch 88: 224.64891667519845\n","Validation elbo for fold 4, epoch 88: -184.82504567434856\n","Training loss for fold 4, epoch 89: 224.44906173213835\n","Validation elbo for fold 4, epoch 89: -186.58728712827366\n","Training loss for fold 4, epoch 90: 224.56136322021484\n","Validation elbo for fold 4, epoch 90: -185.0742365657864\n","Training loss for fold 4, epoch 91: 224.49553729641823\n","Validation elbo for fold 4, epoch 91: -186.255864624136\n","Training loss for fold 4, epoch 92: 224.4092047906691\n","Validation elbo for fold 4, epoch 92: -185.91775343021533\n","Training loss for fold 4, epoch 93: 224.60315089071952\n","Validation elbo for fold 4, epoch 93: -185.5272903398996\n","Training loss for fold 4, epoch 94: 224.71776506977696\n","Validation elbo for fold 4, epoch 94: -187.28748378090253\n","Training loss for fold 4, epoch 95: 224.4451643420804\n","Validation elbo for fold 4, epoch 95: -187.01802884957485\n","Training loss for fold 4, epoch 96: 224.39458613241874\n","Validation elbo for fold 4, epoch 96: -185.0090872530143\n","Training loss for fold 4, epoch 97: 224.32457437822896\n","Validation elbo for fold 4, epoch 97: -185.65924964932648\n","Training loss for fold 4, epoch 98: 224.38351268153036\n","Validation elbo for fold 4, epoch 98: -186.61980277912406\n","Training loss for fold 4, epoch 99: 224.3076659171812\n","Validation elbo for fold 4, epoch 99: -184.456013080947\n","Training loss for fold 4, epoch 100: 224.68225737540953\n","Validation elbo for fold 4, epoch 100: -186.22033898505822\n","Training loss for fold 4, epoch 101: 224.4632359166299\n","Validation elbo for fold 4, epoch 101: -186.56501212750322\n","Training loss for fold 4, epoch 102: 224.42258920977193\n","Validation elbo for fold 4, epoch 102: -186.31812303042426\n","Training loss for fold 4, epoch 103: 224.22029679821384\n","Validation elbo for fold 4, epoch 103: -185.01764293864392\n","Training loss for fold 4, epoch 104: 224.7242709744361\n","Validation elbo for fold 4, epoch 104: -186.54590457725624\n","Training loss for fold 4, epoch 105: 224.4691890593498\n","Validation elbo for fold 4, epoch 105: -187.15753692272997\n","Training loss for fold 4, epoch 106: 224.48955757387222\n","Validation elbo for fold 4, epoch 106: -185.61029076435318\n","Training loss for fold 4, epoch 107: 224.36004195674772\n","Validation elbo for fold 4, epoch 107: -186.77669418632277\n","Training loss for fold 4, epoch 108: 224.352786156439\n","Validation elbo for fold 4, epoch 108: -184.52725520528384\n","Training loss for fold 4, epoch 109: 224.49398483768587\n","Validation elbo for fold 4, epoch 109: -185.9999097596456\n","Training loss for fold 4, epoch 110: 224.31079446115803\n","Validation elbo for fold 4, epoch 110: -185.75298309252958\n","Training loss for fold 4, epoch 111: 224.47913877425654\n","Validation elbo for fold 4, epoch 111: -186.41126239418855\n","Training loss for fold 4, epoch 112: 224.46192710630356\n","Validation elbo for fold 4, epoch 112: -185.45234064945282\n","Training loss for fold 4, epoch 113: 224.42111107610887\n","Validation elbo for fold 4, epoch 113: -185.37398776034317\n","Training loss for fold 4, epoch 114: 224.27591434601814\n","Validation elbo for fold 4, epoch 114: -186.16384600187402\n","Training loss for fold 4, epoch 115: 224.30665194603705\n","Validation elbo for fold 4, epoch 115: -185.7639778165285\n","Training loss for fold 4, epoch 116: 224.4825626496346\n","Validation elbo for fold 4, epoch 116: -185.86823907053355\n","Training loss for fold 4, epoch 117: 224.34634079471712\n","Validation elbo for fold 4, epoch 117: -186.04822937030548\n","Training loss for fold 4, epoch 118: 224.55704596734816\n","Validation elbo for fold 4, epoch 118: -187.21876803771087\n","Training loss for fold 4, epoch 119: 224.39326723160283\n","Validation elbo for fold 4, epoch 119: -187.0649128760456\n","Training loss for fold 4, epoch 120: 224.4301253288023\n","Validation elbo for fold 4, epoch 120: -186.2676127097619\n","Training loss for fold 4, epoch 121: 224.38312628961378\n","Validation elbo for fold 4, epoch 121: -185.57359461557513\n","Training loss for fold 4, epoch 122: 224.57681668189264\n","Validation elbo for fold 4, epoch 122: -185.5757422604198\n","Training loss for fold 4, epoch 123: 224.3461165889617\n","Validation elbo for fold 4, epoch 123: -186.19580844815172\n","Training loss for fold 4, epoch 124: 224.46355782785724\n","Validation elbo for fold 4, epoch 124: -186.25641545634272\n","Training loss for fold 4, epoch 125: 224.38338445848035\n","Validation elbo for fold 4, epoch 125: -185.91426447021155\n","Training loss for fold 4, epoch 126: 224.21616929577243\n","Validation elbo for fold 4, epoch 126: -186.93334998831614\n","Training loss for fold 4, epoch 127: 224.43695511356478\n","Validation elbo for fold 4, epoch 127: -185.9306469007595\n","Training loss for fold 4, epoch 128: 224.46771264845324\n","Validation elbo for fold 4, epoch 128: -186.16532555871686\n","Training loss for fold 4, epoch 129: 224.41536761868386\n","Validation elbo for fold 4, epoch 129: -186.33468417064745\n","Training loss for fold 4, epoch 130: 224.7093235138924\n","Validation elbo for fold 4, epoch 130: -187.1744185063498\n","Training loss for fold 4, epoch 131: 224.4730987548828\n","Validation elbo for fold 4, epoch 131: -184.82131791987752\n","Training loss for fold 4, epoch 132: 224.54251664684665\n","Validation elbo for fold 4, epoch 132: -186.2099431730254\n","Training loss for fold 4, epoch 133: 224.38872823407573\n","Validation elbo for fold 4, epoch 133: -185.82363696182222\n","Training loss for fold 4, epoch 134: 224.25345316240865\n","Validation elbo for fold 4, epoch 134: -186.1405334372834\n","Training loss for fold 4, epoch 135: 224.53034087150328\n","Validation elbo for fold 4, epoch 135: -186.9777540707012\n","Training loss for fold 4, epoch 136: 224.58485289542907\n","Validation elbo for fold 4, epoch 136: -186.41563903013378\n","Training loss for fold 4, epoch 137: 224.38057659518333\n","Validation elbo for fold 4, epoch 137: -186.36796884767003\n","Training loss for fold 4, epoch 138: 224.10304850916708\n","Validation elbo for fold 4, epoch 138: -186.05919374564274\n","Training loss for fold 4, epoch 139: 224.42113790204448\n","Validation elbo for fold 4, epoch 139: -184.97723195609558\n","Training loss for fold 4, epoch 140: 224.45321138443487\n","Validation elbo for fold 4, epoch 140: -186.94597984399357\n","Training loss for fold 4, epoch 141: 224.23833834740424\n","Validation elbo for fold 4, epoch 141: -186.13137471121553\n","Training loss for fold 4, epoch 142: 224.34336902249245\n","Validation elbo for fold 4, epoch 142: -184.62041221324617\n","Training loss for fold 4, epoch 143: 224.46030179915888\n","Validation elbo for fold 4, epoch 143: -185.79751257156516\n","Training loss for fold 4, epoch 144: 224.27618285148375\n","Validation elbo for fold 4, epoch 144: -186.40947592580443\n","Training loss for fold 4, epoch 145: 224.50072971467048\n","Validation elbo for fold 4, epoch 145: -186.45965538210552\n","Training loss for fold 4, epoch 146: 224.47407950124432\n","Validation elbo for fold 4, epoch 146: -185.4572198495535\n","Training loss for fold 4, epoch 147: 224.13585416732295\n","Validation elbo for fold 4, epoch 147: -185.68605016109495\n","Training loss for fold 4, epoch 148: 224.6643595541677\n","Validation elbo for fold 4, epoch 148: -185.59727058021193\n","Training loss for fold 4, epoch 149: 224.40284039897304\n","Validation elbo for fold 4, epoch 149: -185.1612579805822\n","Training loss for fold 4, epoch 150: 224.32689248361896\n","Validation elbo for fold 4, epoch 150: -185.7276953532849\n","Training loss for fold 4, epoch 151: 224.01136484453755\n","Validation elbo for fold 4, epoch 151: -185.65230242170145\n","Training loss for fold 4, epoch 152: 224.19397809428554\n","Validation elbo for fold 4, epoch 152: -185.70002269459076\n","Training loss for fold 4, epoch 153: 224.28914740777785\n","Validation elbo for fold 4, epoch 153: -184.69905265185514\n","Training loss for fold 4, epoch 154: 223.9708242108745\n","Validation elbo for fold 4, epoch 154: -184.89935054954796\n","Training loss for fold 4, epoch 155: 224.26344176261657\n","Validation elbo for fold 4, epoch 155: -184.85226241105545\n","Training loss for fold 4, epoch 156: 224.14132444320185\n","Validation elbo for fold 4, epoch 156: -185.59505199850526\n","Training loss for fold 4, epoch 157: 224.4062054541803\n","Validation elbo for fold 4, epoch 157: -185.22650131547965\n","Training loss for fold 4, epoch 158: 224.48433759135585\n","Validation elbo for fold 4, epoch 158: -187.07876087014847\n","Training loss for fold 4, epoch 159: 224.3173618931924\n","Validation elbo for fold 4, epoch 159: -185.41780054582998\n","Training loss for fold 4, epoch 160: 224.07008238761657\n","Validation elbo for fold 4, epoch 160: -185.4322061102142\n","Training loss for fold 4, epoch 161: 224.10401670394404\n","Validation elbo for fold 4, epoch 161: -185.39964444608427\n","Training loss for fold 4, epoch 162: 224.4387733705582\n","Validation elbo for fold 4, epoch 162: -185.49472788327273\n","Training loss for fold 4, epoch 163: 224.58032177340598\n","Validation elbo for fold 4, epoch 163: -186.19470178393016\n","Training loss for fold 4, epoch 164: 224.33373974215598\n","Validation elbo for fold 4, epoch 164: -185.7017694117735\n","Training loss for fold 4, epoch 165: 224.21879331527217\n","Validation elbo for fold 4, epoch 165: -184.8977382144576\n","Training loss for fold 4, epoch 166: 224.27941230035597\n","Validation elbo for fold 4, epoch 166: -185.59228936342913\n","Training loss for fold 4, epoch 167: 224.23917758080268\n","Validation elbo for fold 4, epoch 167: -185.32242290385423\n","Training loss for fold 4, epoch 168: 224.45919381418537\n","Validation elbo for fold 4, epoch 168: -186.02082565062\n","Training loss for fold 4, epoch 169: 224.42334648870653\n","Validation elbo for fold 4, epoch 169: -186.1379280521562\n","Training loss for fold 4, epoch 170: 224.37658839071952\n","Validation elbo for fold 4, epoch 170: -184.64731223241358\n","Training loss for fold 4, epoch 171: 224.27358098183907\n","Validation elbo for fold 4, epoch 171: -185.60858068807696\n","Training loss for fold 4, epoch 172: 224.3043006158644\n","Validation elbo for fold 4, epoch 172: -185.61622933841028\n","Training loss for fold 4, epoch 173: 224.15991875433153\n","Validation elbo for fold 4, epoch 173: -184.31164524039178\n","Training loss for fold 4, epoch 174: 224.3677003922001\n","Validation elbo for fold 4, epoch 174: -186.51950756336743\n","Training loss for fold 4, epoch 175: 224.3620854039346\n","Validation elbo for fold 4, epoch 175: -185.60340467010204\n","Training loss for fold 4, epoch 176: 224.38292226483745\n","Validation elbo for fold 4, epoch 176: -186.8206816801652\n","Training loss for fold 4, epoch 177: 224.24364225326045\n","Validation elbo for fold 4, epoch 177: -185.78074544545498\n","Training loss for fold 4, epoch 178: 224.72069771059097\n","Validation elbo for fold 4, epoch 178: -186.3105448458138\n","Training loss for fold 4, epoch 179: 224.35571338284402\n","Validation elbo for fold 4, epoch 179: -185.39979870521975\n","Training loss for fold 4, epoch 180: 224.11535989084553\n","Validation elbo for fold 4, epoch 180: -186.84362187680324\n","Training loss for fold 4, epoch 181: 224.17548148862778\n","Validation elbo for fold 4, epoch 181: -186.48272144640688\n","Training loss for fold 4, epoch 182: 224.2603996030746\n","Validation elbo for fold 4, epoch 182: -184.90363051804442\n","Training loss for fold 4, epoch 183: 224.45939070178616\n","Validation elbo for fold 4, epoch 183: -185.21139365608448\n","Training loss for fold 4, epoch 184: 224.2985145814957\n","Validation elbo for fold 4, epoch 184: -185.34815881921543\n","Training loss for fold 4, epoch 185: 224.4057873141381\n","Validation elbo for fold 4, epoch 185: -185.5245472077865\n","Training loss for fold 4, epoch 186: 224.2328882525044\n","Validation elbo for fold 4, epoch 186: -185.16282686833046\n","Training loss for fold 4, epoch 187: 224.51793621432395\n","Validation elbo for fold 4, epoch 187: -185.27221234119787\n","Training loss for fold 4, epoch 188: 224.24907881213772\n","Validation elbo for fold 4, epoch 188: -185.7462926755033\n","Training loss for fold 4, epoch 189: 224.40400843466483\n","Validation elbo for fold 4, epoch 189: -185.00047939705982\n","Training loss for fold 4, epoch 190: 224.46366143995715\n","Validation elbo for fold 4, epoch 190: -187.57922780159737\n","Training loss for fold 4, epoch 191: 224.2764676001764\n","Validation elbo for fold 4, epoch 191: -185.17074572270613\n","Training loss for fold 4, epoch 192: 224.67512955204134\n","Validation elbo for fold 4, epoch 192: -186.7634122146817\n","Training loss for fold 4, epoch 193: 224.39285795150263\n","Validation elbo for fold 4, epoch 193: -186.27267756396685\n","Training loss for fold 4, epoch 194: 224.33524273287864\n","Validation elbo for fold 4, epoch 194: -186.47367258470456\n","Training loss for fold 4, epoch 195: 224.4632359166299\n","Validation elbo for fold 4, epoch 195: -186.36871323669902\n","Training loss for fold 4, epoch 196: 224.2458493632655\n","Validation elbo for fold 4, epoch 196: -185.44011029286196\n","Training loss for fold 4, epoch 197: 224.34957713465536\n","Validation elbo for fold 4, epoch 197: -185.27743435444327\n","Training loss for fold 4, epoch 198: 224.3534371160692\n","Validation elbo for fold 4, epoch 198: -185.90790640141734\n","Training loss for fold 4, epoch 199: 224.21043469828945\n","Validation elbo for fold 4, epoch 199: -186.49053734614643\n","Training loss for fold 4, epoch 200: 224.18987889443673\n","Validation elbo for fold 4, epoch 200: -185.928292716483\n","Training loss for fold 4, epoch 201: 224.07606580180507\n","Validation elbo for fold 4, epoch 201: -184.97706330673392\n","Training loss for fold 4, epoch 202: 224.3545874318769\n","Validation elbo for fold 4, epoch 202: -184.02646086908385\n","Training loss for fold 4, epoch 203: 224.41239781533517\n","Validation elbo for fold 4, epoch 203: -186.24628539393422\n","Training loss for fold 4, epoch 204: 224.11510270641696\n","Validation elbo for fold 4, epoch 204: -185.89585838822632\n","Training loss for fold 4, epoch 205: 224.23533310428743\n","Validation elbo for fold 4, epoch 205: -186.26311962931544\n","Training loss for fold 4, epoch 206: 224.1066911758915\n","Validation elbo for fold 4, epoch 206: -184.6106005962253\n","Training loss for fold 4, epoch 207: 224.22494826778288\n","Validation elbo for fold 4, epoch 207: -186.8862190605783\n","Training loss for fold 4, epoch 208: 224.20774595199092\n","Validation elbo for fold 4, epoch 208: -187.7997783791572\n","Training loss for fold 4, epoch 209: 224.4104788995558\n","Validation elbo for fold 4, epoch 209: -186.09657772996428\n","Training loss for fold 4, epoch 210: 224.2505894322549\n","Validation elbo for fold 4, epoch 210: -185.7110333581952\n","Training loss for fold 4, epoch 211: 224.2474864836662\n","Validation elbo for fold 4, epoch 211: -185.81458040070336\n","Training loss for fold 4, epoch 212: 224.24241515128844\n","Validation elbo for fold 4, epoch 212: -185.40098634203252\n","Training loss for fold 4, epoch 213: 224.41480944233555\n","Validation elbo for fold 4, epoch 213: -186.29237757256516\n","Training loss for fold 4, epoch 214: 224.22603188791584\n","Validation elbo for fold 4, epoch 214: -186.24966939792287\n","Training loss for fold 4, epoch 215: 224.0277370329826\n","Validation elbo for fold 4, epoch 215: -184.86416083141202\n","Training loss for fold 4, epoch 216: 224.58640929191344\n","Validation elbo for fold 4, epoch 216: -186.704492864391\n","Training loss for fold 4, epoch 217: 224.5275177494172\n","Validation elbo for fold 4, epoch 217: -186.46023294762\n","Training loss for fold 4, epoch 218: 224.3188922020697\n","Validation elbo for fold 4, epoch 218: -185.6255112570264\n","Training loss for fold 4, epoch 219: 224.41524038007182\n","Validation elbo for fold 4, epoch 219: -186.07036220159645\n","Training loss for fold 4, epoch 220: 224.21354158463018\n","Validation elbo for fold 4, epoch 220: -185.92319826352787\n","Training loss for fold 4, epoch 221: 224.22215713993197\n","Validation elbo for fold 4, epoch 221: -186.20227054643652\n","Training loss for fold 4, epoch 222: 224.35225554435485\n","Validation elbo for fold 4, epoch 222: -185.29801659933776\n","Training loss for fold 4, epoch 223: 224.2019527804467\n","Validation elbo for fold 4, epoch 223: -186.2063867815329\n","Training loss for fold 4, epoch 224: 224.47189429498488\n","Validation elbo for fold 4, epoch 224: -186.23646255238899\n","Training loss for fold 4, epoch 225: 224.21745743290072\n","Validation elbo for fold 4, epoch 225: -185.5196838825497\n","Training loss for fold 4, epoch 226: 224.5006841844128\n","Validation elbo for fold 4, epoch 226: -186.7035294391461\n","Training loss for fold 4, epoch 227: 224.2110356976909\n","Validation elbo for fold 4, epoch 227: -185.312552754073\n","Training loss for fold 4, epoch 228: 224.2818332795174\n","Validation elbo for fold 4, epoch 228: -185.79694361548064\n","Training loss for fold 4, epoch 229: 224.52576594198905\n","Validation elbo for fold 4, epoch 229: -186.73910944715158\n","Training loss for fold 4, epoch 230: 224.60499720419608\n","Validation elbo for fold 4, epoch 230: -186.20033683730378\n","Training loss for fold 4, epoch 231: 224.58019379646547\n","Validation elbo for fold 4, epoch 231: -186.1650998993172\n","Training loss for fold 4, epoch 232: 224.33992078227382\n","Validation elbo for fold 4, epoch 232: -186.0757044220338\n","Training loss for fold 4, epoch 233: 224.16144463323778\n","Validation elbo for fold 4, epoch 233: -185.7290247723246\n","Training loss for fold 4, epoch 234: 224.4359667378087\n","Validation elbo for fold 4, epoch 234: -185.5456921042554\n","Training loss for fold 4, epoch 235: 224.12314261159588\n","Validation elbo for fold 4, epoch 235: -184.70488493249934\n","Training loss for fold 4, epoch 236: 224.33366246377267\n","Validation elbo for fold 4, epoch 236: -185.13922075863894\n","Training loss for fold 4, epoch 237: 224.27832744967552\n","Validation elbo for fold 4, epoch 237: -185.355478993476\n","Training loss for fold 4, epoch 238: 224.3836165397398\n","Validation elbo for fold 4, epoch 238: -186.14512766942585\n","Training loss for fold 4, epoch 239: 224.43362992809665\n","Validation elbo for fold 4, epoch 239: -186.34749922474896\n","Training loss for fold 4, epoch 240: 224.16501174434538\n","Validation elbo for fold 4, epoch 240: -185.36374214656922\n","Training loss for fold 4, epoch 241: 224.17422903737713\n","Validation elbo for fold 4, epoch 241: -185.61544714277198\n","Training loss for fold 4, epoch 242: 224.31269196541078\n","Validation elbo for fold 4, epoch 242: -185.3361889087596\n","Training loss for fold 4, epoch 243: 224.264283949329\n","Validation elbo for fold 4, epoch 243: -185.95016997878335\n","Training loss for fold 4, epoch 244: 224.31119685019218\n","Validation elbo for fold 4, epoch 244: -186.59044175025699\n","Training loss for fold 4, epoch 245: 224.50337736068232\n","Validation elbo for fold 4, epoch 245: -187.63691156026078\n","Training loss for fold 4, epoch 246: 224.35585538802607\n","Validation elbo for fold 4, epoch 246: -186.10929810247808\n","Training loss for fold 4, epoch 247: 224.18255196848224\n","Validation elbo for fold 4, epoch 247: -186.23190512631948\n","Training loss for fold 4, epoch 248: 224.26893591111707\n","Validation elbo for fold 4, epoch 248: -186.0622143384755\n","Training loss for fold 4, epoch 249: 224.41927263813633\n","Validation elbo for fold 4, epoch 249: -185.63641151207344\n","Training loss for fold 4, epoch 250: 224.46712297008884\n","Validation elbo for fold 4, epoch 250: -185.74116510833255\n","Training loss for fold 4, epoch 251: 224.47383388396233\n","Validation elbo for fold 4, epoch 251: -186.1377803678286\n","Training loss for fold 4, epoch 252: 224.30579794606854\n","Validation elbo for fold 4, epoch 252: -186.7706944699983\n","Training loss for fold 4, epoch 253: 224.3110070997669\n","Validation elbo for fold 4, epoch 253: -185.55111342030216\n","Training loss for fold 4, epoch 254: 224.29337360012917\n","Validation elbo for fold 4, epoch 254: -185.69626709643214\n","Training loss for fold 4, epoch 255: 224.58182894799018\n","Validation elbo for fold 4, epoch 255: -186.53416230414348\n","Training loss for fold 4, epoch 256: 224.3746576616841\n","Validation elbo for fold 4, epoch 256: -186.10003267498905\n","Training loss for fold 4, epoch 257: 224.35567917362337\n","Validation elbo for fold 4, epoch 257: -186.41838908472315\n","Training loss for fold 4, epoch 258: 224.32139341292842\n","Validation elbo for fold 4, epoch 258: -185.77056294830575\n","Training loss for fold 4, epoch 259: 224.5928251204952\n","Validation elbo for fold 4, epoch 259: -186.34249100215436\n","Training loss for fold 4, epoch 260: 224.22070263278098\n","Validation elbo for fold 4, epoch 260: -186.6063431313145\n","Training loss for fold 4, epoch 261: 224.17157400808026\n","Validation elbo for fold 4, epoch 261: -186.52526754371092\n","Training loss for fold 4, epoch 262: 224.25728287235384\n","Validation elbo for fold 4, epoch 262: -186.46919593028892\n","Training loss for fold 4, epoch 263: 224.19410484067856\n","Validation elbo for fold 4, epoch 263: -186.61774498065836\n","Training loss for fold 4, epoch 264: 224.58420931908393\n","Validation elbo for fold 4, epoch 264: -186.63842742008455\n","Training loss for fold 4, epoch 265: 224.33714269822644\n","Validation elbo for fold 4, epoch 265: -185.40903960385359\n","Training loss for fold 4, epoch 266: 224.42103699714906\n","Validation elbo for fold 4, epoch 266: -186.29160718745538\n","Training loss for fold 4, epoch 267: 224.04295275288243\n","Validation elbo for fold 4, epoch 267: -186.1361500306252\n","Training loss for fold 4, epoch 268: 224.1389221683625\n","Validation elbo for fold 4, epoch 268: -187.16030093435458\n","Training loss for fold 4, epoch 269: 224.32474936208416\n","Validation elbo for fold 4, epoch 269: -185.4819413014361\n","Training loss for fold 4, epoch 270: 224.1306110505135\n","Validation elbo for fold 4, epoch 270: -185.10053869062108\n","Training loss for fold 4, epoch 271: 224.2895515195785\n","Validation elbo for fold 4, epoch 271: -184.86122933563973\n","Training loss for fold 4, epoch 272: 224.0258323915543\n","Validation elbo for fold 4, epoch 272: -186.59773945891158\n","Training loss for fold 4, epoch 273: 224.2096648677703\n","Validation elbo for fold 4, epoch 273: -184.6874278592682\n","Training loss for fold 4, epoch 274: 224.1580035301947\n","Validation elbo for fold 4, epoch 274: -186.0449696798753\n","Training loss for fold 4, epoch 275: 224.3852029615833\n","Validation elbo for fold 4, epoch 275: -185.1774379754208\n","Training loss for fold 4, epoch 276: 224.52322412306262\n","Validation elbo for fold 4, epoch 276: -186.4113145530409\n","Training loss for fold 4, epoch 277: 224.14923883253527\n","Validation elbo for fold 4, epoch 277: -185.77786270751545\n","Training loss for fold 4, epoch 278: 224.63082196635585\n","Validation elbo for fold 4, epoch 278: -184.99172163033074\n","Training loss for fold 4, epoch 279: 224.4845189740581\n","Validation elbo for fold 4, epoch 279: -187.06087705500477\n","Training loss for fold 4, epoch 280: 224.4723879906439\n","Validation elbo for fold 4, epoch 280: -185.48731899989676\n","Training loss for fold 4, epoch 281: 224.34104427214592\n","Validation elbo for fold 4, epoch 281: -187.22611402826857\n","Training loss for fold 4, epoch 282: 224.09241067209553\n","Validation elbo for fold 4, epoch 282: -186.09637506490725\n","Training loss for fold 4, epoch 283: 224.15303310271233\n","Validation elbo for fold 4, epoch 283: -185.24077349233718\n","Training loss for fold 4, epoch 284: 224.09304858792214\n","Validation elbo for fold 4, epoch 284: -186.05530093778984\n","Training loss for fold 4, epoch 285: 224.17602268342048\n","Validation elbo for fold 4, epoch 285: -186.14351141122143\n","Training loss for fold 4, epoch 286: 224.40254679033833\n","Validation elbo for fold 4, epoch 286: -185.86155679693968\n","Training loss for fold 4, epoch 287: 224.2905241443265\n","Validation elbo for fold 4, epoch 287: -186.4339813794265\n","Training loss for fold 4, epoch 288: 224.19553744408393\n","Validation elbo for fold 4, epoch 288: -184.2941039007953\n","Training loss for fold 4, epoch 289: 224.19233752835183\n","Validation elbo for fold 4, epoch 289: -184.81491571891263\n","Training loss for fold 4, epoch 290: 224.68226303592806\n","Validation elbo for fold 4, epoch 290: -186.2913123371162\n","Training loss for fold 4, epoch 291: 224.36288968978388\n","Validation elbo for fold 4, epoch 291: -185.72260372325982\n","Training loss for fold 4, epoch 292: 224.1371115407636\n","Validation elbo for fold 4, epoch 292: -186.10752489381264\n","Training loss for fold 4, epoch 293: 224.10262224751133\n","Validation elbo for fold 4, epoch 293: -186.27413181202365\n","Training loss for fold 4, epoch 294: 224.2783670733052\n","Validation elbo for fold 4, epoch 294: -186.54949757479747\n","Training loss for fold 4, epoch 295: 223.97982172812186\n","Validation elbo for fold 4, epoch 295: -185.04208146768377\n","Training loss for fold 4, epoch 296: 224.21070640317856\n","Validation elbo for fold 4, epoch 296: -185.83210588957107\n","Training loss for fold 4, epoch 297: 224.2455094860446\n","Validation elbo for fold 4, epoch 297: -185.85038278818547\n","Training loss for fold 4, epoch 298: 224.37498252622544\n","Validation elbo for fold 4, epoch 298: -185.9180281480842\n","Training loss for fold 4, epoch 299: 224.2410160187752\n","Validation elbo for fold 4, epoch 299: -185.2659063745761\n","Training loss for fold 4, epoch 300: 224.1044168779927\n","Validation elbo for fold 4, epoch 300: -186.4994329295085\n","Training loss for fold 4, epoch 301: 224.07415500763923\n","Validation elbo for fold 4, epoch 301: -186.86971816705898\n","Training loss for fold 4, epoch 302: 223.95311663227696\n","Validation elbo for fold 4, epoch 302: -184.4082681691726\n","Training loss for fold 4, epoch 303: 224.2641857516381\n","Validation elbo for fold 4, epoch 303: -185.9241922435396\n","Training loss for fold 4, epoch 304: 224.05946940760458\n","Validation elbo for fold 4, epoch 304: -185.4831777912712\n","Training loss for fold 4, epoch 305: 224.34548531809162\n","Validation elbo for fold 4, epoch 305: -185.8325533616393\n","Training loss for fold 4, epoch 306: 224.42903998590285\n","Validation elbo for fold 4, epoch 306: -186.2251653796779\n","Training loss for fold 4, epoch 307: 224.19911415346206\n","Validation elbo for fold 4, epoch 307: -185.14462645120773\n","Training loss for fold 4, epoch 308: 224.21699893090033\n","Validation elbo for fold 4, epoch 308: -186.55848458033543\n","Training loss for fold 4, epoch 309: 224.31104450841104\n","Validation elbo for fold 4, epoch 309: -185.69918905161086\n","Training loss for fold 4, epoch 310: 224.49204327983242\n","Validation elbo for fold 4, epoch 310: -186.31001424705852\n","Training loss for fold 4, epoch 311: 224.40123650335497\n","Validation elbo for fold 4, epoch 311: -186.65804915590036\n","Training loss for fold 4, epoch 312: 224.08387608681954\n","Validation elbo for fold 4, epoch 312: -185.60309600673554\n","Training loss for fold 4, epoch 313: 224.20348136655747\n","Validation elbo for fold 4, epoch 313: -185.0437768974026\n","Training loss for fold 4, epoch 314: 224.30788766184162\n","Validation elbo for fold 4, epoch 314: -186.78007126332142\n","Training loss for fold 4, epoch 315: 224.4216069867534\n","Validation elbo for fold 4, epoch 315: -186.40189240638185\n","Training loss for fold 4, epoch 316: 224.53501473703693\n","Validation elbo for fold 4, epoch 316: -185.69726696607518\n","Training loss for fold 4, epoch 317: 224.11207457511657\n","Validation elbo for fold 4, epoch 317: -186.28619774810437\n","Training loss for fold 4, epoch 318: 224.32171212473224\n","Validation elbo for fold 4, epoch 318: -185.29587730956058\n","Training loss for fold 4, epoch 319: 224.26041190854966\n","Validation elbo for fold 4, epoch 319: -186.65222189953423\n","Training loss for fold 4, epoch 320: 224.17705412833922\n","Validation elbo for fold 4, epoch 320: -185.9214085407254\n","Training loss for fold 4, epoch 321: 224.26548496369392\n","Validation elbo for fold 4, epoch 321: -185.951091631315\n","Training loss for fold 4, epoch 322: 224.1942416775611\n","Validation elbo for fold 4, epoch 322: -184.82443659161817\n","Training loss for fold 4, epoch 323: 224.33492303663684\n","Validation elbo for fold 4, epoch 323: -184.78227502661827\n","Training loss for fold 4, epoch 324: 224.42328643798828\n","Validation elbo for fold 4, epoch 324: -186.4695688392663\n","Training loss for fold 4, epoch 325: 224.2366704633159\n","Validation elbo for fold 4, epoch 325: -185.47374361956003\n","Training loss for fold 4, epoch 326: 224.44543703140752\n","Validation elbo for fold 4, epoch 326: -185.87839216325517\n","Training loss for fold 4, epoch 327: 224.27614839615362\n","Validation elbo for fold 4, epoch 327: -185.74458667582724\n","Training loss for fold 4, epoch 328: 224.43150354200793\n","Validation elbo for fold 4, epoch 328: -185.99592112683206\n","Training loss for fold 4, epoch 329: 224.01458174182522\n","Validation elbo for fold 4, epoch 329: -186.48949921468972\n","Training loss for fold 4, epoch 330: 224.2010985343687\n","Validation elbo for fold 4, epoch 330: -186.1561430776282\n","Training loss for fold 4, epoch 331: 224.24105933404738\n","Validation elbo for fold 4, epoch 331: -186.3136393034003\n","Training loss for fold 4, epoch 332: 223.94534178703063\n","Validation elbo for fold 4, epoch 332: -185.80349070738586\n","Training loss for fold 4, epoch 333: 224.3335443312122\n","Validation elbo for fold 4, epoch 333: -186.3560368110404\n","Training loss for fold 4, epoch 334: 224.05482975129158\n","Validation elbo for fold 4, epoch 334: -185.62861597086305\n","Training loss for fold 4, epoch 335: 224.30826224050213\n","Validation elbo for fold 4, epoch 335: -185.5728718361697\n","Training loss for fold 4, epoch 336: 224.34627163794732\n","Validation elbo for fold 4, epoch 336: -186.26976016785514\n","Training loss for fold 4, epoch 337: 224.32491179435485\n","Validation elbo for fold 4, epoch 337: -185.8102485307553\n","Training loss for fold 4, epoch 338: 224.13373073454827\n","Validation elbo for fold 4, epoch 338: -186.29558528661087\n","Training loss for fold 4, epoch 339: 224.1053986087922\n","Validation elbo for fold 4, epoch 339: -183.73948199510733\n","Training loss for fold 4, epoch 340: 224.2893750590663\n","Validation elbo for fold 4, epoch 340: -186.7054395115037\n","Training loss for fold 4, epoch 341: 224.25747926773565\n","Validation elbo for fold 4, epoch 341: -186.30437961472305\n","Training loss for fold 4, epoch 342: 224.0933353054908\n","Validation elbo for fold 4, epoch 342: -184.98719821522872\n","Training loss for fold 4, epoch 343: 224.20100919661982\n","Validation elbo for fold 4, epoch 343: -185.05948283564425\n","Training loss for fold 4, epoch 344: 224.0863553939327\n","Validation elbo for fold 4, epoch 344: -185.97020569567817\n","Training loss for fold 4, epoch 345: 224.27818716725994\n","Validation elbo for fold 4, epoch 345: -185.6528914779356\n","Training loss for fold 4, epoch 346: 224.28724768853957\n","Validation elbo for fold 4, epoch 346: -186.7316374766052\n","Training loss for fold 4, epoch 347: 224.37779974168348\n","Validation elbo for fold 4, epoch 347: -186.36369563683616\n","Training loss for fold 4, epoch 348: 224.26740092615927\n","Validation elbo for fold 4, epoch 348: -186.36042279301728\n","Training loss for fold 4, epoch 349: 224.23822415259576\n","Validation elbo for fold 4, epoch 349: -186.12141503362147\n","Training loss for fold 4, epoch 350: 224.22598192768712\n","Validation elbo for fold 4, epoch 350: -186.49124924483962\n","Training loss for fold 4, epoch 351: 224.33965941398375\n","Validation elbo for fold 4, epoch 351: -185.55968640504247\n","Training loss for fold 4, epoch 352: 224.17568945115613\n","Validation elbo for fold 4, epoch 352: -186.47396692411826\n","Training loss for fold 4, epoch 353: 224.24402766073905\n","Validation elbo for fold 4, epoch 353: -186.84641083978775\n","Training loss for fold 4, epoch 354: 224.01891819123298\n","Validation elbo for fold 4, epoch 354: -185.57078647919138\n","Training loss for fold 4, epoch 355: 224.19496425505608\n","Validation elbo for fold 4, epoch 355: -185.32150158530644\n","Training loss for fold 4, epoch 356: 224.41602300828504\n","Validation elbo for fold 4, epoch 356: -185.53953788747094\n","Training loss for fold 4, epoch 357: 224.0459520893712\n","Validation elbo for fold 4, epoch 357: -185.54938206507995\n","Training loss for fold 4, epoch 358: 224.08219023673766\n","Validation elbo for fold 4, epoch 358: -186.73255034058877\n","Training loss for fold 4, epoch 359: 224.1277123728106\n","Validation elbo for fold 4, epoch 359: -186.0103619646819\n","Training loss for fold 4, epoch 360: 224.10635179088962\n","Validation elbo for fold 4, epoch 360: -184.57639290910586\n","Training loss for fold 4, epoch 361: 224.2887711063508\n","Validation elbo for fold 4, epoch 361: -185.9190001044163\n","Training loss for fold 4, epoch 362: 224.11722540086316\n","Validation elbo for fold 4, epoch 362: -185.04579319436718\n","Training loss for fold 4, epoch 363: 224.37802419354838\n","Validation elbo for fold 4, epoch 363: -185.49343769198916\n","Training loss for fold 4, epoch 364: 224.11929321289062\n","Validation elbo for fold 4, epoch 364: -184.8506046451264\n","Training loss for fold 4, epoch 365: 224.07390496038622\n","Validation elbo for fold 4, epoch 365: -184.42756810438965\n","Training loss for fold 4, epoch 366: 224.71964706913118\n","Validation elbo for fold 4, epoch 366: -186.665189487387\n","Training loss for fold 4, epoch 367: 224.5385929230721\n","Validation elbo for fold 4, epoch 367: -185.1646471938148\n","Training loss for fold 4, epoch 368: 224.40723443800402\n","Validation elbo for fold 4, epoch 368: -186.2283777797022\n","Training loss for fold 4, epoch 369: 224.88006050355972\n","Validation elbo for fold 4, epoch 369: -185.6102980833535\n","Training loss for fold 4, epoch 370: 224.6859645228232\n","Validation elbo for fold 4, epoch 370: -186.36917882557123\n","Training loss for fold 4, epoch 371: 224.24416277485508\n","Validation elbo for fold 4, epoch 371: -186.22400650341643\n","Training loss for fold 4, epoch 372: 224.2263678273847\n","Validation elbo for fold 4, epoch 372: -186.55726998071495\n","Training loss for fold 4, epoch 373: 224.45905328566027\n","Validation elbo for fold 4, epoch 373: -186.32703452068966\n","Training loss for fold 4, epoch 374: 224.22711353917276\n","Validation elbo for fold 4, epoch 374: -186.27519136714824\n","Training loss for fold 4, epoch 375: 224.02408575242566\n","Validation elbo for fold 4, epoch 375: -186.06432886628042\n","Training loss for fold 4, epoch 376: 224.68406898744644\n","Validation elbo for fold 4, epoch 376: -186.55654353738555\n","Training loss for fold 4, epoch 377: 224.29267021917528\n","Validation elbo for fold 4, epoch 377: -186.7744066523583\n","Training loss for fold 4, epoch 378: 223.91595877370526\n","Validation elbo for fold 4, epoch 378: -185.29768294141218\n","Training loss for fold 4, epoch 379: 223.97521406604397\n","Validation elbo for fold 4, epoch 379: -186.66471412499902\n","Training loss for fold 4, epoch 380: 224.33886177309097\n","Validation elbo for fold 4, epoch 380: -185.45840040439634\n","Training loss for fold 4, epoch 381: 224.46415439728767\n","Validation elbo for fold 4, epoch 381: -186.93197484722293\n","Training loss for fold 4, epoch 382: 224.3997586158014\n","Validation elbo for fold 4, epoch 382: -186.08597931368513\n","Training loss for fold 4, epoch 383: 224.03592583440965\n","Validation elbo for fold 4, epoch 383: -185.4343211378272\n","Training loss for fold 4, epoch 384: 224.20920611966042\n","Validation elbo for fold 4, epoch 384: -185.74301442184654\n","Training loss for fold 4, epoch 385: 224.34482919016193\n","Validation elbo for fold 4, epoch 385: -185.91847954993685\n","Training loss for fold 4, epoch 386: 224.2595424036826\n","Validation elbo for fold 4, epoch 386: -185.31443956580762\n","Training loss for fold 4, epoch 387: 224.52744686988092\n","Validation elbo for fold 4, epoch 387: -186.55461942027898\n","Training loss for fold 4, epoch 388: 224.28207077518587\n","Validation elbo for fold 4, epoch 388: -185.40416035968292\n","Training loss for fold 4, epoch 389: 224.459227531187\n","Validation elbo for fold 4, epoch 389: -186.76807299659393\n","Training loss for fold 4, epoch 390: 224.31314726798766\n","Validation elbo for fold 4, epoch 390: -185.18848528459154\n","Training loss for fold 4, epoch 391: 224.16884170040007\n","Validation elbo for fold 4, epoch 391: -185.35303947962234\n","Training loss for fold 4, epoch 392: 224.20013796898627\n","Validation elbo for fold 4, epoch 392: -185.9346020546804\n","Training loss for fold 4, epoch 393: 224.07185905210434\n","Validation elbo for fold 4, epoch 393: -184.65712051564148\n","Training loss for fold 4, epoch 394: 224.22460396059097\n","Validation elbo for fold 4, epoch 394: -184.8290906465191\n","Training loss for fold 4, epoch 395: 224.3215095766129\n","Validation elbo for fold 4, epoch 395: -185.58764822046393\n","Training loss for fold 4, epoch 396: 224.0794411936114\n","Validation elbo for fold 4, epoch 396: -185.26633424363143\n","Training loss for fold 4, epoch 397: 224.25848954723728\n","Validation elbo for fold 4, epoch 397: -186.39993466987082\n","Training loss for fold 4, epoch 398: 224.34747117565524\n","Validation elbo for fold 4, epoch 398: -185.93280305524036\n","Training loss for fold 4, epoch 399: 224.3086892404864\n","Validation elbo for fold 4, epoch 399: -185.5863871056314\n","Training loss for fold 4, epoch 400: 224.2590807022587\n","Validation elbo for fold 4, epoch 400: -184.97025806368458\n","Training loss for fold 4, epoch 401: 224.25798157722718\n","Validation elbo for fold 4, epoch 401: -185.51784663504918\n","Training loss for fold 4, epoch 402: 224.24790437759893\n","Validation elbo for fold 4, epoch 402: -185.83905954759248\n","Training loss for fold 4, epoch 403: 224.08443278651083\n","Validation elbo for fold 4, epoch 403: -185.5028136764118\n","Training loss for fold 4, epoch 404: 224.2975333429152\n","Validation elbo for fold 4, epoch 404: -186.26330466940914\n","Training loss for fold 4, epoch 405: 224.51587923111455\n","Validation elbo for fold 4, epoch 405: -186.03226212702333\n","Training loss for fold 4, epoch 406: 224.18941350137032\n","Validation elbo for fold 4, epoch 406: -184.91576201251124\n","Training loss for fold 4, epoch 407: 224.12804215954196\n","Validation elbo for fold 4, epoch 407: -185.78114666188193\n","Training loss for fold 4, epoch 408: 224.87621307373047\n","Validation elbo for fold 4, epoch 408: -187.20279324014177\n","Training loss for fold 4, epoch 409: 224.32057091497606\n","Validation elbo for fold 4, epoch 409: -186.64056684144413\n","Training loss for fold 4, epoch 410: 224.2125724054152\n","Validation elbo for fold 4, epoch 410: -186.10381602505788\n","Training loss for fold 4, epoch 411: 224.21090304466986\n","Validation elbo for fold 4, epoch 411: -186.68544058835332\n","Training loss for fold 4, epoch 412: 224.3711392802577\n","Validation elbo for fold 4, epoch 412: -187.3413741368804\n","Training loss for fold 4, epoch 413: 224.2866676084457\n","Validation elbo for fold 4, epoch 413: -185.48284390369824\n","Training loss for fold 4, epoch 414: 224.1696518928774\n","Validation elbo for fold 4, epoch 414: -186.19431405957684\n","Training loss for fold 4, epoch 415: 224.14293867541897\n","Validation elbo for fold 4, epoch 415: -185.53660173011934\n","Training loss for fold 4, epoch 416: 224.14105642995526\n","Validation elbo for fold 4, epoch 416: -185.56824130933018\n","Training loss for fold 4, epoch 417: 224.14791722451486\n","Validation elbo for fold 4, epoch 417: -185.86022477248952\n","Training loss for fold 4, epoch 418: 224.21639005599482\n","Validation elbo for fold 4, epoch 418: -185.90740173937064\n","Training loss for fold 4, epoch 419: 224.44540725215788\n","Validation elbo for fold 4, epoch 419: -185.57777828307448\n","Training loss for fold 4, epoch 420: 224.0427194410755\n","Validation elbo for fold 4, epoch 420: -186.64720337789265\n","Training loss for fold 4, epoch 421: 224.194948011829\n","Validation elbo for fold 4, epoch 421: -186.8427432580852\n","Training loss for fold 4, epoch 422: 224.03400248865927\n","Validation elbo for fold 4, epoch 422: -186.91789428540204\n","Training loss for fold 4, epoch 423: 224.2197282852665\n","Validation elbo for fold 4, epoch 423: -187.38133015685037\n","Training loss for fold 4, epoch 424: 224.3575190882529\n","Validation elbo for fold 4, epoch 424: -186.47894031896737\n","Training loss for fold 4, epoch 425: 224.17930110808342\n","Validation elbo for fold 4, epoch 425: -186.77907293939342\n","Training loss for fold 4, epoch 426: 224.0172820552703\n","Validation elbo for fold 4, epoch 426: -185.84763387256822\n","Training loss for fold 4, epoch 427: 224.47903147051412\n","Validation elbo for fold 4, epoch 427: -185.6678820601186\n","Training loss for fold 4, epoch 428: 224.04849883048766\n","Validation elbo for fold 4, epoch 428: -185.23273248531353\n","Training loss for fold 4, epoch 429: 224.2959685787078\n","Validation elbo for fold 4, epoch 429: -186.55272176234672\n","Training loss for fold 4, epoch 430: 224.35201214205833\n","Validation elbo for fold 4, epoch 430: -185.42004404447425\n","Training loss for fold 4, epoch 431: 224.24463702786355\n","Validation elbo for fold 4, epoch 431: -186.04315089928627\n","Training loss for fold 4, epoch 432: 224.12686206448464\n","Validation elbo for fold 4, epoch 432: -186.2328781705773\n","Training loss for fold 4, epoch 433: 224.72578823950982\n","Validation elbo for fold 4, epoch 433: -185.52206326207852\n","Training loss for fold 4, epoch 434: 224.29914954400832\n","Validation elbo for fold 4, epoch 434: -186.11015794712833\n","Training loss for fold 4, epoch 435: 224.36222790133567\n","Validation elbo for fold 4, epoch 435: -186.08951511792833\n","Training loss for fold 4, epoch 436: 224.2328914519279\n","Validation elbo for fold 4, epoch 436: -186.55526572598907\n","Training loss for fold 4, epoch 437: 223.9309835126323\n","Validation elbo for fold 4, epoch 437: -186.4038357462054\n","Training loss for fold 4, epoch 438: 224.23038630331718\n","Validation elbo for fold 4, epoch 438: -185.16386186662785\n","Training loss for fold 4, epoch 439: 224.0389116348759\n","Validation elbo for fold 4, epoch 439: -186.0408968315411\n","Training loss for fold 4, epoch 440: 224.27096040787237\n","Validation elbo for fold 4, epoch 440: -184.99382647968508\n","Training loss for fold 4, epoch 441: 224.39591536983366\n","Validation elbo for fold 4, epoch 441: -186.28674985511964\n","Training loss for fold 4, epoch 442: 224.07420546008694\n","Validation elbo for fold 4, epoch 442: -185.01335275302011\n","Training loss for fold 4, epoch 443: 224.25994700770224\n","Validation elbo for fold 4, epoch 443: -185.42602439739466\n","Training loss for fold 4, epoch 444: 224.36876899965347\n","Validation elbo for fold 4, epoch 444: -185.7371149938614\n","Training loss for fold 4, epoch 445: 224.2927027056294\n","Validation elbo for fold 4, epoch 445: -186.96156715556776\n","Training loss for fold 4, epoch 446: 224.10218171150453\n","Validation elbo for fold 4, epoch 446: -186.1113394642723\n","Training loss for fold 4, epoch 447: 224.18732280115927\n","Validation elbo for fold 4, epoch 447: -185.47676962935213\n","Training loss for fold 4, epoch 448: 224.1601624027375\n","Validation elbo for fold 4, epoch 448: -186.87937103978877\n","Training loss for fold 4, epoch 449: 224.25702248850178\n","Validation elbo for fold 4, epoch 449: -186.5431064055019\n","Training loss for fold 4, epoch 450: 224.05722341229838\n","Validation elbo for fold 4, epoch 450: -185.6430603138591\n","Training loss for fold 4, epoch 451: 224.36218803159653\n","Validation elbo for fold 4, epoch 451: -185.9259454060408\n","Training loss for fold 4, epoch 452: 224.42059203117125\n","Validation elbo for fold 4, epoch 452: -185.5647205490135\n","Training loss for fold 4, epoch 453: 224.22648054553616\n","Validation elbo for fold 4, epoch 453: -185.67634121263237\n","Training loss for fold 4, epoch 454: 224.42273859823905\n","Validation elbo for fold 4, epoch 454: -186.36190194928918\n","Training loss for fold 4, epoch 455: 224.33474509946763\n","Validation elbo for fold 4, epoch 455: -187.26536530402666\n","Training loss for fold 4, epoch 456: 224.3545172906691\n","Validation elbo for fold 4, epoch 456: -185.86448670274407\n","Training loss for fold 4, epoch 457: 224.2170200963174\n","Validation elbo for fold 4, epoch 457: -186.43154184475878\n","Training loss for fold 4, epoch 458: 223.9479982929845\n","Validation elbo for fold 4, epoch 458: -185.61448350078018\n","Training loss for fold 4, epoch 459: 224.30591361753403\n","Validation elbo for fold 4, epoch 459: -186.44373191162765\n","Training loss for fold 4, epoch 460: 224.0603770594443\n","Validation elbo for fold 4, epoch 460: -185.19114141766698\n","Training loss for fold 4, epoch 461: 224.06986359627015\n","Validation elbo for fold 4, epoch 461: -185.75454308860716\n","Training loss for fold 4, epoch 462: 223.9811059274981\n","Validation elbo for fold 4, epoch 462: -185.87597460710458\n","Training loss for fold 4, epoch 463: 224.42700761364353\n","Validation elbo for fold 4, epoch 463: -186.73198460012978\n","Training loss for fold 4, epoch 464: 224.27870768885458\n","Validation elbo for fold 4, epoch 464: -186.23766601535712\n","Training loss for fold 4, epoch 465: 224.23342895507812\n","Validation elbo for fold 4, epoch 465: -184.76028213855926\n","Training loss for fold 4, epoch 466: 224.36888836276145\n","Validation elbo for fold 4, epoch 466: -186.35705705285417\n","Training loss for fold 4, epoch 467: 224.2095718383789\n","Validation elbo for fold 4, epoch 467: -185.80752903524615\n","Training loss for fold 4, epoch 468: 224.46695561562814\n","Validation elbo for fold 4, epoch 468: -184.98410501624792\n","Training loss for fold 4, epoch 469: 224.3386456889491\n","Validation elbo for fold 4, epoch 469: -186.76127980339646\n","Training loss for fold 4, epoch 470: 224.23060903241557\n","Validation elbo for fold 4, epoch 470: -185.78413744856073\n","Training loss for fold 4, epoch 471: 224.20058145830708\n","Validation elbo for fold 4, epoch 471: -185.5293893398857\n","Training loss for fold 4, epoch 472: 224.12371998448526\n","Validation elbo for fold 4, epoch 472: -185.63954045883855\n","Training loss for fold 4, epoch 473: 224.05094343616116\n","Validation elbo for fold 4, epoch 473: -184.9912119113418\n","Training loss for fold 4, epoch 474: 224.26551646571005\n","Validation elbo for fold 4, epoch 474: -186.37598757874466\n","Training loss for fold 4, epoch 475: 224.18746234524636\n","Validation elbo for fold 4, epoch 475: -186.26116561154\n","Training loss for fold 4, epoch 476: 224.2809076616841\n","Validation elbo for fold 4, epoch 476: -185.4805616609234\n","Training loss for fold 4, epoch 477: 224.36119916362148\n","Validation elbo for fold 4, epoch 477: -186.2937004406311\n","Training loss for fold 4, epoch 478: 224.14697019515498\n","Validation elbo for fold 4, epoch 478: -186.44952502045382\n","Training loss for fold 4, epoch 479: 224.23690402123236\n","Validation elbo for fold 4, epoch 479: -185.76014702752093\n","Training loss for fold 4, epoch 480: 224.2438492313508\n","Validation elbo for fold 4, epoch 480: -187.0266104612241\n","Training loss for fold 4, epoch 481: 224.08490285565776\n","Validation elbo for fold 4, epoch 481: -186.17870569290614\n","Training loss for fold 4, epoch 482: 224.19686323596585\n","Validation elbo for fold 4, epoch 482: -185.75661861153512\n","Training loss for fold 4, epoch 483: 224.08674990746283\n","Validation elbo for fold 4, epoch 483: -185.8470707808858\n","Training loss for fold 4, epoch 484: 224.26224517822266\n","Validation elbo for fold 4, epoch 484: -186.530749270458\n","Training loss for fold 4, epoch 485: 224.10425936791205\n","Validation elbo for fold 4, epoch 485: -185.9022469098163\n","Training loss for fold 4, epoch 486: 224.11025927143712\n","Validation elbo for fold 4, epoch 486: -185.85514350417705\n","Training loss for fold 4, epoch 487: 224.25020943918537\n","Validation elbo for fold 4, epoch 487: -185.66048549569854\n","Training loss for fold 4, epoch 488: 224.432496839954\n","Validation elbo for fold 4, epoch 488: -186.1588778384331\n","Training loss for fold 4, epoch 489: 224.14669184530936\n","Validation elbo for fold 4, epoch 489: -186.21519345833792\n","Training loss for fold 4, epoch 490: 223.99312788440335\n","Validation elbo for fold 4, epoch 490: -185.7457546903509\n","Training loss for fold 4, epoch 491: 224.2850592828566\n","Validation elbo for fold 4, epoch 491: -185.94623413100055\n","Training loss for fold 4, epoch 492: 224.3825179069273\n","Validation elbo for fold 4, epoch 492: -186.5868822422677\n","Training loss for fold 4, epoch 493: 224.28111119424142\n","Validation elbo for fold 4, epoch 493: -186.4604571477999\n","Training loss for fold 4, epoch 494: 224.7102092619865\n","Validation elbo for fold 4, epoch 494: -186.01502813399674\n","Training loss for fold 4, epoch 495: 224.67330046622985\n","Validation elbo for fold 4, epoch 495: -185.3507261485865\n","Training loss for fold 4, epoch 496: 224.2310067453692\n","Validation elbo for fold 4, epoch 496: -185.48747372259243\n","Training loss for fold 4, epoch 497: 224.14971579274822\n","Validation elbo for fold 4, epoch 497: -185.8097863671742\n","Training loss for fold 4, epoch 498: 224.07942544260334\n","Validation elbo for fold 4, epoch 498: -186.3888836569095\n","Training loss for fold 4, epoch 499: 224.05980288597846\n","Validation elbo for fold 4, epoch 499: -186.93923036402902\n","Fold 5\n","-------\n","Training loss for fold 5, epoch 0: 242.60192748039\n","Validation elbo for fold 5, epoch 0: -188.4406814010195\n","Training loss for fold 5, epoch 1: 227.1045670047883\n","Validation elbo for fold 5, epoch 1: -184.67457581522999\n","Training loss for fold 5, epoch 2: 226.65584293488533\n","Validation elbo for fold 5, epoch 2: -185.25420640967968\n","Training loss for fold 5, epoch 3: 226.52932911534464\n","Validation elbo for fold 5, epoch 3: -185.84871741722168\n","Training loss for fold 5, epoch 4: 226.46724331763482\n","Validation elbo for fold 5, epoch 4: -187.39497684305923\n","Training loss for fold 5, epoch 5: 226.25073586740803\n","Validation elbo for fold 5, epoch 5: -186.00705480958703\n","Training loss for fold 5, epoch 6: 226.20169707267516\n","Validation elbo for fold 5, epoch 6: -184.78474324413406\n","Training loss for fold 5, epoch 7: 226.3638664983934\n","Validation elbo for fold 5, epoch 7: -184.72533870821786\n","Training loss for fold 5, epoch 8: 225.94256616407824\n","Validation elbo for fold 5, epoch 8: -182.52897530106708\n","Training loss for fold 5, epoch 9: 226.06985842797064\n","Validation elbo for fold 5, epoch 9: -184.58931733464703\n","Training loss for fold 5, epoch 10: 226.10250485327936\n","Validation elbo for fold 5, epoch 10: -185.95787679658505\n","Training loss for fold 5, epoch 11: 226.01954823155558\n","Validation elbo for fold 5, epoch 11: -185.74747772766466\n","Training loss for fold 5, epoch 12: 225.80213485225553\n","Validation elbo for fold 5, epoch 12: -186.05818987186305\n","Training loss for fold 5, epoch 13: 225.60046337496848\n","Validation elbo for fold 5, epoch 13: -183.3962610694417\n","Training loss for fold 5, epoch 14: 226.04861671693862\n","Validation elbo for fold 5, epoch 14: -184.75892348181367\n","Training loss for fold 5, epoch 15: 225.78599597561745\n","Validation elbo for fold 5, epoch 15: -186.51885575114187\n","Training loss for fold 5, epoch 16: 225.7108900008663\n","Validation elbo for fold 5, epoch 16: -186.01589117585334\n","Training loss for fold 5, epoch 17: 225.72133981027912\n","Validation elbo for fold 5, epoch 17: -185.20408635154928\n","Training loss for fold 5, epoch 18: 225.7641094576928\n","Validation elbo for fold 5, epoch 18: -186.74456258534147\n","Training loss for fold 5, epoch 19: 225.65582004670173\n","Validation elbo for fold 5, epoch 19: -184.25363303089364\n","Training loss for fold 5, epoch 20: 225.7407477594191\n","Validation elbo for fold 5, epoch 20: -186.94095629607315\n","Training loss for fold 5, epoch 21: 225.76673446163053\n","Validation elbo for fold 5, epoch 21: -186.07790318352733\n","Training loss for fold 5, epoch 22: 225.85432803246283\n","Validation elbo for fold 5, epoch 22: -186.63930947828413\n","Training loss for fold 5, epoch 23: 225.46659580353767\n","Validation elbo for fold 5, epoch 23: -186.01042022966897\n","Training loss for fold 5, epoch 24: 225.60987214119203\n","Validation elbo for fold 5, epoch 24: -185.50333558017533\n","Training loss for fold 5, epoch 25: 225.3859602405179\n","Validation elbo for fold 5, epoch 25: -188.4442632702187\n","Training loss for fold 5, epoch 26: 225.44034822525518\n","Validation elbo for fold 5, epoch 26: -185.97134116826572\n","Training loss for fold 5, epoch 27: 225.5947794760427\n","Validation elbo for fold 5, epoch 27: -185.00425911074768\n","Training loss for fold 5, epoch 28: 225.58251411684097\n","Validation elbo for fold 5, epoch 28: -185.21602539026242\n","Training loss for fold 5, epoch 29: 225.3742156490203\n","Validation elbo for fold 5, epoch 29: -186.98918851757992\n","Training loss for fold 5, epoch 30: 225.47085497456212\n","Validation elbo for fold 5, epoch 30: -185.750624408993\n","Training loss for fold 5, epoch 31: 225.61116643105782\n","Validation elbo for fold 5, epoch 31: -185.32053108607334\n","Training loss for fold 5, epoch 32: 225.6307638845136\n","Validation elbo for fold 5, epoch 32: -184.88543879768844\n","Training loss for fold 5, epoch 33: 225.4394051336473\n","Validation elbo for fold 5, epoch 33: -184.40270645396782\n","Training loss for fold 5, epoch 34: 225.41619897657824\n","Validation elbo for fold 5, epoch 34: -185.04959057506164\n","Training loss for fold 5, epoch 35: 225.33960428545552\n","Validation elbo for fold 5, epoch 35: -186.87544903982342\n","Training loss for fold 5, epoch 36: 225.53682684129285\n","Validation elbo for fold 5, epoch 36: -187.06908838829546\n","Training loss for fold 5, epoch 37: 225.35006812310988\n","Validation elbo for fold 5, epoch 37: -186.94228603827025\n","Training loss for fold 5, epoch 38: 225.26138600995463\n","Validation elbo for fold 5, epoch 38: -184.6732112028676\n","Training loss for fold 5, epoch 39: 225.46044208157448\n","Validation elbo for fold 5, epoch 39: -186.3730800790684\n","Training loss for fold 5, epoch 40: 225.19082789267264\n","Validation elbo for fold 5, epoch 40: -185.6566725903806\n","Training loss for fold 5, epoch 41: 225.15719186106037\n","Validation elbo for fold 5, epoch 41: -185.81585141071014\n","Training loss for fold 5, epoch 42: 225.24166771673387\n","Validation elbo for fold 5, epoch 42: -184.50133928335114\n","Training loss for fold 5, epoch 43: 225.2028308991463\n","Validation elbo for fold 5, epoch 43: -184.12447147406172\n","Training loss for fold 5, epoch 44: 225.8634559877457\n","Validation elbo for fold 5, epoch 44: -187.36717908845435\n","Training loss for fold 5, epoch 45: 226.0146255493164\n","Validation elbo for fold 5, epoch 45: -187.99306638274783\n","Training loss for fold 5, epoch 46: 225.49498601113595\n","Validation elbo for fold 5, epoch 46: -186.87563338608786\n","Training loss for fold 5, epoch 47: 225.09121679490613\n","Validation elbo for fold 5, epoch 47: -187.35934616432377\n","Training loss for fold 5, epoch 48: 225.13453477428806\n","Validation elbo for fold 5, epoch 48: -184.61886691387906\n","Training loss for fold 5, epoch 49: 225.42891865391886\n","Validation elbo for fold 5, epoch 49: -186.50998917657995\n","Training loss for fold 5, epoch 50: 225.30064662810295\n","Validation elbo for fold 5, epoch 50: -186.3381298752089\n","Training loss for fold 5, epoch 51: 225.49572507796748\n","Validation elbo for fold 5, epoch 51: -185.51981117692603\n","Training loss for fold 5, epoch 52: 225.2379883796938\n","Validation elbo for fold 5, epoch 52: -185.96981330883358\n","Training loss for fold 5, epoch 53: 225.3653842556861\n","Validation elbo for fold 5, epoch 53: -185.62205828723208\n","Training loss for fold 5, epoch 54: 225.1498765022524\n","Validation elbo for fold 5, epoch 54: -184.804212630876\n","Training loss for fold 5, epoch 55: 225.3323502079133\n","Validation elbo for fold 5, epoch 55: -185.19529605576238\n","Training loss for fold 5, epoch 56: 224.9958784041866\n","Validation elbo for fold 5, epoch 56: -183.42673404499732\n","Training loss for fold 5, epoch 57: 225.13741228657383\n","Validation elbo for fold 5, epoch 57: -186.9691786264131\n","Training loss for fold 5, epoch 58: 225.41034157045425\n","Validation elbo for fold 5, epoch 58: -185.245819791735\n","Training loss for fold 5, epoch 59: 225.26021231374432\n","Validation elbo for fold 5, epoch 59: -187.32618174959808\n","Training loss for fold 5, epoch 60: 225.35133977090157\n","Validation elbo for fold 5, epoch 60: -187.43361028053084\n","Training loss for fold 5, epoch 61: 225.09770965576172\n","Validation elbo for fold 5, epoch 61: -185.57736615042245\n","Training loss for fold 5, epoch 62: 225.11389996928554\n","Validation elbo for fold 5, epoch 62: -186.96555179158065\n","Training loss for fold 5, epoch 63: 225.39353721372544\n","Validation elbo for fold 5, epoch 63: -185.02335800142015\n","Training loss for fold 5, epoch 64: 225.27679172638923\n","Validation elbo for fold 5, epoch 64: -185.83791787375696\n","Training loss for fold 5, epoch 65: 225.45762240502143\n","Validation elbo for fold 5, epoch 65: -186.28535570353722\n","Training loss for fold 5, epoch 66: 225.18575163810485\n","Validation elbo for fold 5, epoch 66: -187.21619348156463\n","Training loss for fold 5, epoch 67: 225.33008870770854\n","Validation elbo for fold 5, epoch 67: -186.83679214308765\n","Training loss for fold 5, epoch 68: 225.1132049560547\n","Validation elbo for fold 5, epoch 68: -186.38396238943034\n","Training loss for fold 5, epoch 69: 225.2272009080456\n","Validation elbo for fold 5, epoch 69: -186.63416786300186\n","Training loss for fold 5, epoch 70: 224.93390015632875\n","Validation elbo for fold 5, epoch 70: -185.5662818528754\n","Training loss for fold 5, epoch 71: 225.27662141861455\n","Validation elbo for fold 5, epoch 71: -187.00212544182526\n","Training loss for fold 5, epoch 72: 225.1934341922883\n","Validation elbo for fold 5, epoch 72: -186.4370533705299\n","Training loss for fold 5, epoch 73: 225.13394066595262\n","Validation elbo for fold 5, epoch 73: -185.12284563274187\n","Training loss for fold 5, epoch 74: 224.75651082684917\n","Validation elbo for fold 5, epoch 74: -185.83945139572455\n","Training loss for fold 5, epoch 75: 225.57189498409147\n","Validation elbo for fold 5, epoch 75: -187.52322032120065\n","Training loss for fold 5, epoch 76: 225.35608402375252\n","Validation elbo for fold 5, epoch 76: -187.8938858579962\n","Training loss for fold 5, epoch 77: 225.10708618164062\n","Validation elbo for fold 5, epoch 77: -188.06555649519458\n","Training loss for fold 5, epoch 78: 225.0115750220514\n","Validation elbo for fold 5, epoch 78: -187.61292828550978\n","Training loss for fold 5, epoch 79: 225.13020915369833\n","Validation elbo for fold 5, epoch 79: -185.6396180265774\n","Training loss for fold 5, epoch 80: 224.83814214891004\n","Validation elbo for fold 5, epoch 80: -186.24892163490523\n","Training loss for fold 5, epoch 81: 224.99671123873802\n","Validation elbo for fold 5, epoch 81: -185.3803429442685\n","Training loss for fold 5, epoch 82: 224.88136045394404\n","Validation elbo for fold 5, epoch 82: -185.08021104496453\n","Training loss for fold 5, epoch 83: 224.99857133434665\n","Validation elbo for fold 5, epoch 83: -185.79882099956365\n","Training loss for fold 5, epoch 84: 224.981811277328\n","Validation elbo for fold 5, epoch 84: -185.47625413528974\n","Training loss for fold 5, epoch 85: 224.89454601657005\n","Validation elbo for fold 5, epoch 85: -186.53491306999166\n","Training loss for fold 5, epoch 86: 225.1570599463678\n","Validation elbo for fold 5, epoch 86: -185.12775095683662\n","Training loss for fold 5, epoch 87: 224.97865270799207\n","Validation elbo for fold 5, epoch 87: -184.6869308519754\n","Training loss for fold 5, epoch 88: 225.0294947470388\n","Validation elbo for fold 5, epoch 88: -187.52559499205265\n","Training loss for fold 5, epoch 89: 225.0454854657573\n","Validation elbo for fold 5, epoch 89: -185.4362719686764\n","Training loss for fold 5, epoch 90: 224.9591541905557\n","Validation elbo for fold 5, epoch 90: -185.27668139737068\n","Training loss for fold 5, epoch 91: 224.8844501126197\n","Validation elbo for fold 5, epoch 91: -186.79074278448246\n","Training loss for fold 5, epoch 92: 224.95023493612968\n","Validation elbo for fold 5, epoch 92: -186.15401465741175\n","Training loss for fold 5, epoch 93: 225.1447522563319\n","Validation elbo for fold 5, epoch 93: -185.60690403647422\n","Training loss for fold 5, epoch 94: 224.95532571115803\n","Validation elbo for fold 5, epoch 94: -185.1636348852092\n","Training loss for fold 5, epoch 95: 224.9020272531817\n","Validation elbo for fold 5, epoch 95: -187.18388262063078\n","Training loss for fold 5, epoch 96: 224.85781638853013\n","Validation elbo for fold 5, epoch 96: -186.76082888139763\n","Training loss for fold 5, epoch 97: 224.97694495416457\n","Validation elbo for fold 5, epoch 97: -185.83646414959773\n","Training loss for fold 5, epoch 98: 225.04378509521484\n","Validation elbo for fold 5, epoch 98: -185.02581228204232\n","Training loss for fold 5, epoch 99: 224.8561524421938\n","Validation elbo for fold 5, epoch 99: -186.42522295000404\n","Training loss for fold 5, epoch 100: 225.0148694438319\n","Validation elbo for fold 5, epoch 100: -185.92314303736723\n","Training loss for fold 5, epoch 101: 225.08054647138042\n","Validation elbo for fold 5, epoch 101: -187.76447160089825\n","Training loss for fold 5, epoch 102: 225.44903441398375\n","Validation elbo for fold 5, epoch 102: -186.00682648683392\n","Training loss for fold 5, epoch 103: 225.09281183058215\n","Validation elbo for fold 5, epoch 103: -185.5419861404752\n","Training loss for fold 5, epoch 104: 225.03435713245022\n","Validation elbo for fold 5, epoch 104: -185.95068964213067\n","Training loss for fold 5, epoch 105: 224.98616421607232\n","Validation elbo for fold 5, epoch 105: -185.87684921286407\n","Training loss for fold 5, epoch 106: 224.94937601397115\n","Validation elbo for fold 5, epoch 106: -184.86841404098794\n","Training loss for fold 5, epoch 107: 224.72664371613533\n","Validation elbo for fold 5, epoch 107: -185.4181747426208\n","Training loss for fold 5, epoch 108: 225.00421364076675\n","Validation elbo for fold 5, epoch 108: -186.55415054538844\n","Training loss for fold 5, epoch 109: 224.94935287967806\n","Validation elbo for fold 5, epoch 109: -185.60899699109734\n","Training loss for fold 5, epoch 110: 224.7585636261971\n","Validation elbo for fold 5, epoch 110: -187.73407202200343\n","Training loss for fold 5, epoch 111: 224.95237411991243\n","Validation elbo for fold 5, epoch 111: -186.61396140631678\n","Training loss for fold 5, epoch 112: 225.00215936476184\n","Validation elbo for fold 5, epoch 112: -187.04888206416436\n","Training loss for fold 5, epoch 113: 224.91265401532573\n","Validation elbo for fold 5, epoch 113: -186.7883284951285\n","Training loss for fold 5, epoch 114: 225.09237670898438\n","Validation elbo for fold 5, epoch 114: -186.00302774156796\n","Training loss for fold 5, epoch 115: 224.8201685259419\n","Validation elbo for fold 5, epoch 115: -185.929914394948\n","Training loss for fold 5, epoch 116: 225.26971681656377\n","Validation elbo for fold 5, epoch 116: -187.32535951277254\n","Training loss for fold 5, epoch 117: 224.9906520228232\n","Validation elbo for fold 5, epoch 117: -185.6774317799959\n","Training loss for fold 5, epoch 118: 224.9203136813256\n","Validation elbo for fold 5, epoch 118: -186.15921002759177\n","Training loss for fold 5, epoch 119: 225.12895645633822\n","Validation elbo for fold 5, epoch 119: -185.84348892350144\n","Training loss for fold 5, epoch 120: 225.06267892160724\n","Validation elbo for fold 5, epoch 120: -188.28665969423034\n","Training loss for fold 5, epoch 121: 224.83389577557963\n","Validation elbo for fold 5, epoch 121: -186.4011972683047\n","Training loss for fold 5, epoch 122: 224.95170297930318\n","Validation elbo for fold 5, epoch 122: -184.74002232488908\n","Training loss for fold 5, epoch 123: 224.97022148870653\n","Validation elbo for fold 5, epoch 123: -185.26939736011707\n","Training loss for fold 5, epoch 124: 225.05318204818232\n","Validation elbo for fold 5, epoch 124: -186.66068707129\n","Training loss for fold 5, epoch 125: 224.83375450872606\n","Validation elbo for fold 5, epoch 125: -184.83761901011366\n","Training loss for fold 5, epoch 126: 225.14854308097594\n","Validation elbo for fold 5, epoch 126: -186.79430162610254\n","Training loss for fold 5, epoch 127: 224.7184303037582\n","Validation elbo for fold 5, epoch 127: -186.91192496020665\n","Training loss for fold 5, epoch 128: 225.529543968939\n","Validation elbo for fold 5, epoch 128: -186.43034139941057\n","Training loss for fold 5, epoch 129: 225.44182832779424\n","Validation elbo for fold 5, epoch 129: -186.2969095679063\n","Training loss for fold 5, epoch 130: 225.06813049316406\n","Validation elbo for fold 5, epoch 130: -185.63941741686733\n","Training loss for fold 5, epoch 131: 225.48855689264113\n","Validation elbo for fold 5, epoch 131: -187.057975435792\n","Training loss for fold 5, epoch 132: 225.0313991423576\n","Validation elbo for fold 5, epoch 132: -187.3228024903999\n","Training loss for fold 5, epoch 133: 225.0699202014554\n","Validation elbo for fold 5, epoch 133: -184.79663648354847\n","Training loss for fold 5, epoch 134: 225.0850330475838\n","Validation elbo for fold 5, epoch 134: -186.70670996816102\n","Training loss for fold 5, epoch 135: 224.8878710346837\n","Validation elbo for fold 5, epoch 135: -185.64779817715697\n","Training loss for fold 5, epoch 136: 224.829097624748\n","Validation elbo for fold 5, epoch 136: -186.54641084771993\n","Training loss for fold 5, epoch 137: 224.8996623869865\n","Validation elbo for fold 5, epoch 137: -186.31058845908228\n","Training loss for fold 5, epoch 138: 224.89854874149447\n","Validation elbo for fold 5, epoch 138: -185.81747853221896\n","Training loss for fold 5, epoch 139: 224.73961664015246\n","Validation elbo for fold 5, epoch 139: -186.93476076708868\n","Training loss for fold 5, epoch 140: 225.08643070344002\n","Validation elbo for fold 5, epoch 140: -186.20097290270476\n","Training loss for fold 5, epoch 141: 224.95971827353202\n","Validation elbo for fold 5, epoch 141: -185.38307718838388\n","Training loss for fold 5, epoch 142: 224.8844055668\n","Validation elbo for fold 5, epoch 142: -186.54282120491644\n","Training loss for fold 5, epoch 143: 224.866580347861\n","Validation elbo for fold 5, epoch 143: -186.60746514232721\n","Training loss for fold 5, epoch 144: 224.882202394547\n","Validation elbo for fold 5, epoch 144: -187.2464229019305\n","Training loss for fold 5, epoch 145: 224.73988514561808\n","Validation elbo for fold 5, epoch 145: -186.86325730833815\n","Training loss for fold 5, epoch 146: 224.7062505906628\n","Validation elbo for fold 5, epoch 146: -187.86867427350893\n","Training loss for fold 5, epoch 147: 224.6897509174962\n","Validation elbo for fold 5, epoch 147: -185.51603632754862\n","Training loss for fold 5, epoch 148: 224.75877331149192\n","Validation elbo for fold 5, epoch 148: -185.74294475707597\n","Training loss for fold 5, epoch 149: 224.85099054151965\n","Validation elbo for fold 5, epoch 149: -186.21011895557763\n","Training loss for fold 5, epoch 150: 224.87977698541457\n","Validation elbo for fold 5, epoch 150: -185.6826045248581\n","Training loss for fold 5, epoch 151: 224.90030128725112\n","Validation elbo for fold 5, epoch 151: -185.88774127008753\n","Training loss for fold 5, epoch 152: 224.7533702235068\n","Validation elbo for fold 5, epoch 152: -185.24249802277313\n","Training loss for fold 5, epoch 153: 224.9903751496346\n","Validation elbo for fold 5, epoch 153: -185.4564878605762\n","Training loss for fold 5, epoch 154: 224.97096350885207\n","Validation elbo for fold 5, epoch 154: -186.37187340264416\n","Training loss for fold 5, epoch 155: 224.778194058326\n","Validation elbo for fold 5, epoch 155: -186.18510932423516\n","Training loss for fold 5, epoch 156: 224.8974309121409\n","Validation elbo for fold 5, epoch 156: -186.09080800420898\n","Training loss for fold 5, epoch 157: 224.87663170599168\n","Validation elbo for fold 5, epoch 157: -185.35913177647475\n","Training loss for fold 5, epoch 158: 225.02383619739163\n","Validation elbo for fold 5, epoch 158: -186.14729089729462\n","Training loss for fold 5, epoch 159: 224.85565751598728\n","Validation elbo for fold 5, epoch 159: -184.03817813226212\n","Training loss for fold 5, epoch 160: 224.92913596860825\n","Validation elbo for fold 5, epoch 160: -185.25668946104832\n","Training loss for fold 5, epoch 161: 224.97961991833103\n","Validation elbo for fold 5, epoch 161: -186.52949655235386\n","Training loss for fold 5, epoch 162: 224.9249518609816\n","Validation elbo for fold 5, epoch 162: -185.40040674648594\n","Training loss for fold 5, epoch 163: 224.8838136734501\n","Validation elbo for fold 5, epoch 163: -186.45944654646667\n","Training loss for fold 5, epoch 164: 225.09193051245904\n","Validation elbo for fold 5, epoch 164: -186.23870919886235\n","Training loss for fold 5, epoch 165: 224.9924072757844\n","Validation elbo for fold 5, epoch 165: -184.8764985546451\n","Training loss for fold 5, epoch 166: 224.6904732488817\n","Validation elbo for fold 5, epoch 166: -185.2508975314865\n","Training loss for fold 5, epoch 167: 224.99102709370274\n","Validation elbo for fold 5, epoch 167: -184.86354828715702\n","Training loss for fold 5, epoch 168: 225.01556322651524\n","Validation elbo for fold 5, epoch 168: -187.3764069952299\n","Training loss for fold 5, epoch 169: 224.9483354630009\n","Validation elbo for fold 5, epoch 169: -187.02579535337662\n","Training loss for fold 5, epoch 170: 224.66431131670552\n","Validation elbo for fold 5, epoch 170: -185.46634629546747\n","Training loss for fold 5, epoch 171: 224.94843513734878\n","Validation elbo for fold 5, epoch 171: -186.6349380906933\n","Training loss for fold 5, epoch 172: 224.8821256083827\n","Validation elbo for fold 5, epoch 172: -186.1696711383301\n","Training loss for fold 5, epoch 173: 224.57655482138358\n","Validation elbo for fold 5, epoch 173: -186.17085435282493\n","Training loss for fold 5, epoch 174: 224.94854588662423\n","Validation elbo for fold 5, epoch 174: -186.362876665495\n","Training loss for fold 5, epoch 175: 225.0380374539283\n","Validation elbo for fold 5, epoch 175: -186.8370142159192\n","Training loss for fold 5, epoch 176: 225.0397715414724\n","Validation elbo for fold 5, epoch 176: -187.17669992845885\n","Training loss for fold 5, epoch 177: 224.91814299552672\n","Validation elbo for fold 5, epoch 177: -187.1009115745121\n","Training loss for fold 5, epoch 178: 224.8256607055664\n","Validation elbo for fold 5, epoch 178: -186.16637703778503\n","Training loss for fold 5, epoch 179: 224.81077994069744\n","Validation elbo for fold 5, epoch 179: -185.02968604501154\n","Training loss for fold 5, epoch 180: 224.6864500968687\n","Validation elbo for fold 5, epoch 180: -185.85824017237422\n","Training loss for fold 5, epoch 181: 225.02196724184097\n","Validation elbo for fold 5, epoch 181: -186.27331862925968\n","Training loss for fold 5, epoch 182: 224.76961714221585\n","Validation elbo for fold 5, epoch 182: -186.5463428174017\n","Training loss for fold 5, epoch 183: 224.59002857823526\n","Validation elbo for fold 5, epoch 183: -186.7275689140851\n","Training loss for fold 5, epoch 184: 224.6734358264554\n","Validation elbo for fold 5, epoch 184: -185.53697729339348\n","Training loss for fold 5, epoch 185: 225.37088751023816\n","Validation elbo for fold 5, epoch 185: -186.26215781846622\n","Training loss for fold 5, epoch 186: 224.94489017609627\n","Validation elbo for fold 5, epoch 186: -186.2630708182097\n","Training loss for fold 5, epoch 187: 224.64122747605848\n","Validation elbo for fold 5, epoch 187: -186.2078461312883\n","Training loss for fold 5, epoch 188: 224.6530542681294\n","Validation elbo for fold 5, epoch 188: -185.17515296786863\n","Training loss for fold 5, epoch 189: 225.27557717600178\n","Validation elbo for fold 5, epoch 189: -185.78988876662325\n","Training loss for fold 5, epoch 190: 224.83151343560988\n","Validation elbo for fold 5, epoch 190: -186.18969273455713\n","Training loss for fold 5, epoch 191: 224.77587890625\n","Validation elbo for fold 5, epoch 191: -185.0353116277605\n","Training loss for fold 5, epoch 192: 224.5512882355721\n","Validation elbo for fold 5, epoch 192: -186.13260337600911\n","Training loss for fold 5, epoch 193: 225.0753444548576\n","Validation elbo for fold 5, epoch 193: -184.77020303345864\n","Training loss for fold 5, epoch 194: 225.03786000897807\n","Validation elbo for fold 5, epoch 194: -186.20008639761386\n","Training loss for fold 5, epoch 195: 224.81399757631362\n","Validation elbo for fold 5, epoch 195: -184.58837223937934\n","Training loss for fold 5, epoch 196: 224.84185791015625\n","Validation elbo for fold 5, epoch 196: -185.7232441088596\n","Training loss for fold 5, epoch 197: 225.44825966127456\n","Validation elbo for fold 5, epoch 197: -186.6563515129501\n","Training loss for fold 5, epoch 198: 224.99898873606037\n","Validation elbo for fold 5, epoch 198: -184.16399678666988\n","Training loss for fold 5, epoch 199: 224.64647674560547\n","Validation elbo for fold 5, epoch 199: -186.5838609008263\n","Training loss for fold 5, epoch 200: 224.73849093529486\n","Validation elbo for fold 5, epoch 200: -184.73617451131\n","Training loss for fold 5, epoch 201: 224.6839104929278\n","Validation elbo for fold 5, epoch 201: -185.1347717830898\n","Training loss for fold 5, epoch 202: 224.8076671477287\n","Validation elbo for fold 5, epoch 202: -187.30514432360042\n","Training loss for fold 5, epoch 203: 224.99670754709553\n","Validation elbo for fold 5, epoch 203: -185.55259820880138\n","Training loss for fold 5, epoch 204: 224.99246954148816\n","Validation elbo for fold 5, epoch 204: -185.59629712183767\n","Training loss for fold 5, epoch 205: 224.944336921938\n","Validation elbo for fold 5, epoch 205: -185.28711820693164\n","Training loss for fold 5, epoch 206: 224.84844823037423\n","Validation elbo for fold 5, epoch 206: -184.6033433155792\n","Training loss for fold 5, epoch 207: 224.92865876228578\n","Validation elbo for fold 5, epoch 207: -186.1133759546247\n","Training loss for fold 5, epoch 208: 225.055543222735\n","Validation elbo for fold 5, epoch 208: -186.88211210391034\n","Training loss for fold 5, epoch 209: 224.72402831046813\n","Validation elbo for fold 5, epoch 209: -185.35701930228797\n","Training loss for fold 5, epoch 210: 225.04906094458795\n","Validation elbo for fold 5, epoch 210: -185.63055821673572\n","Training loss for fold 5, epoch 211: 224.85693359375\n","Validation elbo for fold 5, epoch 211: -185.2192926449081\n","Training loss for fold 5, epoch 212: 224.8925252114573\n","Validation elbo for fold 5, epoch 212: -185.49208583822752\n","Training loss for fold 5, epoch 213: 225.2184074155746\n","Validation elbo for fold 5, epoch 213: -186.06262499538838\n","Training loss for fold 5, epoch 214: 224.6849611343876\n","Validation elbo for fold 5, epoch 214: -185.48934124167033\n","Training loss for fold 5, epoch 215: 224.74520381804436\n","Validation elbo for fold 5, epoch 215: -186.24135061071678\n","Training loss for fold 5, epoch 216: 224.75983330511278\n","Validation elbo for fold 5, epoch 216: -186.93758288423686\n","Training loss for fold 5, epoch 217: 224.79973996070123\n","Validation elbo for fold 5, epoch 217: -186.26581762965222\n","Training loss for fold 5, epoch 218: 225.0111832157258\n","Validation elbo for fold 5, epoch 218: -187.29180246121882\n","Training loss for fold 5, epoch 219: 224.9912399784211\n","Validation elbo for fold 5, epoch 219: -185.44099781626255\n","Training loss for fold 5, epoch 220: 224.9433406706779\n","Validation elbo for fold 5, epoch 220: -185.8038489474083\n","Training loss for fold 5, epoch 221: 224.6801560924899\n","Validation elbo for fold 5, epoch 221: -186.42918828017883\n","Training loss for fold 5, epoch 222: 224.86984154485887\n","Validation elbo for fold 5, epoch 222: -185.64737566889946\n","Training loss for fold 5, epoch 223: 224.8183108914283\n","Validation elbo for fold 5, epoch 223: -187.15711810041017\n","Training loss for fold 5, epoch 224: 224.98621466852003\n","Validation elbo for fold 5, epoch 224: -186.02147076701596\n","Training loss for fold 5, epoch 225: 224.65794766333795\n","Validation elbo for fold 5, epoch 225: -186.54483940766406\n","Training loss for fold 5, epoch 226: 225.0511007001323\n","Validation elbo for fold 5, epoch 226: -185.59838818294634\n","Training loss for fold 5, epoch 227: 224.70318135907573\n","Validation elbo for fold 5, epoch 227: -185.95555198456532\n","Training loss for fold 5, epoch 228: 224.87750760970576\n","Validation elbo for fold 5, epoch 228: -187.08630015390537\n","Training loss for fold 5, epoch 229: 224.82582067674207\n","Validation elbo for fold 5, epoch 229: -186.90147771623674\n","Training loss for fold 5, epoch 230: 224.64023024036038\n","Validation elbo for fold 5, epoch 230: -185.90179998825212\n","Training loss for fold 5, epoch 231: 224.79108748897428\n","Validation elbo for fold 5, epoch 231: -186.7487415260527\n","Training loss for fold 5, epoch 232: 224.61213782525832\n","Validation elbo for fold 5, epoch 232: -184.151537528324\n","Training loss for fold 5, epoch 233: 224.86706789078252\n","Validation elbo for fold 5, epoch 233: -185.63846387052442\n","Training loss for fold 5, epoch 234: 224.9995597101027\n","Validation elbo for fold 5, epoch 234: -185.90384799919582\n","Training loss for fold 5, epoch 235: 224.6470691311744\n","Validation elbo for fold 5, epoch 235: -186.14568036607332\n","Training loss for fold 5, epoch 236: 224.7133602019279\n","Validation elbo for fold 5, epoch 236: -185.5284965324416\n","Training loss for fold 5, epoch 237: 224.63562503937752\n","Validation elbo for fold 5, epoch 237: -185.65957368004035\n","Training loss for fold 5, epoch 238: 224.8765140656502\n","Validation elbo for fold 5, epoch 238: -185.36728972857844\n","Training loss for fold 5, epoch 239: 224.69309923725743\n","Validation elbo for fold 5, epoch 239: -187.44654031043746\n","Training loss for fold 5, epoch 240: 224.87166890790385\n","Validation elbo for fold 5, epoch 240: -186.40487319339036\n","Training loss for fold 5, epoch 241: 224.88089358422064\n","Validation elbo for fold 5, epoch 241: -185.87231846518984\n","Training loss for fold 5, epoch 242: 224.90576860981602\n","Validation elbo for fold 5, epoch 242: -185.2863890664825\n","Training loss for fold 5, epoch 243: 224.42291481264175\n","Validation elbo for fold 5, epoch 243: -186.04962284819226\n","Training loss for fold 5, epoch 244: 224.63643941571635\n","Validation elbo for fold 5, epoch 244: -185.43064815520378\n","Training loss for fold 5, epoch 245: 224.46593278454196\n","Validation elbo for fold 5, epoch 245: -186.2109325229369\n","Training loss for fold 5, epoch 246: 224.60498539094002\n","Validation elbo for fold 5, epoch 246: -186.1413499720568\n","Training loss for fold 5, epoch 247: 224.6619149484942\n","Validation elbo for fold 5, epoch 247: -186.22751109163954\n","Training loss for fold 5, epoch 248: 224.90025403422695\n","Validation elbo for fold 5, epoch 248: -184.33398272270585\n","Training loss for fold 5, epoch 249: 224.81194699195123\n","Validation elbo for fold 5, epoch 249: -185.8987576755621\n","Training loss for fold 5, epoch 250: 224.72676258702433\n","Validation elbo for fold 5, epoch 250: -185.61465258153467\n","Training loss for fold 5, epoch 251: 224.73611819359564\n","Validation elbo for fold 5, epoch 251: -185.30754005337013\n","Training loss for fold 5, epoch 252: 224.79529374645603\n","Validation elbo for fold 5, epoch 252: -185.71519100549435\n","Training loss for fold 5, epoch 253: 224.61390661424207\n","Validation elbo for fold 5, epoch 253: -185.87610547025315\n","Training loss for fold 5, epoch 254: 225.20610858548073\n","Validation elbo for fold 5, epoch 254: -186.35014151080918\n","Training loss for fold 5, epoch 255: 224.90889346215033\n","Validation elbo for fold 5, epoch 255: -187.0045335670931\n","Training loss for fold 5, epoch 256: 225.02720617478894\n","Validation elbo for fold 5, epoch 256: -186.67545815030724\n","Training loss for fold 5, epoch 257: 224.72028473884828\n","Validation elbo for fold 5, epoch 257: -185.34323661764614\n","Training loss for fold 5, epoch 258: 224.72466351909023\n","Validation elbo for fold 5, epoch 258: -185.92661689441275\n","Training loss for fold 5, epoch 259: 224.63558024744833\n","Validation elbo for fold 5, epoch 259: -186.09464381192092\n","Training loss for fold 5, epoch 260: 224.7796901579826\n","Validation elbo for fold 5, epoch 260: -186.82110213242356\n","Training loss for fold 5, epoch 261: 224.99740034534085\n","Validation elbo for fold 5, epoch 261: -187.41129011452642\n","Training loss for fold 5, epoch 262: 224.59034605949157\n","Validation elbo for fold 5, epoch 262: -185.09537803819924\n","Training loss for fold 5, epoch 263: 224.67305091119582\n","Validation elbo for fold 5, epoch 263: -185.99153154592778\n","Training loss for fold 5, epoch 264: 224.71509281281502\n","Validation elbo for fold 5, epoch 264: -185.01133357254213\n","Training loss for fold 5, epoch 265: 224.7074501283707\n","Validation elbo for fold 5, epoch 265: -185.08551623752686\n","Training loss for fold 5, epoch 266: 224.8130382414787\n","Validation elbo for fold 5, epoch 266: -185.82350402512287\n","Training loss for fold 5, epoch 267: 224.6965528918851\n","Validation elbo for fold 5, epoch 267: -185.87189959926104\n","Training loss for fold 5, epoch 268: 224.86140761836882\n","Validation elbo for fold 5, epoch 268: -186.37462073219393\n","Training loss for fold 5, epoch 269: 224.97001180341167\n","Validation elbo for fold 5, epoch 269: -186.02045778120026\n","Training loss for fold 5, epoch 270: 224.80790415117818\n","Validation elbo for fold 5, epoch 270: -185.13073929477923\n","Training loss for fold 5, epoch 271: 224.92817737210183\n","Validation elbo for fold 5, epoch 271: -185.66187409724608\n","Training loss for fold 5, epoch 272: 224.75586405108052\n","Validation elbo for fold 5, epoch 272: -185.30839349424465\n","Training loss for fold 5, epoch 273: 224.93340203069872\n","Validation elbo for fold 5, epoch 273: -185.93734804706952\n","Training loss for fold 5, epoch 274: 224.74342567689956\n","Validation elbo for fold 5, epoch 274: -185.6197130863015\n","Training loss for fold 5, epoch 275: 225.1709439677577\n","Validation elbo for fold 5, epoch 275: -186.24305797719944\n","Training loss for fold 5, epoch 276: 224.96630367155998\n","Validation elbo for fold 5, epoch 276: -186.50592290583498\n","Training loss for fold 5, epoch 277: 224.68123922040385\n","Validation elbo for fold 5, epoch 277: -186.71730307147817\n","Training loss for fold 5, epoch 278: 224.94377579227572\n","Validation elbo for fold 5, epoch 278: -186.48488480746386\n","Training loss for fold 5, epoch 279: 224.77916569863595\n","Validation elbo for fold 5, epoch 279: -185.66563916213545\n","Training loss for fold 5, epoch 280: 224.67220995503087\n","Validation elbo for fold 5, epoch 280: -185.7290419750058\n","Training loss for fold 5, epoch 281: 224.78913854783582\n","Validation elbo for fold 5, epoch 281: -186.00304501906584\n","Training loss for fold 5, epoch 282: 225.1316424954322\n","Validation elbo for fold 5, epoch 282: -185.6252506801192\n","Training loss for fold 5, epoch 283: 224.6552453810169\n","Validation elbo for fold 5, epoch 283: -185.09950756105675\n","Training loss for fold 5, epoch 284: 224.76928243329448\n","Validation elbo for fold 5, epoch 284: -186.5714553129269\n","Training loss for fold 5, epoch 285: 224.86774296914376\n","Validation elbo for fold 5, epoch 285: -185.32977250548728\n","Training loss for fold 5, epoch 286: 224.91239633867818\n","Validation elbo for fold 5, epoch 286: -185.42676246665593\n","Training loss for fold 5, epoch 287: 224.77349656628024\n","Validation elbo for fold 5, epoch 287: -186.1365819635557\n","Training loss for fold 5, epoch 288: 224.66046782462828\n","Validation elbo for fold 5, epoch 288: -184.75539286470695\n","Training loss for fold 5, epoch 289: 224.65613875850553\n","Validation elbo for fold 5, epoch 289: -186.06523780771312\n","Training loss for fold 5, epoch 290: 224.88887836087136\n","Validation elbo for fold 5, epoch 290: -185.10777870402254\n","Training loss for fold 5, epoch 291: 224.65847778320312\n","Validation elbo for fold 5, epoch 291: -185.66314396283414\n","Training loss for fold 5, epoch 292: 224.87396437121976\n","Validation elbo for fold 5, epoch 292: -184.9880818591841\n","Training loss for fold 5, epoch 293: 224.51563607492756\n","Validation elbo for fold 5, epoch 293: -185.6225007482709\n","Training loss for fold 5, epoch 294: 224.772709015877\n","Validation elbo for fold 5, epoch 294: -185.80269093775073\n","Training loss for fold 5, epoch 295: 224.82422244164252\n","Validation elbo for fold 5, epoch 295: -185.7086394046062\n","Training loss for fold 5, epoch 296: 224.67311613021357\n","Validation elbo for fold 5, epoch 296: -185.88403543377825\n","Training loss for fold 5, epoch 297: 224.89248681837512\n","Validation elbo for fold 5, epoch 297: -185.87544816183936\n","Training loss for fold 5, epoch 298: 224.87309388191468\n","Validation elbo for fold 5, epoch 298: -187.3740893099008\n","Training loss for fold 5, epoch 299: 224.7689447710591\n","Validation elbo for fold 5, epoch 299: -185.17732886015816\n","Training loss for fold 5, epoch 300: 224.6821756670552\n","Validation elbo for fold 5, epoch 300: -186.1315005823489\n","Training loss for fold 5, epoch 301: 224.742309324203\n","Validation elbo for fold 5, epoch 301: -186.2802527803624\n","Training loss for fold 5, epoch 302: 224.67368858091294\n","Validation elbo for fold 5, epoch 302: -185.15671022262472\n","Training loss for fold 5, epoch 303: 224.77392578125\n","Validation elbo for fold 5, epoch 303: -186.18647832418935\n","Training loss for fold 5, epoch 304: 224.70848132717995\n","Validation elbo for fold 5, epoch 304: -185.91423425839943\n","Training loss for fold 5, epoch 305: 224.8098609678207\n","Validation elbo for fold 5, epoch 305: -186.6713452782558\n","Training loss for fold 5, epoch 306: 224.43748720230596\n","Validation elbo for fold 5, epoch 306: -185.89459889278592\n","Training loss for fold 5, epoch 307: 224.7533889278289\n","Validation elbo for fold 5, epoch 307: -184.40650322312746\n","Training loss for fold 5, epoch 308: 224.69061008576423\n","Validation elbo for fold 5, epoch 308: -185.32344953937024\n","Training loss for fold 5, epoch 309: 224.64548836984943\n","Validation elbo for fold 5, epoch 309: -185.8590564822027\n","Training loss for fold 5, epoch 310: 224.81767814390122\n","Validation elbo for fold 5, epoch 310: -184.96281290745554\n","Training loss for fold 5, epoch 311: 224.59520204605596\n","Validation elbo for fold 5, epoch 311: -185.80927515317416\n","Training loss for fold 5, epoch 312: 224.97188912668537\n","Validation elbo for fold 5, epoch 312: -186.09571604504848\n","Training loss for fold 5, epoch 313: 224.86186045985067\n","Validation elbo for fold 5, epoch 313: -186.0276739618878\n","Training loss for fold 5, epoch 314: 224.95144973262663\n","Validation elbo for fold 5, epoch 314: -185.79830281189732\n","Training loss for fold 5, epoch 315: 224.72762520082534\n","Validation elbo for fold 5, epoch 315: -185.63492784581928\n","Training loss for fold 5, epoch 316: 224.52419034896357\n","Validation elbo for fold 5, epoch 316: -184.1034711293907\n","Training loss for fold 5, epoch 317: 224.64361867597026\n","Validation elbo for fold 5, epoch 317: -186.3643388319472\n","Training loss for fold 5, epoch 318: 224.96028383316533\n","Validation elbo for fold 5, epoch 318: -185.68301893384228\n","Training loss for fold 5, epoch 319: 224.81341183570123\n","Validation elbo for fold 5, epoch 319: -185.8620872487727\n","Training loss for fold 5, epoch 320: 224.7697022961032\n","Validation elbo for fold 5, epoch 320: -186.1838414784989\n","Training loss for fold 5, epoch 321: 224.79016900831652\n","Validation elbo for fold 5, epoch 321: -186.00236610708754\n","Training loss for fold 5, epoch 322: 225.0263962284211\n","Validation elbo for fold 5, epoch 322: -185.76174974609538\n","Training loss for fold 5, epoch 323: 224.67584080849923\n","Validation elbo for fold 5, epoch 323: -185.9226169234908\n","Training loss for fold 5, epoch 324: 224.82064548615486\n","Validation elbo for fold 5, epoch 324: -185.3247343592482\n","Training loss for fold 5, epoch 325: 224.56350388065462\n","Validation elbo for fold 5, epoch 325: -185.58497306553153\n","Training loss for fold 5, epoch 326: 224.76070207165134\n","Validation elbo for fold 5, epoch 326: -185.49322451297454\n","Training loss for fold 5, epoch 327: 224.72488624818862\n","Validation elbo for fold 5, epoch 327: -185.90342557746084\n","Training loss for fold 5, epoch 328: 224.84557440973097\n","Validation elbo for fold 5, epoch 328: -186.80498691517727\n","Training loss for fold 5, epoch 329: 224.71732256489415\n","Validation elbo for fold 5, epoch 329: -187.00139280387003\n","Training loss for fold 5, epoch 330: 224.78005144673008\n","Validation elbo for fold 5, epoch 330: -186.3308294912706\n","Training loss for fold 5, epoch 331: 224.79485739431072\n","Validation elbo for fold 5, epoch 331: -184.60215092715245\n","Training loss for fold 5, epoch 332: 224.96295855122227\n","Validation elbo for fold 5, epoch 332: -186.1360910134797\n","Training loss for fold 5, epoch 333: 224.71652418567288\n","Validation elbo for fold 5, epoch 333: -185.86787439637135\n","Training loss for fold 5, epoch 334: 224.72261022752332\n","Validation elbo for fold 5, epoch 334: -186.6718241346102\n","Training loss for fold 5, epoch 335: 224.60960757347846\n","Validation elbo for fold 5, epoch 335: -184.93849557248785\n","Training loss for fold 5, epoch 336: 224.670899421938\n","Validation elbo for fold 5, epoch 336: -185.87897482618013\n","Training loss for fold 5, epoch 337: 224.8111291700794\n","Validation elbo for fold 5, epoch 337: -187.25065902901218\n","Training loss for fold 5, epoch 338: 224.9991208968624\n","Validation elbo for fold 5, epoch 338: -186.4030917859171\n","Training loss for fold 5, epoch 339: 224.63783214938255\n","Validation elbo for fold 5, epoch 339: -184.70722248928007\n","Training loss for fold 5, epoch 340: 224.6932899721207\n","Validation elbo for fold 5, epoch 340: -186.21862047729653\n","Training loss for fold 5, epoch 341: 224.81503049788935\n","Validation elbo for fold 5, epoch 341: -186.75641294303207\n","Training loss for fold 5, epoch 342: 224.85643989809097\n","Validation elbo for fold 5, epoch 342: -186.19050302822075\n","Training loss for fold 5, epoch 343: 224.69557288385207\n","Validation elbo for fold 5, epoch 343: -185.8626301116846\n","Training loss for fold 5, epoch 344: 224.69445456227947\n","Validation elbo for fold 5, epoch 344: -186.6182476403849\n","Training loss for fold 5, epoch 345: 224.70310924899192\n","Validation elbo for fold 5, epoch 345: -185.9027049062458\n","Training loss for fold 5, epoch 346: 224.8273463095388\n","Validation elbo for fold 5, epoch 346: -185.5985185787997\n","Training loss for fold 5, epoch 347: 224.88751983642578\n","Validation elbo for fold 5, epoch 347: -186.1030914358815\n","Training loss for fold 5, epoch 348: 224.72115768924837\n","Validation elbo for fold 5, epoch 348: -184.90835224736554\n","Training loss for fold 5, epoch 349: 224.66088005804247\n","Validation elbo for fold 5, epoch 349: -186.1743507134181\n","Training loss for fold 5, epoch 350: 224.72647562334615\n","Validation elbo for fold 5, epoch 350: -185.41715200164896\n","Training loss for fold 5, epoch 351: 224.65638068414503\n","Validation elbo for fold 5, epoch 351: -185.4162697524101\n","Training loss for fold 5, epoch 352: 224.82864256828063\n","Validation elbo for fold 5, epoch 352: -185.26126801448373\n","Training loss for fold 5, epoch 353: 224.75247709212763\n","Validation elbo for fold 5, epoch 353: -185.03423148053318\n","Training loss for fold 5, epoch 354: 224.964475324077\n","Validation elbo for fold 5, epoch 354: -186.5864007995573\n","Training loss for fold 5, epoch 355: 224.78891409597088\n","Validation elbo for fold 5, epoch 355: -186.61653923945101\n","Training loss for fold 5, epoch 356: 224.6588134765625\n","Validation elbo for fold 5, epoch 356: -185.47720001275343\n","Training loss for fold 5, epoch 357: 224.61481082054877\n","Validation elbo for fold 5, epoch 357: -186.23554155186332\n","Training loss for fold 5, epoch 358: 224.58536455708165\n","Validation elbo for fold 5, epoch 358: -185.87789294073937\n","Training loss for fold 5, epoch 359: 224.61455954274822\n","Validation elbo for fold 5, epoch 359: -185.22456704465333\n","Training loss for fold 5, epoch 360: 224.83588360201927\n","Validation elbo for fold 5, epoch 360: -185.7447134316888\n","Training loss for fold 5, epoch 361: 224.79827831637473\n","Validation elbo for fold 5, epoch 361: -186.50724101228298\n","Training loss for fold 5, epoch 362: 224.56514715379285\n","Validation elbo for fold 5, epoch 362: -186.1222659594256\n","Training loss for fold 5, epoch 363: 224.67466735839844\n","Validation elbo for fold 5, epoch 363: -186.14905838189907\n","Training loss for fold 5, epoch 364: 224.86668346774192\n","Validation elbo for fold 5, epoch 364: -185.7500711523125\n","Training loss for fold 5, epoch 365: 224.82496692288308\n","Validation elbo for fold 5, epoch 365: -185.4751337112786\n","Training loss for fold 5, epoch 366: 224.51663527950163\n","Validation elbo for fold 5, epoch 366: -184.77609902990474\n","Training loss for fold 5, epoch 367: 224.54732907202936\n","Validation elbo for fold 5, epoch 367: -185.0352828029959\n","Training loss for fold 5, epoch 368: 224.56138290897493\n","Validation elbo for fold 5, epoch 368: -185.89398794077346\n","Training loss for fold 5, epoch 369: 224.77745400705646\n","Validation elbo for fold 5, epoch 369: -185.53179945814946\n","Training loss for fold 5, epoch 370: 224.71910710488595\n","Validation elbo for fold 5, epoch 370: -184.82729341293106\n","Training loss for fold 5, epoch 371: 224.73819732666016\n","Validation elbo for fold 5, epoch 371: -185.57288853520964\n","Training loss for fold 5, epoch 372: 225.03519661195816\n","Validation elbo for fold 5, epoch 372: -185.67301553426083\n","Training loss for fold 5, epoch 373: 224.87977181711506\n","Validation elbo for fold 5, epoch 373: -185.29013919342415\n","Training loss for fold 5, epoch 374: 224.99209791614163\n","Validation elbo for fold 5, epoch 374: -186.06414932321476\n","Training loss for fold 5, epoch 375: 224.85201706424837\n","Validation elbo for fold 5, epoch 375: -187.00850055982716\n","Training loss for fold 5, epoch 376: 224.7762192756899\n","Validation elbo for fold 5, epoch 376: -185.340875829603\n","Training loss for fold 5, epoch 377: 224.8808347640499\n","Validation elbo for fold 5, epoch 377: -185.82315998063743\n","Training loss for fold 5, epoch 378: 224.71826221096902\n","Validation elbo for fold 5, epoch 378: -187.08658179281394\n","Training loss for fold 5, epoch 379: 224.6424289826424\n","Validation elbo for fold 5, epoch 379: -185.3351721234738\n","Training loss for fold 5, epoch 380: 224.7356444328062\n","Validation elbo for fold 5, epoch 380: -187.20157376949822\n","Training loss for fold 5, epoch 381: 224.71727605019845\n","Validation elbo for fold 5, epoch 381: -185.78255627241637\n","Training loss for fold 5, epoch 382: 224.6039300733997\n","Validation elbo for fold 5, epoch 382: -185.71639214530904\n","Training loss for fold 5, epoch 383: 224.7235791606288\n","Validation elbo for fold 5, epoch 383: -185.39733584391848\n","Training loss for fold 5, epoch 384: 224.87208630961757\n","Validation elbo for fold 5, epoch 384: -186.22492297847384\n","Training loss for fold 5, epoch 385: 224.79024456393333\n","Validation elbo for fold 5, epoch 385: -186.00813403814993\n","Training loss for fold 5, epoch 386: 224.85757421678113\n","Validation elbo for fold 5, epoch 386: -186.09810778958268\n","Training loss for fold 5, epoch 387: 224.50687826833416\n","Validation elbo for fold 5, epoch 387: -186.12676608314547\n","Training loss for fold 5, epoch 388: 224.4905570245558\n","Validation elbo for fold 5, epoch 388: -185.4116637918576\n","Training loss for fold 5, epoch 389: 224.7624002272083\n","Validation elbo for fold 5, epoch 389: -184.52710721654393\n","Training loss for fold 5, epoch 390: 224.7588348388672\n","Validation elbo for fold 5, epoch 390: -185.3914640549653\n","Training loss for fold 5, epoch 391: 224.76934592954575\n","Validation elbo for fold 5, epoch 391: -184.96701228012108\n","Training loss for fold 5, epoch 392: 224.7002883418914\n","Validation elbo for fold 5, epoch 392: -185.58898530926734\n","Training loss for fold 5, epoch 393: 225.02241589946132\n","Validation elbo for fold 5, epoch 393: -186.31874019311942\n","Training loss for fold 5, epoch 394: 224.6458248015373\n","Validation elbo for fold 5, epoch 394: -185.92380484647282\n","Training loss for fold 5, epoch 395: 224.67267362533076\n","Validation elbo for fold 5, epoch 395: -185.85435241092344\n","Training loss for fold 5, epoch 396: 224.8313207318706\n","Validation elbo for fold 5, epoch 396: -185.92795369851257\n","Training loss for fold 5, epoch 397: 224.51769576534147\n","Validation elbo for fold 5, epoch 397: -185.6396315672264\n","Training loss for fold 5, epoch 398: 224.5073025611139\n","Validation elbo for fold 5, epoch 398: -185.76357102354643\n","Training loss for fold 5, epoch 399: 224.950926749937\n","Validation elbo for fold 5, epoch 399: -185.4296250562011\n","Training loss for fold 5, epoch 400: 224.73544508411038\n","Validation elbo for fold 5, epoch 400: -185.70961621268674\n","Training loss for fold 5, epoch 401: 224.63278419740737\n","Validation elbo for fold 5, epoch 401: -185.67348726195354\n","Training loss for fold 5, epoch 402: 224.7005849038401\n","Validation elbo for fold 5, epoch 402: -186.8080299155979\n","Training loss for fold 5, epoch 403: 224.64393911054057\n","Validation elbo for fold 5, epoch 403: -185.97915839637068\n","Training loss for fold 5, epoch 404: 224.92105127150012\n","Validation elbo for fold 5, epoch 404: -186.46988229957913\n","Training loss for fold 5, epoch 405: 224.69198706842238\n","Validation elbo for fold 5, epoch 405: -185.72300676211387\n","Training loss for fold 5, epoch 406: 224.9126719813193\n","Validation elbo for fold 5, epoch 406: -185.87572908404684\n","Training loss for fold 5, epoch 407: 224.801149675923\n","Validation elbo for fold 5, epoch 407: -186.4669506375646\n","Training loss for fold 5, epoch 408: 224.99711288944368\n","Validation elbo for fold 5, epoch 408: -185.48492248943546\n","Training loss for fold 5, epoch 409: 224.80654070454258\n","Validation elbo for fold 5, epoch 409: -185.55811052516083\n","Training loss for fold 5, epoch 410: 224.78282362414944\n","Validation elbo for fold 5, epoch 410: -186.4642339177907\n","Training loss for fold 5, epoch 411: 225.03472260505922\n","Validation elbo for fold 5, epoch 411: -185.89404874764705\n","Training loss for fold 5, epoch 412: 224.9267516597625\n","Validation elbo for fold 5, epoch 412: -185.21198345009321\n","Training loss for fold 5, epoch 413: 224.75401822982295\n","Validation elbo for fold 5, epoch 413: -186.7533426091191\n","Training loss for fold 5, epoch 414: 224.67103158274006\n","Validation elbo for fold 5, epoch 414: -186.4278421070209\n","Training loss for fold 5, epoch 415: 224.86463436003655\n","Validation elbo for fold 5, epoch 415: -185.98798676490324\n","Training loss for fold 5, epoch 416: 224.59845339867377\n","Validation elbo for fold 5, epoch 416: -186.75214278416388\n","Training loss for fold 5, epoch 417: 224.91983081448464\n","Validation elbo for fold 5, epoch 417: -185.9068176206833\n","Training loss for fold 5, epoch 418: 224.57872944493448\n","Validation elbo for fold 5, epoch 418: -186.3287546574194\n","Training loss for fold 5, epoch 419: 224.7953643798828\n","Validation elbo for fold 5, epoch 419: -185.95806845886779\n","Training loss for fold 5, epoch 420: 224.75442480271863\n","Validation elbo for fold 5, epoch 420: -185.927173887529\n","Training loss for fold 5, epoch 421: 225.0191123716293\n","Validation elbo for fold 5, epoch 421: -185.35161882040907\n","Training loss for fold 5, epoch 422: 224.64117111698275\n","Validation elbo for fold 5, epoch 422: -186.07963203809192\n","Training loss for fold 5, epoch 423: 224.79814960110573\n","Validation elbo for fold 5, epoch 423: -187.25667093261816\n","Training loss for fold 5, epoch 424: 224.7541307018649\n","Validation elbo for fold 5, epoch 424: -185.81266847480967\n","Training loss for fold 5, epoch 425: 225.08789825439453\n","Validation elbo for fold 5, epoch 425: -185.6778991654106\n","Training loss for fold 5, epoch 426: 224.82476437476373\n","Validation elbo for fold 5, epoch 426: -185.66710538206553\n","Training loss for fold 5, epoch 427: 224.8784625145697\n","Validation elbo for fold 5, epoch 427: -185.23252724891415\n","Training loss for fold 5, epoch 428: 225.11092573596585\n","Validation elbo for fold 5, epoch 428: -186.36627074845333\n","Training loss for fold 5, epoch 429: 224.6311800556798\n","Validation elbo for fold 5, epoch 429: -185.16172693693701\n","Training loss for fold 5, epoch 430: 224.67518591111707\n","Validation elbo for fold 5, epoch 430: -184.9360958863896\n","Training loss for fold 5, epoch 431: 224.74451028147053\n","Validation elbo for fold 5, epoch 431: -185.74067284963218\n","Training loss for fold 5, epoch 432: 225.1403520645634\n","Validation elbo for fold 5, epoch 432: -185.0618005366672\n","Training loss for fold 5, epoch 433: 224.57372283935547\n","Validation elbo for fold 5, epoch 433: -185.23076457121383\n","Training loss for fold 5, epoch 434: 224.9523450789913\n","Validation elbo for fold 5, epoch 434: -186.07951541175203\n","Training loss for fold 5, epoch 435: 224.82547883064515\n","Validation elbo for fold 5, epoch 435: -185.83376673905713\n","Training loss for fold 5, epoch 436: 224.77882705196257\n","Validation elbo for fold 5, epoch 436: -186.37477435424276\n","Training loss for fold 5, epoch 437: 224.86864102271295\n","Validation elbo for fold 5, epoch 437: -185.9100213433416\n","Training loss for fold 5, epoch 438: 224.82071513514364\n","Validation elbo for fold 5, epoch 438: -184.70933513333864\n","Training loss for fold 5, epoch 439: 224.86102122645224\n","Validation elbo for fold 5, epoch 439: -185.65123692799125\n","Training loss for fold 5, epoch 440: 224.45603647539693\n","Validation elbo for fold 5, epoch 440: -186.35668622335885\n","Training loss for fold 5, epoch 441: 224.8301239013672\n","Validation elbo for fold 5, epoch 441: -185.31786919931585\n","Training loss for fold 5, epoch 442: 224.54586742770286\n","Validation elbo for fold 5, epoch 442: -185.81911643323963\n","Training loss for fold 5, epoch 443: 224.71316331432712\n","Validation elbo for fold 5, epoch 443: -186.0658852737265\n","Training loss for fold 5, epoch 444: 224.60304777083857\n","Validation elbo for fold 5, epoch 444: -185.24718584674486\n","Training loss for fold 5, epoch 445: 224.7009274882655\n","Validation elbo for fold 5, epoch 445: -185.6315685676853\n","Training loss for fold 5, epoch 446: 224.82570352861958\n","Validation elbo for fold 5, epoch 446: -186.72437704810594\n","Training loss for fold 5, epoch 447: 224.7148137246409\n","Validation elbo for fold 5, epoch 447: -185.93577040515913\n","Training loss for fold 5, epoch 448: 225.05743777367377\n","Validation elbo for fold 5, epoch 448: -187.22889254394244\n","Training loss for fold 5, epoch 449: 224.77964979602444\n","Validation elbo for fold 5, epoch 449: -186.90224283231052\n","Training loss for fold 5, epoch 450: 224.7587612521264\n","Validation elbo for fold 5, epoch 450: -185.40729176413294\n","Training loss for fold 5, epoch 451: 224.68290045953566\n","Validation elbo for fold 5, epoch 451: -186.9996689446665\n","Training loss for fold 5, epoch 452: 224.68169599963772\n","Validation elbo for fold 5, epoch 452: -185.7099922653158\n","Training loss for fold 5, epoch 453: 224.82000338646674\n","Validation elbo for fold 5, epoch 453: -186.51897256080258\n","Training loss for fold 5, epoch 454: 224.82626096663935\n","Validation elbo for fold 5, epoch 454: -185.53071922964358\n","Training loss for fold 5, epoch 455: 224.69331335252332\n","Validation elbo for fold 5, epoch 455: -187.24943374962905\n","Training loss for fold 5, epoch 456: 224.77633494715536\n","Validation elbo for fold 5, epoch 456: -186.8416385438074\n","Training loss for fold 5, epoch 457: 224.88308961929815\n","Validation elbo for fold 5, epoch 457: -187.2375469919125\n","Training loss for fold 5, epoch 458: 224.5701367778163\n","Validation elbo for fold 5, epoch 458: -185.94453857360926\n","Training loss for fold 5, epoch 459: 224.90912135954827\n","Validation elbo for fold 5, epoch 459: -185.25423975100347\n","Training loss for fold 5, epoch 460: 224.68402739494078\n","Validation elbo for fold 5, epoch 460: -185.45253017571633\n","Training loss for fold 5, epoch 461: 224.5551976849956\n","Validation elbo for fold 5, epoch 461: -186.00859434712282\n","Training loss for fold 5, epoch 462: 224.55310821533203\n","Validation elbo for fold 5, epoch 462: -185.38213419683876\n","Training loss for fold 5, epoch 463: 224.87917254048008\n","Validation elbo for fold 5, epoch 463: -185.90754224043417\n","Training loss for fold 5, epoch 464: 224.73137787849672\n","Validation elbo for fold 5, epoch 464: -185.85277144842183\n","Training loss for fold 5, epoch 465: 224.39515316870904\n","Validation elbo for fold 5, epoch 465: -185.2247469250426\n","Training loss for fold 5, epoch 466: 224.5956526725523\n","Validation elbo for fold 5, epoch 466: -185.8683185990958\n","Training loss for fold 5, epoch 467: 224.58077830653036\n","Validation elbo for fold 5, epoch 467: -185.25569958429244\n","Training loss for fold 5, epoch 468: 224.77993823635964\n","Validation elbo for fold 5, epoch 468: -185.38635045731837\n","Training loss for fold 5, epoch 469: 224.76675710370463\n","Validation elbo for fold 5, epoch 469: -186.28700783035617\n","Training loss for fold 5, epoch 470: 224.80481769192605\n","Validation elbo for fold 5, epoch 470: -184.85521975798144\n","Training loss for fold 5, epoch 471: 224.90886589788622\n","Validation elbo for fold 5, epoch 471: -185.7459296434934\n","Training loss for fold 5, epoch 472: 224.84547424316406\n","Validation elbo for fold 5, epoch 472: -186.2316475108845\n","Training loss for fold 5, epoch 473: 224.62190517302483\n","Validation elbo for fold 5, epoch 473: -185.6492762797511\n","Training loss for fold 5, epoch 474: 224.8613997428648\n","Validation elbo for fold 5, epoch 474: -186.60041832976336\n","Training loss for fold 5, epoch 475: 224.87042187106223\n","Validation elbo for fold 5, epoch 475: -185.46233205310182\n","Training loss for fold 5, epoch 476: 224.6115255048198\n","Validation elbo for fold 5, epoch 476: -185.12928508990564\n","Training loss for fold 5, epoch 477: 224.89987502559538\n","Validation elbo for fold 5, epoch 477: -186.00429859683078\n","Training loss for fold 5, epoch 478: 224.4826662617345\n","Validation elbo for fold 5, epoch 478: -185.24346619580416\n","Training loss for fold 5, epoch 479: 224.94092707480155\n","Validation elbo for fold 5, epoch 479: -185.78148639859717\n","Training loss for fold 5, epoch 480: 224.58345523957283\n","Validation elbo for fold 5, epoch 480: -185.34242235871477\n","Training loss for fold 5, epoch 481: 225.05202311854208\n","Validation elbo for fold 5, epoch 481: -186.27870444355204\n","Training loss for fold 5, epoch 482: 224.6196042952999\n","Validation elbo for fold 5, epoch 482: -187.09441527576462\n","Training loss for fold 5, epoch 483: 224.62430474065965\n","Validation elbo for fold 5, epoch 483: -184.9291471394182\n","Training loss for fold 5, epoch 484: 224.79574166574787\n","Validation elbo for fold 5, epoch 484: -186.53542217546635\n","Training loss for fold 5, epoch 485: 224.82169932703817\n","Validation elbo for fold 5, epoch 485: -186.68637060762228\n","Training loss for fold 5, epoch 486: 224.53624159289944\n","Validation elbo for fold 5, epoch 486: -187.14575281947018\n","Training loss for fold 5, epoch 487: 224.62516513947517\n","Validation elbo for fold 5, epoch 487: -185.50821432551265\n","Training loss for fold 5, epoch 488: 224.59826094104397\n","Validation elbo for fold 5, epoch 488: -185.84189556353493\n","Training loss for fold 5, epoch 489: 224.8154245192005\n","Validation elbo for fold 5, epoch 489: -185.9912384912708\n","Training loss for fold 5, epoch 490: 224.66638626590853\n","Validation elbo for fold 5, epoch 490: -185.85393791399292\n","Training loss for fold 5, epoch 491: 224.61938944170552\n","Validation elbo for fold 5, epoch 491: -187.4387826189851\n","Training loss for fold 5, epoch 492: 224.89531067878968\n","Validation elbo for fold 5, epoch 492: -185.6695248104366\n","Training loss for fold 5, epoch 493: 224.64962571667087\n","Validation elbo for fold 5, epoch 493: -186.5018656992088\n","Training loss for fold 5, epoch 494: 224.78288072155368\n","Validation elbo for fold 5, epoch 494: -186.0060902629243\n","Training loss for fold 5, epoch 495: 224.8151131906817\n","Validation elbo for fold 5, epoch 495: -186.1510942440135\n","Training loss for fold 5, epoch 496: 224.39956640428113\n","Validation elbo for fold 5, epoch 496: -185.49350807969617\n","Training loss for fold 5, epoch 497: 224.65665928010017\n","Validation elbo for fold 5, epoch 497: -185.84274170952833\n","Training loss for fold 5, epoch 498: 224.5692128827495\n","Validation elbo for fold 5, epoch 498: -185.10031918121427\n","Training loss for fold 5, epoch 499: 224.5003199423513\n","Validation elbo for fold 5, epoch 499: -185.58009948395298\n"]}]},{"cell_type":"code","source":["MSA_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xE7n6DQ3iQz8","executionInfo":{"status":"ok","timestamp":1711055274972,"user_tz":300,"elapsed":337,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"72c92343-09f9-4218-c68f-3d928ae3a6f6"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'PF00067_full'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["fig, axs = plt.subplots(5, 2, sharex = \"all\")\n","for k in range(K):\n","  axs[k,0].plot(train_loss[k+1], label = \"training loss\", color = 'r')\n","  #plt.plot(test_loss_epoch, label = \"test\", color = 'b')\n","  #plt.ylim((140, 180))\n","  axs[k,0].set_xlabel('epoch')\n","  axs[k,0].set_ylabel(f\"Fold {k+1}\")\n","handles, labels = axs[0,0].get_legend_handles_labels()\n","fig.legend(handles, labels,loc = \"upper left\")\n","\n","for k in range(K):\n","  axs[k,1].plot([x/batch_size for x in valid_elbos[k+1]], label = \"validation elbo\", color = 'b')\n","  #plt.plot(test_loss_epoch, label = \"test\", color = 'b')\n","  #plt.ylim((140, 180))\n","  axs[k,1].set_xlabel('epoch')\n","handles, labels = axs[0,1].get_legend_handles_labels()\n","fig.legend(handles, labels,  loc = \"upper right\")\n"],"metadata":{"id":"fKVNSGX_2kbG","colab":{"base_uri":"https://localhost:8080/","height":517},"executionInfo":{"status":"ok","timestamp":1711055279969,"user_tz":300,"elapsed":2331,"user":{"displayName":"Mengze Tang","userId":"00338099096995044238"}},"outputId":"d2ec43f2-b349-4d5d-8d8b-15e9f8d8eaa1"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fd0b32f1390>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"9Z15u3fYGa2J"},"execution_count":null,"outputs":[]}]}